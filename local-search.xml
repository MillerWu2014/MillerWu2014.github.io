<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ReID的OSnet网络详解</title>
    <link href="/2021/01/03/reid%E7%9A%84osnet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/"/>
    <url>/2021/01/03/reid%E7%9A%84osnet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>最近在做一个关于行人重识别的相关项目，对ReID的一些背景和相关技术进行了一些了解。ReID属于行人检测的延伸或叫做子任务，但比行人检测的难度更大，主要在于ReID是利用计算机视觉技术对特定行人进行跨视域匹配和检索，所谓的跨视域即是图像来源于不同的摄像头，因此可能因为角度，遮挡，光线，换装等导致检测的难度增大。ReID的应用场景较多，如无人超市中的人员监控，刑侦中的目标搜索，车辆追踪等等。</p><p>ReID主要是在行人检测之后的识别过程，所以一半会经过两部分任务，第一部分主要完成行人检测，第二部分再进行ReID检测。而ReID检测中主要进行两个步骤：第一步，对目标(输入图像)进行特征提取，即feature extraction；第二步，在特征提取之后计算各个输入之间的相似度或距离，即similarity measurement。通过相似度度量值排序后输出。</p><h2 id="开源数据集"><a href="#开源数据集" class="headerlink" title="开源数据集"></a>开源数据集</h2><p>目前，ReID相关开源的数据集较多，如Market1501，cuhk03，viper，dukemtmc，msmt17，grid等等。这些数据集基本包括<code>bounding_box_train</code>，<code>bounding_box_test</code>和<code>query</code>三部分，会将数据划分为train，test，query和gallery，训练集中的人和测试集中的人没有重合。</p><p>在Market1501数据集中每个图片命名格式为:<code>1500_c6s3_086567_01.jpg</code>，1500为person_id，c6s3表示摄像机编号和序列编号，086567为视频中所在的帧数位置，01表示该person在该摄像机序列下的第01个图像。一个person在对应的摄像机和序列下可能对应多个图像。其他开源数据集的命名格式类似。</p><p>数据下载地址可以参考此处的汇总：<a href="https://blog.csdn.net/qiuchangyong/article/details/82219775" target="_blank" rel="noopener">ReID开源数据集汇总</a></p><a id="more"></a><h2 id="OSnet"><a href="#OSnet" class="headerlink" title="OSnet"></a>OSnet</h2><p>本次项目中最初使用了resent50网络作为reid阶段的网络，基于开源数据集进行了重新训练，但是在计算相似度时发现提取的特征计算出相似度都比较大，而且每个行人之间的特征比较接近(行人之间的相似度都比较大)。说明模型在提取特征方面存在一些问题，特征局部表示能力可能较弱。因此需要更换网络。</p><p>osnet称为全尺度(<strong>omni-scale</strong>)网络，用于ReID的全尺度特征学习，将局部特征和全局特征进行对应和组合。OSnet主要是基于CNN体系建立，基本构建块由多个卷积流构成，每个卷积流检测不同尺度范围的特征，对全尺度特征学习，提出了一种统一的聚合门(aggregation gate, AG)，通过AG将多尺度特征与信道权值动态融合。同时OSnet也是轻量级的网络，全过程由卷积构成，在构建块中将标准卷积分解为点卷积和深度卷积。</p><p><img src="/uploads/multi-struct.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><h3 id="深度可分离卷积-Depthwise-Separable-Convolutions"><a href="#深度可分离卷积-Depthwise-Separable-Convolutions" class="headerlink" title="深度可分离卷积(Depthwise Separable Convolutions)"></a>深度可分离卷积(Depthwise Separable Convolutions)</h3><p>该方法主要用来减少网络的参数数量和计算量，在轻量级网络中可以采用深度可分离卷积来设计轻量级结构，主要是将一个原始的标准卷积核$w\in \R ^ {k \times k \times c \times {c’}}$分解为点卷积核深度卷积：$w=u*v$，其中：</p><p>$u \in \R ^ {1 \times 1 \times c \times {c’}}$为一个点卷积，为通道维度上的卷积；$ v \in \R ^ {k \times k \times 1 \times {c’}}$为一个深度卷积，在feature map上用感受视野k来聚合局部信息。其结构对比如下图所示，标准3*3的卷积进行分离后：</p><p><img src="/uploads/deep-seg-struct.png" srcset="/img/loading.gif" style="zoom:70%;" /></p><p>假设一张图片的大小为w,h,那么标准卷积的参数量为$h<em>w</em>k<em>k</em>c<em>c’$,在使用分离卷积后参数量变为$h</em>w<em>(k</em>k+c)<em>c’$（此处省略了偏置项），参数量会成倍的减少，作者称`Lite3</em>3`。还值得注意的是<strong>作者在此处先用的点卷积再使用的深度卷积</strong>。</p><h3 id="全尺度残差块"><a href="#全尺度残差块" class="headerlink" title="全尺度残差块"></a>全尺度残差块</h3><p>该部分主要是利用不同尺度的卷积流提取不同尺度的特征，并将不同尺度的特征进行融合。作者提出了一种<code>residual bottleneck</code>结构，是基于<code>baseline residual bottleneck</code>结构进行创建。如下图所示。</p><p><img src="/uploads/redidual_bottleneck.png" srcset="/img/loading.gif" style="zoom:70%;" /></p><p><code>baseline residual bottleneck</code>（如上图如(a)）在输入$x$后，而bottleneck结构学习残差部分$\bar x$，公式如下，其中$F$主要学习单尺度的特征(scale=3)，其中$1 \times 1$的layer主要用于操作特征的维度，对空间信息没有作用。</p><script type="math/tex; mode=display">y = x + \bar x,   s.t.    \bar x = F(x)</script><h4 id="多尺度特征学习"><a href="#多尺度特征学习" class="headerlink" title="多尺度特征学习"></a>多尺度特征学习</h4><p>作者创建基于上图(a)创建了<code>residual bottleneck</code>块(上图(b))，引入了指数$t$来表示特征尺度，对$F$进行扩展为$F^t$，表示将t个<code>Lite3*3</code>叠加起来，这时候的感受视野也变为$(2t+1) \times (2t+1)$，因此，$\bar x$可以表示为多个尺度之和：</p><script type="math/tex; mode=display">\bar x = \sum_{t=1}^{T} F^t(x), T \ge 1</script><p>T为结构块的分支数，$T=4$时，OSNet bottleneck结构如上图b所示，此时共有4个分支，感受视野分别为$3\times3$，$5\times5$，$7\times7$，$9\times9$；然后会通过聚合门将所有特征进行聚合。整体结构类似于<code>baseline residual bottleneck</code>结构。</p><h4 id="统一聚合们"><a href="#统一聚合们" class="headerlink" title="统一聚合们"></a>统一聚合们</h4><p>在多尺度学习中，已经获取到了不同尺度的特征，每个流都可以提供不同尺度的特征，为了学习全尺度特征，作者建议以动态的方式组合不同流的输出，根据输入图像赋予不同尺度特征以不同的权重。作者使用的是一种可学习的网络结构(aggregation gate, AG)，如下所示：</p><script type="math/tex; mode=display">\bar x = \sum_{t=1}^T \alpha{_t} x^t = \sum_{t=1}^T G(x^t) x^t, \qquad s.t. \qquad x_t=F^t(x)</script><p>其中G为一个子网络结构学习得到，子网络G包含三个层：一个全局平均池化层，然后是两个全连接(FC)层。输出后的$\alpha_t$为一个向量，而非标量，实现更加细粒度的融合，对通道的权值进行了优化。这和其他的gate不同，实现了更加抽象的，更细粒度的特征融合。</p><p>在每个<code>residual bottleneck</code>块中，AG在各个分支中共享参数，参数的数量独立于分支数目T。即上图b中的虚线部分中的AG是共享参数的。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>OSNet是通过简单地将提出的的轻量级bottleneck逐层叠加而构建的，不需要在网络的不同深度(阶段)定制块。OSnet的整体网络结构如下表所示。OSnet网络的输入image大小为256*128。在模型实现时为了更好的泛化，引入了instance normalisation layer，作者在论文中并未进行重点说明，但在实现的源码中进行了体现。</p><p><img src="/uploads/osnet_struct.png" srcset="/img/loading.gif" alt="osnet_struct" style="zoom:50%;" /></p><h2 id="OSnet实现"><a href="#OSnet实现" class="headerlink" title="OSnet实现"></a>OSnet实现</h2><p>OSnet使用最后一层使用了标准的<code>multi-label classifier</code>(FC+ softmax)层，通过SGD优化，数据集中每个person-id作为一个单独类别，并使用<code>cross entropy loss</code>进行训练，学习率从0.065开始，并在不同的epoch衰减0.1倍。在计算相似度或距离度量时采用L2距离，512维的向量作为person-id的特征表示，该向量来自于最后一层FC层的512长度向量输出。</p><h3 id="OSnet的pytorch实现"><a href="#OSnet的pytorch实现" class="headerlink" title="OSnet的pytorch实现"></a>OSnet的pytorch实现</h3><p>作者在开源的Torchreid库中对OSnet网络，下面为基础版本OSnet源码。其中部分代码进行了注释。</p><pre><code class="lang-python">class ConvLayer(nn.Module):    &quot;&quot;&quot;实现一个标准的卷积层(conv + bn + relu)&quot;&quot;&quot;    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, IN=False):        super(ConvLayer, self).__init__()        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False, groups=groups)        # 是否使用instance normalisation        if IN:            self.bn = nn.InstanceNorm2d(out_channels, affine=True)        else:            self.bn = nn.BatchNorm2d(out_channels)        self.relu = nn.ReLU(inplace=True)    def forward(self, x):        x = self.conv(x)        x = self.bn(x)        x = self.relu(x)        return xclass Conv1x1(nn.Module):    &quot;&quot;&quot;1x1 convolution + bn + relu.&quot;&quot;&quot;    def __init__(self, in_channels, out_channels, stride=1, groups=1):        super(Conv1x1, self).__init__()        self.conv = nn.Conv2d(in_channels, out_channels, 1, stride=stride, padding=0, bias=False, groups=groups)        self.bn = nn.BatchNorm2d(out_channels)  # 直接使用了batch normalisation        self.relu = nn.ReLU(inplace=True)    def forward(self, x):        x = self.conv(x)        x = self.bn(x)        x = self.relu(x)        return xclass Conv1x1Linear(nn.Module):    &quot;&quot;&quot;1x1 convolution + bn (w/o non-linearity).&quot;&quot;&quot;    def __init__(self, in_channels, out_channels, stride=1):        super(Conv1x1Linear, self).__init__()        self.conv = nn.Conv2d(            in_channels, out_channels, 1, stride=stride, padding=0, bias=False        )        self.bn = nn.BatchNorm2d(out_channels)    def forward(self, x):        x = self.conv(x)        x = self.bn(x)        return xclass Conv3x3(nn.Module):    &quot;&quot;&quot;3x3 convolution + bn + relu.&quot;&quot;&quot;    def __init__(self, in_channels, out_channels, stride=1, groups=1):        super(Conv3x3, self).__init__()        self.conv = nn.Conv2d(            in_channels,            out_channels,            3,            stride=stride,            padding=1,            bias=False,            groups=groups        )        self.bn = nn.BatchNorm2d(out_channels)        self.relu = nn.ReLU(inplace=True)    def forward(self, x):        x = self.conv(x)        x = self.bn(x)        x = self.relu(x)        return xclass LightConv3x3(nn.Module):    &quot;&quot;&quot;深度可分解卷积(Light3*3): weight 3x3 convolution. 1x1 (linear) + dw 3x3 (nonlinear).    &quot;&quot;&quot;    def __init__(self, in_channels, out_channels):        super(LightConv3x3, self).__init__()        # 点卷积        self.conv1 = nn.Conv2d(            in_channels, out_channels, 1, stride=1, padding=0, bias=False        )        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False, groups=out_channels)  # groups设置为了out_channels        self.bn = nn.BatchNorm2d(out_channels)        self.relu = nn.ReLU(inplace=True)    def forward(self, x):        x = self.conv1(x)        x = self.conv2(x)        x = self.bn(x)        x = self.relu(x)        return x########### Building blocks for omni-scale feature learning##########class ChannelGate(nn.Module):    &quot;&quot;&quot;A mini-network that generates channel-wise gates conditioned on input tensor.    aggregation gate, AG: 平均池化层+卷积层1(全连接)+Relu+卷积层2(全连接)+Sigmoid, 对通道进行了gate    &quot;&quot;&quot;    def __init__(self, in_channels, num_gates=None, return_gates=False, gate_activation=&#39;sigmoid&#39;, reduction=16, layer_norm=False):        super(ChannelGate, self).__init__()        if num_gates is None:            num_gates = in_channels        self.return_gates = return_gates        self.global_avgpool = nn.AdaptiveAvgPool2d(1)        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=True, padding=0)        self.norm1 = None        if layer_norm:            self.norm1 = nn.LayerNorm((in_channels // reduction, 1, 1))        self.relu = nn.ReLU(inplace=True)        self.fc2 = nn.Conv2d(in_channels // reduction, num_gates, kernel_size=1, bias=True, padding=0)        if gate_activation == &#39;sigmoid&#39;:            self.gate_activation = nn.Sigmoid()        elif gate_activation == &#39;relu&#39;:            self.gate_activation = nn.ReLU(inplace=True)        elif gate_activation == &#39;linear&#39;:            self.gate_activation = None        else:            raise RuntimeError(                &quot;Unknown gate activation: {}&quot;.format(gate_activation)            )    def forward(self, x):        input = x        x = self.global_avgpool(x)        x = self.fc1(x)        if self.norm1 is not None:            x = self.norm1(x)        x = self.relu(x)        x = self.fc2(x)        if self.gate_activation is not None:            x = self.gate_activation(x)        if self.return_gates:            return x        return input * xclass OSBlock(nn.Module):    &quot;&quot;&quot;论文中的residual bottleneck block实现: Omni-scale feature learning block.&quot;&quot;&quot;    def __init__(self, in_channels, out_channels, IN=False, bottleneck_reduction=4, **kwargs):        super(OSBlock, self).__init__()        mid_channels = out_channels // bottleneck_reduction        self.conv1 = Conv1x1(in_channels, mid_channels)  # 先使用了1*1的卷积对通道进行一个reduce的操作        self.conv2a = LightConv3x3(mid_channels, mid_channels)  # 基于Light3*3构建t=1的卷积流         # 基于Light3*3构建t=2的卷积流        self.conv2b = nn.Sequential(              LightConv3x3(mid_channels, mid_channels),            LightConv3x3(mid_channels, mid_channels),        )        # 基于Light3*3构建t=3的卷积流        self.conv2c = nn.Sequential(            LightConv3x3(mid_channels, mid_channels),            LightConv3x3(mid_channels, mid_channels),            LightConv3x3(mid_channels, mid_channels),        )        # 基于Light3*3构建t=4的卷积流        self.conv2d = nn.Sequential(            LightConv3x3(mid_channels, mid_channels),            LightConv3x3(mid_channels, mid_channels),            LightConv3x3(mid_channels, mid_channels),            LightConv3x3(mid_channels, mid_channels),        )        self.gate = ChannelGate(mid_channels)        self.conv3 = Conv1x1Linear(mid_channels, out_channels)  # 线性组合的方式        self.downsample = None        if in_channels != out_channels:            self.downsample = Conv1x1Linear(in_channels, out_channels)        self.IN = None        if IN:            self.IN = nn.InstanceNorm2d(out_channels, affine=True)    def forward(self, x):        identity = x        x1 = self.conv1(x)        x2a = self.conv2a(x1)        x2b = self.conv2b(x1)        x2c = self.conv2c(x1)        x2d = self.conv2d(x1)        x2 = self.gate(x2a) + self.gate(x2b) + self.gate(x2c) + self.gate(x2d)  # 每个流对应的是不同的向量，向量是针对通道的        x3 = self.conv3(x2)        if self.downsample is not None:            identity = self.downsample(identity)        out = x3 + identity  # y = x + x^hat        if self.IN is not None:            out = self.IN(out)  # 可能会进行instance normazation，对应的结构为OS + INout block        return F.relu(out)########### Network architecture: OSnet的网络结构实现##########class OSNet(nn.Module):    &quot;&quot;&quot;Omni-Scale Network.    Reference:        - Zhou et al. Omni-Scale Feature Learning for Person Re-Identification. ICCV, 2019.        - Zhou et al. Learning Generalisable Omni-Scale Representations          for Person Re-Identification. arXiv preprint, 2019.    &quot;&quot;&quot;    def __init__(self, num_classes, blocks, layers, channels, feature_dim=512, loss=&#39;softmax&#39;, IN=False, **kwargs):        super(OSNet, self).__init__()        num_blocks = len(blocks)        assert num_blocks == len(layers)        assert num_blocks == len(channels) - 1        self.loss = loss        self.feature_dim = feature_dim        # convolutional backbone        self.conv1 = ConvLayer(3, channels[0], 7, stride=2, padding=3, IN=IN) # kernel size: 7*7        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)        self.conv2 = self._make_layer(blocks[0], layers[0], channels[0], channels[1], reduce_spatial_size=True, IN=IN)        self.conv3 = self._make_layer(blocks[1], layers[1], channels[1], channels[2], reduce_spatial_size=True)        self.conv4 = self._make_layer(blocks[2], layers[2], channels[2], channels[3], reduce_spatial_size=False)        self.conv5 = Conv1x1(channels[3], channels[3])        self.global_avgpool = nn.AdaptiveAvgPool2d(1)        # fully connected layer        self.fc = self._construct_fc_layer(            self.feature_dim, channels[3], dropout_p=None        )        # identity classification layer        self.classifier = nn.Linear(self.feature_dim, num_classes)        self._init_params()    def _make_layer(self, block, layer, in_channels, out_channels, reduce_spatial_size, IN=False):        layers = []        layers.append(block(in_channels, out_channels, IN=IN))        for i in range(1, layer):            layers.append(block(out_channels, out_channels, IN=IN))        # 对应网络结构中的transition stage部分：1*1 conv + 2*2 avg pool        if reduce_spatial_size:            layers.append(                nn.Sequential(                    Conv1x1(out_channels, out_channels),                    nn.AvgPool2d(2, stride=2)                )            )        return nn.Sequential(*layers)    def _construct_fc_layer(self, fc_dims, input_dim, dropout_p=None):        if fc_dims is None or fc_dims &lt; 0:            self.feature_dim = input_dim            return None        if isinstance(fc_dims, int):            fc_dims = [fc_dims]        layers = []        for dim in fc_dims:            layers.append(nn.Linear(input_dim, dim))            layers.append(nn.BatchNorm1d(dim))            layers.append(nn.ReLU(inplace=True))            if dropout_p is not None:                layers.append(nn.Dropout(p=dropout_p))            input_dim = dim        self.feature_dim = fc_dims[-1]        return nn.Sequential(*layers)    def _init_params(self):        for m in self.modules():            if isinstance(m, nn.Conv2d):                nn.init.kaiming_normal_(                    m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;                )                if m.bias is not None:                    nn.init.constant_(m.bias, 0)            elif isinstance(m, nn.BatchNorm2d):                nn.init.constant_(m.weight, 1)                nn.init.constant_(m.bias, 0)            elif isinstance(m, nn.BatchNorm1d):                nn.init.constant_(m.weight, 1)                nn.init.constant_(m.bias, 0)            elif isinstance(m, nn.Linear):                nn.init.normal_(m.weight, 0, 0.01)                if m.bias is not None:                    nn.init.constant_(m.bias, 0)    def featuremaps(self, x):        x = self.conv1(x)        x = self.maxpool(x)        x = self.conv2(x)        x = self.conv3(x)        x = self.conv4(x)        x = self.conv5(x)        return x    def forward(self, x, return_featuremaps=False):        x = self.featuremaps(x)        if return_featuremaps:            return x        v = self.global_avgpool(x)        v = v.view(v.size(0), -1)        if self.fc is not None:            v = self.fc(v)        if not self.training:            return v  # 在检测时,输出全连接层的特征        y = self.classifier(v)        if self.loss == &#39;softmax&#39;:            return y        elif self.loss == &#39;triplet&#39;:            return y, v        else:            raise KeyError(&quot;Unsupported loss: {}&quot;.format(self.loss))</code></pre><h3 id="关于InstanceNorm在不同的位置的实现"><a href="#关于InstanceNorm在不同的位置的实现" class="headerlink" title="关于InstanceNorm在不同的位置的实现"></a>关于InstanceNorm在不同的位置的实现</h3><p>另外，还是实现了osnet_ain的网络结构，整体结构和上面基础版本的osnet基本一致，在一些细节上出现差异，比如新增了<code>LightConvStream</code>，是对<code>LightConv3x3</code>进行的叠加实现，并加入参数<code>t</code>来控制叠加的层数，并对<code>OSBlockINin</code>修改了上面osnet中<code>OSBlock</code>的InstanceNorm的位置。基于InstanceNorm在不同的位置，如下图所示：</p><p><img src="/uploads/instance_norm.png" srcset="/img/loading.gif" alt="instance normalzation"></p><p>当instance norm(在每个通道上对W*H算均值方差计算，进行归一化操作，具体可以参考batch norm，instance norm，layer norm及group norm区别等)在不同的位置时，<code>os residual block</code>结构存在小的差异。作者在实现时也考虑到了位置不同结构的实现。</p><h2 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h2><p>作者在不同的数据集(大型数据集，小型数据集)上，以及不同类型的任务上都进行了测试，模型效果非常理想。在项目上运用该模型，最终效果也是较好的。在不同的person上，特征之间的差异较大。在reid上的效果好于resnet，且在特征提取的效率上也提升了很多。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>OSnet论文阅读笔记：<a href="https://blog.csdn.net/weixin_40671425/article/details/93846404" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40671425/article/details/93846404</a></li><li>论文地址：<a href="https://arxiv.org/pdf/1905.00953.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.00953.pdf</a></li><li>osnet实现：<a href="https://github.com/KaiyangZhou/deep-person-reid/tree/master/torchreid/models" target="_blank" rel="noopener">https://github.com/KaiyangZhou/deep-person-reid/tree/master/torchreid/models</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep-learning</tag>
      
      <tag>CNN</tag>
      
      <tag>convolution</tag>
      
      <tag>reid</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能陷进观后感</title>
    <link href="/2020/10/24/%E6%99%BA%E8%83%BD%E9%99%B7%E8%BF%9B%E8%A7%82%E5%90%8E%E6%84%9F/"/>
    <url>/2020/10/24/%E6%99%BA%E8%83%BD%E9%99%B7%E8%BF%9B%E8%A7%82%E5%90%8E%E6%84%9F/</url>
    
    <content type="html"><![CDATA[<p>智能陷进是NetfFix推出的一步对于科技对人类负面影响的记录片，片中讲述的主要是社交网络或媒体对于人类的影响，影响包括多方面的：行为上，语言上，思想上，道德上等等。整个影片分2段进行，第一部分主要是对个人的影响，第二部分讲述对整个社会，国家及人类的影响。我对文中提到的一些点感到非常震撼。</p><h2 id="人类期货"><a href="#人类期货" class="headerlink" title="人类期货"></a>人类期货</h2><p>其实像Facebook，IG，Twitter等这样的社交媒体，他们的终极产品是“人”，而不是所谓的内容，他们通过不断的吸取用户的关注，对人进行交易。背后的资本主义的产品是人，他们可以通过内容，算法来引导人的行为，预测人的行为，控制人的行为。这种控制和传统意义的控制差别很大，是乐于被控制，无形中被控制。人只是一个商品甚至行尸走肉的商品。</p><h2 id="二级分化和极端性"><a href="#二级分化和极端性" class="headerlink" title="二级分化和极端性"></a>二级分化和极端性</h2><p>影片中提到了现在社会分化严重，如美国目前的选举，要么选A，要么选B；没有中间地带选择，再比如香港时间。这些只需要背后的”金主“操控给人们看什么，就会影响人的行为。同时人们的行为也更加分化，极端。如影片中统计的青少年自杀的人数和方式，从社交网络开始青少年自杀的人数就开始上升。其实这些行为主要是受到了智能引擎(算法)的影响，通过算法能预测用户的行为，向用户推送用户可能想看到的内容，这样人一直都会关注上面的信息(榨取人的专注力和时间)，而推送的东西随时间会对人的思想产生影响，在潜意识里会改变你的思想，甚至驯化你的思想。比如看到了某些负面的消息，后续可能一直推送的负面消息，这样会将你的负面情绪快速的，无限的放大。</p><blockquote><p>它针对的是人类的生物算法（<strong>多巴胺奖赏回路</strong>），信息茧房效应诱导的后真相和观点极化，已是铁板钉钉无远弗届的现实</p></blockquote><h2 id="信息的真假"><a href="#信息的真假" class="headerlink" title="信息的真假"></a>信息的真假</h2><p>互联网上的消息人们无法辨别真假，连FB，IG，Twitter，Tiktok这些平台也无法辨别真假，人们一般会将假消息当成真消息。这是对人来说是非常讽刺的，而假消息可能对人们的行为影响非常大，对人们的思想影响非常大，从而出现极端行为，分化严重。</p><h2 id="思想停滞"><a href="#思想停滞" class="headerlink" title="思想停滞"></a>思想停滞</h2><p>当我们浏览某个媒体上的内容是，不断又新的内容推荐给我们，这些内容的很多是没有深度的和很多价值的，但是基于兴趣和人性的弱点我们还是会看，看了之后我们只会有一些简单的判断或思想的镜像(就像抄袭了别人的想法)，我们可能没有时间，没有专注力，没有深度思考其内容。这样往复循环，我们的专注力和时间被这些社交网络，繁杂的信息所榨取，抢劫。我们并未对一些内容进行深度的思考，我们的思想未进化，思想停滞。</p><p>有时我们陷入了看手机→空虚→看手机的循环中，这就是一个巨大的黑洞。现在年轻人大部分都离不开手机，他们的专注力不在工作，不在生活，而在于FB，TikTok这些APP上。长时间，会导致有些人患上抑郁症，人们变得浮躁，人们变得傻瓜式等。最近几年我们国家自杀的人数也是急剧上升，社会上的企业，个人都变得更加浮躁都体现了出来。其实，这可能不是人的问题，就像影片中说到利用了人性的弱点，触动了你灵魂深处的，已经超过人的极限。就像以前我身边只有3个人年薪百万，但是现在通过这些APP工具看到了100个，甚至1000个普通人都是年薪百万（且这些人说的真假还无从判定），能不浮躁吗？这已经超过了人的极限。</p><h2 id="批判者才是真正的乐观主义者"><a href="#批判者才是真正的乐观主义者" class="headerlink" title="批判者才是真正的乐观主义者"></a>批判者才是真正的乐观主义者</h2><p>任何时代都应该出现批判者，批判者会推动社会以及技术的发展。如果缺少或没有批判者说明整个系统出现了问题。事物在批判和被批判会在相互作用下逐渐成熟，完美。</p><h2 id="根源"><a href="#根源" class="headerlink" title="根源"></a>根源</h2><p>大部分人还是认为这些工具的初衷是好的，只是社会黑暗的一部分利用工具(app, 大数据, 算法等)来操纵。还包括了一些其他原因，如：信息的真实性无从确定，监管的不健全等等。那怎么去解决这样的问题呢？</p><p>在纪录片中没有标准答案，也没有人知道如何解决，但也不是没有可以做的。比如我们可以加强自身的自制力，有这样的意识，控制自己接触手机电脑的时间。同时政府也可以进行监管，出台相应的政策来辅助等等。</p>]]></content>
    
    
    <categories>
      
      <category>电影</category>
      
    </categories>
    
    
    <tags>
      
      <tag>思考</tag>
      
      <tag>生活</tag>
      
      <tag>电影</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Convolutional Pose Machines论文记录</title>
    <link href="/2020/09/27/ConvolutionalPoseMachines%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/09/27/ConvolutionalPoseMachines%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>CPM模型的结构如下图所示。主要分为了不同的Stage。在<strong>stage1</strong>中，输入图片368×368，卷积层不改变feature maps的width和height，经三次 pooling 层，输出的feature maps大小46×46，共P+1个feature maps。stage≥2时，输入包括3部分，原始图片的特征X_z,前一stage输出的局部置信图(belief maps)和生成的center的Gaussian中心约束；网络的输出是一致的，都是46×46×(P+1)的feature maps。</p><p><img src="/uploads/cpm_struct.png" srcset="/img/loading.gif" alt="./CPM"></p><p><img src="/uploads/cpm_cnn.png" srcset="/img/loading.gif" alt="CPM_CNN"></p><p>Stage1：input是原始图像，经过全卷机网络，输出是一个P+1层的2Dmap。其中，全卷积网络中有7个卷积层，3个池化层，原始输入图片是 368<em>368 ，经过3次池化后得到 46</em>46 大小。又因为这里使用的数据库是半身结构，只有9个关节点，因此加上背景，输出的响应图大小应该是46<em>46</em>10。<a id="more"></a></p><p>Stage2：input是 Stage1 的 Output 响应谱，并且加上原始图像通过几层网络后的特征谱 feature map。输出是一个P+1层的2Dmap。其中，stage 2 融合了三部分的信息–一是stage1的响应图，二是原始图像的图像特征，三是高斯模版生成的中心约束。图像深度变为10+32+1=43。</p><p>Stage3：及其后面各个阶段的网络结构和 Stage 2 相似为了防止训练时出现梯度消失的问题：论文采用了中层监督（加入中层loss），加强反向传播。</p><ul><li>center map为一个高斯响应,因为cpm处理的是单人pose的问题，如果图片中有多人，那么center map可以告诉网络，目前要处理的那个人的位置。因为这样的设置，cpm也可以自底向上地处理多人pose的问题。</li></ul><h2 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h2><ol><li>在Stage2中将原始的图像X进行了变换，生成了X‘，再将X‘输入到stage≥2的网络中。</li><li>Center Map(生成的Center的Gaussian中心约束)是怎么样生成的？高斯模版生成的中心约束</li><li>图像增强处理：随机旋转图片 [-40, 40]，图片缩放 [0.7, 1.3]，水平翻转</li><li>中继监督：每个stage都进行loss计算，最终将每个stage的loss进行求和，作为总的loss。防止梯度小时，保证底层参数正常更新。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_36165459/article/details/78321054" target="_blank" rel="noopener">论文阅读：《Convolutional Pose Machines》CVPR 2016_青青韶华-CSDN博客</a></p><p><a href="https://blog.csdn.net/u010579901/article/details/79606257" target="_blank" rel="noopener">论文阅读笔记: 2016 cvpr Convolutional Pose Machines_ProYH-CSDN博客</a></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep-learning</tag>
      
      <tag>CNN</tag>
      
      <tag>convolution</tag>
      
      <tag>HPE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MetNet网络对未来降雨量预测</title>
    <link href="/2020/09/05/MetNet%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/"/>
    <url>/2020/09/05/MetNet%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>MetNet是Google提出的用于空间降雨量预测的模型，通过深度学习的方式对未来的降雨量进行预测，可以实现未来8小时的降雨量预测，而且每2分钟更新一次。而模型的最终效果好于传统的传统的方法。</p><h2 id="1-降雨量预测"><a href="#1-降雨量预测" class="headerlink" title="1.降雨量预测"></a>1.降雨量预测</h2><p>主要使用了MRMS的数据和GOES-16的16个波段数据作为输入，覆盖了全美国区域，输入为一个张量，为时空型数据，每个像元代表为1km的空间分辨率。模型输出的张量为一个3维张量，每个值代表了一个时间，一个位置对应的降雨量。</p><h2 id="2-NWMs-Neural-Weather-Models"><a href="#2-NWMs-Neural-Weather-Models" class="headerlink" title="2.NWMs(Neural Weather Models)"></a>2.NWMs(Neural Weather Models)</h2><p>NWMs就是预测在时间T下，$X$条件下的概率$y$，可以见简单的表示为：</p><script type="math/tex; mode=display">p(y|x)=DNN_{\theta}(x)</script><p>其中$p(y|x)$就是基于输入$x$下$y$的概率分布，$\theta$为深度学习要学习到的参数。NWM是通过概率形式计算可能结果的概率分布来说明不确定性，而不是产生单一的确定性输出。模型的整体结构如下图所示：</p><p><img src="/uploads/image-20200715140513390.png" srcset="/img/loading.gif" alt="image-20200715140513390"></p><p>从上面NWMs的整体框架图可以看出模型的输入和输出，整个的结构由<strong>空间下采样器，时间编码器和空间聚合器</strong>三大部分组成。</p><h2 id="3-MetNet架构"><a href="#3-MetNet架构" class="headerlink" title="3.MetNet架构"></a>3.MetNet架构</h2><h3 id="3-1-输入层"><a href="#3-1-输入层" class="headerlink" title="3.1 输入层"></a>3.1 输入层</h3><p>MetNet接收四维的张量$(t,w,h,c)$，分别为时间维度，width，height和通道数。在时间维度上取前90分钟的数据，每15分钟一次。输入的width和height分别为1024，1024。MRMS的雷达图像为单一特征，GOES-16的波段特征为多通道（16个波段）。同时会输入每个位置的经度和纬度数据，每个位置的海拔高度，需要预测的时间（拆分为月，天，小时）。</p><h3 id="3-2-输出层"><a href="#3-2-输出层" class="headerlink" title="3.2 输出层"></a>3.2 输出层</h3><p>MetNet输出一个512个类别的分布，每个类别为从0mm/h-102.4mm/h间隔为0.2mm/h的降雨量。将雨量预测变为了512个类别的分类问题。</p><h3 id="3-3-空间下采样器"><a href="#3-3-空间下采样器" class="headerlink" title="3.3 空间下采样器"></a>3.3 空间下采样器</h3><p>MetNet这一部分主要是压缩输入张量。在空间上使用一系列卷积和池化操作。沿着时间维度进切片，每个切片分别进行处理。每个切片首先被打包成一个空间的输入张量，尺寸为256<em>256，具体打包可以参考下图<em>*打包流程</em></em>。然后，每个切片都被以下神经网络层处理：一个有160个通道的 $3\times3$卷积；一个$2\times2$的max-pooling层，stride为2；3个$3\times3$的256通道卷积；一个$2\times2$最大池化层，stride为2。这些操作产生了空间维度的$64 \times 64$和256通道张量。</p><p><img src="/uploads/image-20200715145510115.png" srcset="/img/loading.gif" alt="打包流程"></p><h3 id="3-4-时间编码器"><a href="#3-4-时间编码器" class="headerlink" title="3.4 时间编码器"></a>3.4 时间编码器</h3><p>这一部分主要是沿时间维度对输入的张量进行编码，将输入到一个convLSTM网络中，该网络的kernel size为$3 \times 3$，输出384个通道数。利用RNN网络在时间序列中动态找出时间维度的模式特征。最终输出一个单向量为$64\times64$，通道数为384的张量。再将结果输入到下一步骤中。convLSTM结构（FC-LSTM的改进）参考如下：</p><p><img src="/uploads/image-20200715162343841.png" srcset="/img/loading.gif" alt="image-20200715162343841"></p><p>其中，<code>*</code>为卷积运算，<code>o</code>为Hadamard乘积。</p><h3 id="3-5-空间聚合器"><a href="#3-5-空间聚合器" class="headerlink" title="3.5 空间聚合器"></a>3.5 空间聚合器</h3><p>MetNet在这部分主要通过<code>self-attention blocks</code>进行操作，包含8个轴注意力网络，其中4个是沿宽度操作的，4个是沿高度操作的。这8个网络的作用是使MetNet的接收域涵盖全局信。每个<code>self-attention blocks</code>有2048个通道和16个<code>attention heads</code>。</p><h2 id="4-效果"><a href="#4-效果" class="headerlink" title="4.效果"></a>4.效果</h2><p>最终效果远超过物理模型，谷歌的神经天气模型的一个显著优点是，它是为密集并行计算而优化的，并且非常适合在专用硬件（如 TPU）上运行。无论是针对纽约市这样的特定地点还是针对整个美国，预测可以在几秒钟内并行进行。而物理模型在超级计算机上的运行时间约为一小时。和NOAA的HRRR系统进行了对比，通过F1指标对比如下，其结果MetNet始终是优于HRRR模型。</p><p><img src="/uploads/image-20200715160040183.png" srcset="/img/loading.gif" alt="image-20200715160040183"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><ol><li><p>该模型在预测降雨时输入的数据主要包括了雷达数据，GEOS-16的16个波段数据（波段数据主要是能反映气象信息，如云，地表属性，间接性温度等等），高程（海拔)，这些数据都是时空型数据。另外加入了将要预测的日期(月，天，小时)和经纬度。</p></li><li><p>降雨量预测本质还是时空型数据预测问题，因此MetNet使用了卷积对空间特征进行提取，再使用LSTM对时间维度上的模型进行提取，最后加入Attention综合全局信息，对降雨量进行预测。</p></li><li><p>在对目标的处理上，将降雨量以0.2mm进行分割为512个类别。这种处理方式直接将预测变为分类问题，在应用中是非常可取的。</p></li><li><p>存在的几个疑问：</p><ul><li><strong>为什么不直接使用历史的降雨量数据作为输入？</strong>（可能是因为降雨本身也是通过云图等产生的，通过雷达图推断出来的降雨。另外就是降雨量的历史真真实数据收集存在误差，NOAA的结果也是存在较大误差。作为输入存在问题，作为目标是可以的。）</li><li><strong>预测未来某个t+k时刻的降雨，并未使用t+k-1时刻的预测值？</strong>（传统的时间序列问题，在预测未来某个时刻会使用前一个时刻的结果，作为张量处理的时候直接采用了端到端的结构，直接预测未来一段时间。）</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>deeplearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>gradient</tag>
      
      <tag>attention</tag>
      
      <tag>last</tag>
      
      <tag>deep-learning</tag>
      
      <tag>convLSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>boosting系列之NGBoost算法详解</title>
    <link href="/2020/06/11/Boosting%E7%AE%97%E6%B3%95%E4%B9%8BNGBoost%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <url>/2020/06/11/Boosting%E7%AE%97%E6%B3%95%E4%B9%8BNGBoost%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>去年末看过NGBoost的论文，由于时间原因没有仔细的去了解，现在终于可以花点时间来补一下NGBoost，NGBoost是斯坦福的研究团队提出的一种梯度提升方法，解决概率预测中的问题，在现实中预测问题带有很多不确定性，比如天气的预测，顾客流失率，信用等等。传统的机器学习一般是一个点的估计，NGBoost则是对概率预测，可以得到概率分布的预测。</p><p>论文中通过模块化描述了NGBoost模型的整体结构，如下图所示.NGBoost通过自然梯度来实现拟合，其中模块主要包括<strong>基础学习器</strong>，<strong>参数的概率分布</strong>，<strong>评分规则</strong>这三大部分。</p><p><img src="/uploads/model_struct.png" srcset="/img/loading.gif" alt="NGBoost三个模块"></p><p>通过优化评分规则（例如最大似然估计（MLE）或更稳健的连续排名概率评分（CRPS）），对模型进行了训练，以使锐度(sharpness)最大化，并进行校准，最后会得到经过校准的估计。</p><h2 id="自然梯度-Nature-Gradient"><a href="#自然梯度-Nature-Gradient" class="headerlink" title="自然梯度(Nature Gradient)"></a>自然梯度(Nature Gradient)</h2><p>NGBoost是一种概率预测的监督性学习法，通过概率分布的形式将不确定行带入了梯度计算中，再以函数的形式来预测条件概率($P(y|x)$)分布的参数。</p><p>在标准预测中，目标对象是标量函数$E[y|x]$的估计，其中$x$是观察到的特征的向量，$y$是预测目标。 在NGBoost中，通过预测参数$θ∈R$来生成概率密度为$P<em>θ(y|x)$，$\theta$的概率预测， 相应的累积密度表示为$F</em>\theta$。</p><p>此处会有KL散度来定义预测分布P和真实分布Q之间的距离，通过KL散度来定义规则评分，因此最终的目标就是最小化这个规则评分S。在每次迭代中，参数的更新就会影响分布的更新，从而影响规则评分。因此参数的更新非常重要，即分布的移动和更新，论文中是通过自然梯度解决。</p><h2 id="模块化"><a href="#模块化" class="headerlink" title="模块化"></a>模块化</h2><p>NGBoost主要通过基础学习器，参数概率分布和评分规则三大模块：</p><ul><li>基础学习器为学习函数$f$，比如决策树</li><li>参数概率分布（$P_\theta$），如正太分布，多变量正太分布，拉普拉斯分布（开源代码中未实现），伯努利分布，指数分布等。</li><li>评分规则($S$)，如MLE，CRPS等</li></ul><h3 id="评分规则"><a href="#评分规则" class="headerlink" title="评分规则"></a>评分规则</h3><p>评分规则对「预测的概率分布」和「目标分布」的进行对照观察，评分规则$S$将一个预测的概率分布$P$和一个观测值$y$（结果）作为输入，并给预测赋分$S(P;y)$。得出结果的真实分布可获得预期中的最佳分数。在目前的实现中支持MLE和CRPS，CRPS可以提供更加稳定的结果。</p><p>从上图中的模块化流程中可以看出，从输入X到Y，预测$Y|X$，是由条件分布$P_\theta$经过评分规则$S$产生的，其中$\theta$可以是一个值也可以是一个向量，它最终影响预测结果。</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>整个算法流程比较简单，和一般的GBDT算法流程相同，整个算法需要提前确定概率分布，迭代次数，学习率，评分规则和基础学习器。在算法中有一个缩放因子，该缩放因子在每次迭代中都会进行更新。</p><p><img src="/uploads/ngboost_alg_flow.png" srcset="/img/loading.gif" alt="image-20200609152114311" style="zoom:50%;" /></p><p>从算法流程中可以看出模型是按序列学习的，每个阶段都有一组函数$f$和一个缩放因子$\varphi$。算法在开始时，首先会估计一个共同的初始分布$\theta(0)$，并通过训练使它能最小化评分规则$S$，在所有训练样本的影响了变量上的总和，在本质上就是拟合y的边际分布。</p><p>在每次迭代过程中，每批次的样本都会通过评分规则$S$，计算自然梯度$g$，再通过自然梯度在基学习器上进行投影(Projection)，基学习器上投影结果通过缩放因子进行梯度缩放，再引入学习率参数$\eta$等，来更新参数$\theta$：</p><script type="math/tex; mode=display">y|x \sim P_{\theta}(x), \theta = \theta^{(0)}-\eta\sum_{m=1}^{M}\varphi^{(m)}*f^{(m)}(x)</script><p>其中，$\varphi$为缩放因子，在每次迭代中都会进行更新。</p><h3 id="训练过程源码解析"><a href="#训练过程源码解析" class="headerlink" title="训练过程源码解析"></a>训练过程源码解析</h3><p>在官方开源的NGBoost源码中，对训练过程进行了详细的分析，目前的代码主要基于scikit-learn之上实现了NGBoost，目前实现了回归和分类(分类本身具有概率预测，因此一般都用于回归预测)。下面对源码关键部分进行注释，若要更详细了解可以参考官方开源代码：</p><pre><code class="lang-python">def fit(self, X, Y, X_val=None, Y_val=None, sample_weight=None, val_sample_weight=None, train_loss_monitor=None,        val_loss_monitor=None, early_stopping_rounds=None):    &quot;&quot;&quot;    Fits an NGBoost model to the data    Parameters:    X                       : DataFrame object or List or numpy array of predictors (n x p) in Numeric format    Y                       : Type same as X. Should be floats for regression and integers from 0 to K-1 for K-class classification    X_val                   : DataFrame object or List or numpy array    Y_val                   : DataFrame object or List or numpy array    sample_weight           : how much to weigh each example in the training set. (defaults to 1)    val_sample_weight       : how much to weigh each example in the validation set. (defaults to 1)    train_loss_monitor      : Defaults to the score defined in the NGBoost constructor    val_loss_monitor        : Defaults to the score defined in the NGBoost constructor    early_stopping_rounds   : the number of consecutive boosting iterations during which the loss has to increase before the algorithm stops early    Output:        A fit NGBRegressor object    &quot;&quot;&quot;    if Y is None:        raise ValueError(&quot;y cannot be None&quot;)    X, Y = check_X_y(X, Y, y_numeric=True)    loss_list = []    self.fit_init_params_to_marginal(Y)  # 初始化分布的参数    params = self.pred_param(X)  # 参数初始化，在该方法中将使用self.fit_init_params_to_marginal(Y)的初始化参数结果    # -------- 此处省略部分非核心代码 ----------------    for itr in range(self.n_estimators):  # n_estimators为迭代次数和gbdt类似         # 首先对全量样本进行下采样, 默认为所有数据, 获得每次迭代的下采用数据, P_batch为该batch下的参数        _, col_idx, X_batch, Y_batch, weight_batch, P_batch = self.sample(            X, Y, sample_weight, params        )         self.col_idxs.append(col_idx)        D = self.Manifold(P_batch.T) # 获取参数分布,对应论文中的统计流形,通过Score规则和分布生成        loss_list += [train_loss_monitor(D, Y_batch, weight_batch)]  # 记录评分(score),评分规则的输入为分布和目标Y        loss = loss_list[-1]        grads = D.grad(Y_batch, natural=self.natural_gradient)  # 求梯度，自然梯度        proj_grad = self.fit_base(X_batch, grads, weight_batch)  # 将自然梯度在base learner上projection        # 下面更新缩放因子,每次迭代都会更新一次缩放因子,缩放因子的更新会参考初始参数和梯度值        # line_search方法中有向上和向下缩放,根据初始分布和Y的评分结果选择更新策略        scale = self.line_search(proj_grad, P_batch, Y_batch, weight_batch)          # 开始更新分布的参数        params -= (            self.learning_rate            * scale            * np.array([m.predict(X[:, col_idx]) for m in self.base_models[-1]]).T        )          # -------------------- 省略非核心代码 -------------------------------        # 模型训练的停止条件,将自然梯度在基学习器上映射后的梯度值进行norm后计算均值        if np.linalg.norm(proj_grad, axis=1).mean() &lt; self.tol:            if self.verbose:                print(f&quot;== Quitting at iteration / GRAD {itr}&quot;)            break    self.evals_result = {}    metric = self.Score.__name__.upper()    self.evals_result[&quot;train&quot;] = {metric: loss_list}    if X_val is not None and Y_val is not None:        self.evals_result[&quot;val&quot;] = {metric: val_loss_list}    return self</code></pre><p><code>NGBoost</code>和其他<code>boosting</code>算法不同的是可以预测概率分布，因此可以使用<code>pred_dist</code>方法返回每个预测的概率分布。同时也支持<code>predict</code>接口，直接返回预测值。</p><p>上面训练过程中其中统计流形的定义也是非常重要的。定义是通过评分规则和参数的分布混合定义统计流形。流行包含了分布的所有参数，而且有<code>fit</code>和<code>sample</code>方法，通过<code>Distribution</code>实现；还有<code>total_score</code>和<code>grad</code>方法，通过<code>Score</code>实现。</p><pre><code class="lang-python">def manifold(Score, Distribution):    &quot;&quot;&quot;    Mixes a scoring rule and a distribution together to create the resultant &quot;Reimannian Manifold&quot;    (thus the name of the function). The resulting object has all the parameters of the distribution     can be sliced and indexed like one, and carries the distributions `fit` and `sample` methods, but     it also carries the appropriate `total_score` and `grad` methods that are inherited through     distribution-specific inheritence of the relevant implementation of the scoring rule    &quot;&quot;&quot;    class Manifold(Distribution.implementation(Score), Distribution):        pass    return Manifoldclass Score:    def total_score(self, Y, sample_weight=None):        return np.average(self.score(Y), weights=sample_weight)    def grad(self, Y, natural=True):        grad = self.d_score(Y)        if natural:            metric = self.metric()            grad = np.linalg.solve(metric, grad)        return gradclass Normal(RegressionDistn):    n_params = 2    scores = [NormalLogScore, NormalCRPScore]    def __init__(self, params):        super().__init__(params)        self.loc = params[0]        self.scale = np.exp(params[1])        self.var = self.scale ** 2        self.dist = dist(loc=self.loc, scale=self.scale)    def fit(Y):        m, s = sp.stats.norm.fit(Y)        return np.array([m, np.log(s)])    def sample(self, m):        return np.array([self.rvs() for i in range(m)])    def __getattr__(        self, name    ):  # gives us Normal.mean() required for RegressionDist.predict()        if name in dir(self.dist):            return getattr(self.dist, name)        return None    @property    def params(self):        return {&quot;loc&quot;: self.loc, &quot;scale&quot;: self.scale}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>从官方的测试结果来看，NGBoost和其他Boosting算法的预测效果不分上下，甚至效果更好。</li><li>效率上比其他模型差，目前的实现中使用了batch的模式。但是目前主要是通过python实现，核心部分并没有通过C/C++实现。</li><li>目前官方开源的代码还没有实现<code>early stop</code>，基础学习器也支持较少。</li><li>NGBoost的核心主要是自然梯度和评分规则的结合，并将自然梯度进行了一般化的简易实现。这部分也是很难理解的部分。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>1.<a href="https://medium.com/@benbenbang/ngboost-intro-and-comparisons-df72adf94096" target="_blank" rel="noopener">https://medium.com/@benbenbang/ngboost-intro-and-comparisons-df72adf94096</a></p><p>2.论文地址<a href="https://arxiv.org/pdf/1910.03225v1.pdf" target="_blank" rel="noopener">NGBoost: Natural Gradient Boosting for Probabilistic Prediction</a></p><p>3.源码: <a href="https://github.com/stanfordmlgroup/ngboost" target="_blank" rel="noopener">https://github.com/stanfordmlgroup/ngboost</a></p><p>4.自然梯度：<a href="https://www.zhihu.com/question/21923317" target="_blank" rel="noopener">https://www.zhihu.com/question/21923317</a></p><p>5.NGBoost(自然梯度提升)：<a href="https://zhuanlan.zhihu.com/p/100271626" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/100271626</a></p><p>6.NGBoost论文研读：<a href="https://blog.csdn.net/weixin_44750583/article/details/103940140?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44750583/article/details/103940140?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>boosting</tag>
      
      <tag>catboost</tag>
      
      <tag>probabilistic</tag>
      
      <tag>gradient</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WWDC机器学习与产品设计笔记</title>
    <link href="/2020/05/28/WWWDC%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1%E7%AC%94%E8%AE%B0/"/>
    <url>/2020/05/28/WWWDC%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>机器学习的范围很广，包括语音识别的，计算机视觉的，自然语言处理的等等；特别是深度学习技术的进步。但是这些最终都要进行应用，与产品进行结合，以产品的形式提供给用户，与用户进行交互，这些都需要设计，且精心设计。</p><h2 id="思考怎样造就优秀的机器学习产品"><a href="#思考怎样造就优秀的机器学习产品" class="headerlink" title="思考怎样造就优秀的机器学习产品"></a>思考怎样造就优秀的机器学习产品</h2><p>机器学习或许不能解决某个问题，但是至少要提升产品质量或产品体验。就像iphone中的Face ID解锁，照片中直接可以搜索某个事物，可以查看回忆，Siri的事务处理等等。要设计优秀的机器学习体验，不仅要设计它的工作模式，还要设计它的<strong>外观和感觉</strong>；不仅要设计模型，还要设计界面。下面是机器学习在产品设计上的模型图：</p><p><img src="/uploads/机器学习与产品设计.png" srcset="/img/loading.gif" alt="机器学习与产品设计" style="zoom:56%;" /></p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p><a id="more"></a>机器学习模型是非常复杂和庞大的。创建模型必须做出很多决策，关于数据的，关于算法的，参数，框架，这些都会影响模型的效果；这些有专业的工程师或专家负责。但是在产品设计上一般关心的是数据和指标。</p><p>数据是机器学习的基础，是解决用什么教计算机。指标是要评估的东西，一般都会将指标设计为一个值，评估机器学习的好坏，机器学习体验的好坏等等。</p><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>数据对机器学习模型是至关重要的，数据决定模型的行为，以及后续的一切。因此在设计阶段就必须考虑。比如要做某个情景下的机器学习模型，那么就必须要包括该情景下的数据，如果没有捕捉到该情景下的重要信息，那么模型不会见得会多好。</p><p>数据应该是基于场景的，如果说产品或应用是基于许多场景，那就这些场景的数据就必须都要进行收集，而且需要考虑数据在不同场景下的平衡性（数据偏差）。比如你正在创建一个有趣或丰富的体验你应该为有趣、丰富的体验提供样本数据，如果你的产品是在户外使用你应该提供户外情境的样本。这样数据才能包容足够的信息，在后续的模型中才会更具有泛化性，用户体验上才会更好。</p><blockquote><p>关于数据收集的决策就是，从你现有的客户手中收集样本数据并从其它机构中获取全世界的统一的样本数据，但你不能反映世界，反映世界会增加系统偏差。你不应该针对现有客户进行优化，你应该针对你想要获得的客户进行优化；你不应该按世界原本的样子反映它，你应该反映一个更好的世界，是你所希望的那个世界。收集数据是设计体验的一种方式，优秀的体验常常能改变世界，它们不只是反映现在所存在的东西</p></blockquote><p>上面是演讲者所说的关于数据收集时的决策，原文的翻译中有一些问题，我简单进行了修改。主要有以下几点必须关注：</p><ol><li>数据收集可以非常全，也必须统一，但不能反映世界（这里应该是宏观上的全面的意思，能意会就好）</li><li>收集数据时不应该针对现有客户进行优化，而是要针对你想要获取的客户进行优化。</li><li>收集数据时不能就按原有世界样子进行收集，收集到的数据应该反映出一个更好的世界（这里的世界可以理解为场景，用户体验时的情景）。总之收集到的数据需要在原有的基础之上有所改变，有所改善。</li></ol><p>在实践时，需要思考产品为用户创建什么样的体验？需要在某个场景下进行思考数据的收集。<strong>使用产品的用户，如何使用产品，思考数据可能会朝向或背离某些人群或某种体验而产生偏差的方式</strong>，并开始数据收集。</p><p>随着产品的迭代更新，数据也需要被更新；可以了解使用你产品的用户更多需求，更好的在迭代中优化产品。在迭代中保持数据和产品的目标一致。</p><h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><p>主要是对模型进行评估，对模型的评估包括很多，比如模型的精度，准确率，召回率。这些都是对模型的测试及效果评价。还包括模型运行有多快？或模型支持多少个类别等等，根据不同的场景设计不同的指标，但最终指标都是反映了良好的体。</p><p>对于机器学习每次的失败都应该被追踪，需要对每次失败的任务进行思考，任务对应的用户，体验，行为，情景等等。特别是容错性低的情景下需要对失败的情景独立思考，处理。</p><p>机器学习具有容错性，模型不太可能达到100%。而对于错误，要对它进行理解，需要进行解释，可能基于错误创建新的模型改进它们，也可以解释技术的局限性。我<strong>们不仅仅要关注模型正常下的情况，可能更多要关注失败下的情景，错误可以帮助我们了解每个情景的重要性，失败的情况还能帮助我们了解使用它的人更多需求或体验</strong>。</p><p>在用户体验中，模型的体验不是体验，实际体验（用户使用产品时的体验）才是体验。因此需要尝试通过指标量化该体验，必须准确的量化，如果出现体验糟糕，指标确很好，那么指标就可能出错了。</p><p>指标的演变也很重要，随时间的增长要经常质疑和评估指标的有效性（<strong>越依赖一样东西就越应该质疑它</strong>）。指标反映了价值，必须使价值和指标保持一致。</p><h2 id="用户交互-界面"><a href="#用户交互-界面" class="headerlink" title="用户交互(界面)"></a>用户交互(界面)</h2><p>界面是非常重要的，用户通过直观、简单的方式和机器学习产品进行交互，体验。而界面主要是将模型的结果转化为输出，用户可以与之进行交互或体验。从界面中收集到反馈，即输入。</p><h3 id="输出-Output"><a href="#输出-Output" class="headerlink" title="输出(Output)"></a>输出(Output)</h3><p>输出包括很多类型：多种选择，属性，可信度，限制；多种选择是以多种输出呈现给用户；属性是一种解释，帮助用户更加了解，帮助决策；可信度是对输出的确定性度量；限制发生在当用户对某功能的心理模型与实际所能实现的功能不相匹配时。</p><h4 id="多种选择-Multiple-Options"><a href="#多种选择-Multiple-Options" class="headerlink" title="多种选择(Multiple Options)"></a>多种选择(Multiple Options)</h4><p>有时模型给出的输出可能不是最佳体验，在现实中存在太多的不确定性，那么就需要给出用户更多的选择；在交互中完成用户心目中最佳的选择。比如线路预测：有些人可能喜欢最近的，有些人可能不喜欢走高速，有些人喜欢风景好的路线…，而且在影响该模型的因素很多多，并在实时的变化，因此只能预测可能的最佳线路，输出多个选择。</p><p>还有一个例子非常好，用户一天中要看很多信息，但是在某个时刻，某个地点，基于历史行为我可能只需要看TOP3的信息。因此多种选择的输出在这种场景中是非常重要的。</p><h4 id="属性-Attributes"><a href="#属性-Attributes" class="headerlink" title="属性(Attributes)"></a>属性(Attributes)</h4><p>主要在解释性场景中的输出类型，比如推荐，通过属性解释推荐，关联客观事实，而不是主管臆测。比如，音乐推荐中，我们推荐了乡村音乐，是基于你听过“Love Story”……</p><p>我们不能完全了解用户的喜好和口味，每个人每天都在发生变化，去剖析他们会让他们困惑，且收到局限性。</p><h4 id="可信度-Confidence"><a href="#可信度-Confidence" class="headerlink" title="可信度(Confidence)"></a>可信度(Confidence)</h4><p>可信度可以和属性结合，也可以单独给出。比如我在iPhone中搜索某个术语，给我的结果中首先列出给我的事这个术语的解释信息，并标记为来自维基百科。这样就更具有可信度，因为维基百科的可信度非常高，这样就帮助我更加信赖该信息的准确性。</p><p>在某些场景下直接使用可信度在设计上不太适合，比如说某个用户和另外的用户相似度达到85%，那么用户在体验式对85%是非常模糊的，可能很难理解这个85%，因此也是通过属性来解释性输出设计，比如根据你之前听了……歌单作出推荐。</p><p>在某些场景下可以直接使用可信度，比如天气预报，明天有80%的可能下雨，那么用户在体验时根据可信度作出决策，应该带雨伞。用户对☁️预报的场景已经习惯这个百分比。</p><p>还有在订购航班时，某个应用上给出这趟航班可能有70%的可能价格下降，这样用户体验会降低，用户可能会纠结这个百分比，而且预测为70%与65%的区别呢？还不如直接建议用户怎么决策，如建议购买，等待……</p><h4 id="限制-Limitations"><a href="#限制-Limitations" class="headerlink" title="限制(Limitations)"></a>限制(Limitations)</h4><p>限制这中输出方式主要从产品的角度进行设计，当限制发生时，要引导用户，处理这种局限性，解除限制。比如我要在mac上通过siri设置一个定时器，但是mac上没有定时器，它做不到。但是在设计时，出处理这种限制，引导用户：你可以通过设置一个提醒来代替它。</p><p>总之，输出只是一个设计媒介，通过了解可用的输出类型，可以选择与我们想要创建的体验相一致的输出而不仅仅是依赖于来自模型的标准输出。我们应该通过选择简单易懂和毫不费力的有帮助的输出，将机器学习的输出和产品设计完美结合。</p><h3 id="输入-Input"><a href="#输入-Input" class="headerlink" title="输入(Input)"></a>输入(Input)</h3><p>提到了输入有四种方式：校准，隐性反馈，明确的反馈，修正；这四种方式在产品中实现方式以及达到的目的也有所不同。比如，校准主要是用于收集重要的信息。</p><h4 id="校准-Calibration"><a href="#校准-Calibration" class="headerlink" title="校准(Calibration)"></a>校准(Calibration)</h4><p>使用校准的输入时 要让它保持流畅，用户体验毫不费力。 并且只请求重要信息，避免多次校准。</p><h4 id="隐性反馈-Implicit-Feedback"><a href="#隐性反馈-Implicit-Feedback" class="headerlink" title="隐性反馈(Implicit Feedback)"></a>隐性反馈(Implicit Feedback)</h4><p>隐性主要是用户在和自己产品交互的过程中进行产生的信息，这些信息可以用于改善模型或功能。比如用户的历史行为记录信息，定位信息，当前行为信息等等。比如隐形反馈在个性化的服务中使用是非常有意义。</p><h4 id="明确反馈-Explicit-Feedback"><a href="#明确反馈-Explicit-Feedback" class="headerlink" title="明确反馈(Explicit Feedback)"></a>明确反馈(Explicit Feedback)</h4><p>允许你的产品通过提出关于结果的具体问题而收集信息，直接让用户反馈一些信息，比如不像看到的建议的反馈，直接影响模型效果的一些的反馈，如将某首歌曲加入喜欢列表，标记为喜爱等等。</p><p>明确反馈也分为消极反馈和积极反馈，实际上建议消极反馈优先于积极反馈，积极反馈可以更好地通过隐含信息进行推测，比如当阅读文章时，我给它添加书签或分享了它。因此在设计时不需要同时显示“喜欢”和“不喜欢”，只需要显示“不喜欢”的反馈即可。</p><p>当使用明确反馈时，清楚地描述选项及其结果，使用能解释即将产生什么结果的描述，可能的话提供不同的选项来更好地理解用户的喜。当然，当我选择其中一个选项时，界面应该立即反映我的选择并隐藏与我的喜好所匹配的建议。 因此当用户提供明确反馈时 总是要立即并持久地采取措施。</p><h4 id="修正-Corrections"><a href="#修正-Corrections" class="headerlink" title="修正(Corrections)"></a>修正(Corrections)</h4><p>更正是对结果的优化，让用户有更棒的体验，这种模式也是非常好的。修正同时也是隐性反馈的一种方式，它可以很好的改善机器学习结果。</p>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>产品</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python3中实现函数(方法)范型</title>
    <link href="/2020/04/29/Python3%E4%B8%AD)%E5%AE%9E%E7%8E%B0%E5%87%BD%E6%95%B0(%E6%96%B9%E6%B3%95%E8%8C%83%E5%9E%8B/"/>
    <url>/2020/04/29/Python3%E4%B8%AD)%E5%AE%9E%E7%8E%B0%E5%87%BD%E6%95%B0(%E6%96%B9%E6%B3%95%E8%8C%83%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>最近浏览python官方文档中的new feature中看到一些新的功能，其中有几个功能是非常棒的，在使用python时可能会经常使用。因此进行测试以及记录。其中就包括singledispatch，这个功能好比c++面向对象中，可以存在多个同名的方法，可以根据不同类型的参数自己选择适配的方法。也可以称为范型，重载等。在很多的编程语言中都存在这样的情况，比如swift语言中也类似，通过函数的参数类型和返回类型确定一个函数（函数签名），在函数调用时根据参数的类型选择适配的函数。</p><h2 id="装饰器singledispatch"><a href="#装饰器singledispatch" class="headerlink" title="装饰器singledispatch"></a>装饰器<code>singledispatch</code></h2><p>该装饰器在python3.7中已经存在了，主要是对函数实现范型，官方的描述为：将一个函数转换为 <a href="https://docs.python.org/zh-cn/3/glossary.html#term-single-dispatch" target="_blank" rel="noopener">单分派</a> <a href="https://docs.python.org/zh-cn/3/glossary.html#term-generic-function" target="_blank" rel="noopener">generic function</a>。要定义一个泛型函数，应使用 <code>@singledispatch</code> 装饰器进行装饰。但是必须注意的是<strong>当函数有多个参数时，只作用于第一个参数</strong>。下面通过示例观察：</p><pre><code class="lang-python">from functools import singledispatch@singledispatchdef func(x: int, verbose: bool = True):    if verbose:        print(&quot;The param &#39;x&#39; type: &quot;, type(x))    print(&quot;\t&quot;, x)@func.registerdef _(x: list, verbose: bool = True):    if verbose:        print(&quot;The param &#39;x&#39; type: &quot;, type(x))    for _x in x:        print(&quot;\t&quot;, _x)@func.register()def _(x, verbose: bool = True):    if verbose:        print(&quot;The param &#39;x&#39; type: &quot;, type(x))    for _k, _x in x.items():        print(&quot;\t&quot;, _k, _x)def reg(x: str, verbose: bool = False):    if verbose:        print(print(&quot;The param &#39;x&#39; type: &quot;, type(x)))    print(&quot;---------- register -------------&quot;)    print(&quot;\t&quot;, x)if __name__ == &#39;__main__&#39;:    func.register(reg)    for p in (10, [101, 102, 103], {&quot;a&quot;: 99, &quot;b&quot;: 98, &quot;c&quot;: 97}, &quot;Hello World!&quot;):        func(p)# outputThe param &#39;x&#39; type:  &lt;class &#39;int&#39;&gt;         10The param &#39;x&#39; type:  &lt;class &#39;list&#39;&gt;         101         102         103The param &#39;x&#39; type:  &lt;class &#39;dict&#39;&gt;         a 99         b 98         c 97---------- register -------------         Hello World!</code></pre><p><a id="more"></a>可以看出，不同的参数类型，选择的函数不同。实现方式如上，通过装饰器的方式进行。在编码中通过<code>register</code>来实现分配。在python3.8中对该方法进行了优化，在<code>register</code>中可以传入类型参数，如下：</p><pre><code class="lang-python">@func.register(dict)def _(x, verbose: bool = True):    if verbose:        print(&quot;The param &#39;x&#39; type: &quot;, type(x))    for _k, _x in x.items():        print(&quot;\t&quot;, _k, _x)def reg(x, verbose: bool = False):    if verbose:        print(print(&quot;The param &#39;x&#39; type: &quot;, type(x)))    print(&quot;---------- register -------------&quot;)    print(&quot;\t&quot;, x)func.register(str, reg)</code></pre><h2 id="装饰器singledispatchmethod"><a href="#装饰器singledispatchmethod" class="headerlink" title="装饰器singledispatchmethod"></a>装饰器<code>singledispatchmethod</code></h2><p>该装饰器主要是针对类下面的方法实现单分配，实现和上面<code>singledispatch</code>一样的功能。但是在python3.8中支持对<code>classmethod</code>，<code>staticmethod</code>，<code>abstractmethod</code>等等的嵌套。但是<code>singledispatchmethod</code>必须在最外层进行装饰。必须注意的是，<strong>该装饰器也是根据非<code>self</code>，<code>class</code>的第一个参数有效</strong>。下面是对它的测试：</p><pre><code class="lang-python">class MyClass(object):    @singledispatchmethod    def singe(self, args, verbose=1):        if verbose:            print(&quot;The args type: &quot;, type(args))        raise NotImplementedError    @singe.register    def _(self, args: int, verbose=1):        if verbose:            print(&quot;The args type: &quot;, type(args))        print(&quot;\t&quot;, args)    @singe.register    def _(self, args: dict, verbose=0):        if verbose:            print(&quot;The args type: &quot;, type(args))        for _k, _x in args.items():            print(&quot;\t&quot;, _k, _x)c = MyClass()for p in (10, {&quot;a&quot;: 99, &quot;b&quot;: 98, &quot;c&quot;: 97}, [101, 102, 103]):    c.singe(p)# outputThe args type:  &lt;class &#39;int&#39;&gt;     10     a 99     b 98     c 97The args type:  &lt;class &#39;list&#39;&gt;raise NotImplementedError</code></pre><h2 id="类属性缓存cached-property"><a href="#类属性缓存cached-property" class="headerlink" title="类属性缓存cached_property"></a>类属性缓存<code>cached_property</code></h2><p>该方法在很早以前就已经实现，但是都未纳入到标准库中，该装饰器主要实现类中属性的缓存。再未纳入标准库之前就有很多实现方法，如函数装饰器，类装饰器等等。现在使用只需要从<code>functools</code>中导入<code>cached_property</code>即可。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python3</tag>
      
      <tag>装饰器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【转载】机器学习中样本失衡下的分类评估指标选取指南</title>
    <link href="/2020/04/17/Tour%20of%20Evaluation%20Metrics%20for%20Imbalanced%20Classification/"/>
    <url>/2020/04/17/Tour%20of%20Evaluation%20Metrics%20for%20Imbalanced%20Classification/</url>
    
    <content type="html"><![CDATA[<p>From:<a href="https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/" target="_blank" rel="noopener">https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/</a></p><p>A classifier is only as good as the metric used to evaluate it.</p><p>If you choose the wrong metric to evaluate your models, you are likely to choose a poor model, or in the worst case, be misled about the expected performance of your model.</p><p>Choosing an appropriate metric is challenging generally in applied machine learning, but is particularly difficult for imbalanced classification problems. Firstly, because most of the standard metrics that are widely used assume a balanced class distribution, and because typically not all classes, and therefore, not all prediction errors, are equal for imbalanced classification.</p><p>In this tutorial, you will discover metrics that you can use for imbalanced classification.</p><p>After completing this tutorial, you will know:</p><ul><li>About the challenge of choosing metrics for classification, and how it is particularly difficult when there is a skewed class distribution.</li><li>How there are three main types of metrics for evaluating classifier models, referred to as rank, threshold, and probability.</li><li>How to choose a metric for imbalanced classification if you don’t know where to start.</li></ul><p>Discover SMOTE, one-class classification, cost-sensitive learning, threshold moving, and much more <a href="https://machinelearningmastery.com/imbalanced-classification-with-python/" target="_blank" rel="noopener">in my new book</a>, with 30 step-by-step tutorials and full Python source code.</p><p>Let’s get started.</p><a id="more"></a><h2 id="Tutorial-Overview"><a href="#Tutorial-Overview" class="headerlink" title="Tutorial Overview"></a><strong>Tutorial Overview</strong></h2><p>This tutorial is divided into three parts; they are:</p><ol><li>Challenge of Evaluation Metrics</li><li>Taxonomy of Classifier Evaluation Metrics</li><li>How to Choose an Evaluation Metric</li></ol><h2 id="Challenge-of-Evaluation-Metrics"><a href="#Challenge-of-Evaluation-Metrics" class="headerlink" title="Challenge of Evaluation Metrics"></a><strong>Challenge of Evaluation Metrics</strong></h2><p>An evaluation metric quantifies the performance of a predictive model.</p><p>This typically involves training a model on a dataset, using the model to make predictions on a holdout dataset not used during training, then comparing the predictions to the expected values in the holdout dataset.</p><p>For classification problems, metrics involve comparing the expected class label to the predicted class label or interpreting the predicted probabilities for the class labels for the problem.</p><p>Selecting a model, and even the data preparation methods together are a search problem that is guided by the evaluation metric. Experiments are performed with different models and the outcome of each experiment is quantified with a metric.</p><blockquote><p><em>Evaluation measures play a crucial role in both assessing the classification performance and guiding the classifier modeling.</em></p></blockquote><p>— <a href="https://www.worldscientific.com/doi/abs/10.1142/S0218001409007326" target="_blank" rel="noopener">Classification Of Imbalanced Data: A Review</a>, 2009.</p><p>There are standard metrics that are widely used for evaluating classification predictive models, such as classification accuracy or classification error.</p><p>There are standard metrics that are widely used for evaluating classification predictive models, such as classification accuracy or classification error.</p><p>Standard metrics work well on most problems, which is why they are widely adopted. But all metrics make assumptions about the problem or about what is important in the problem. Therefore an evaluation metric must be chosen that best captures what you or your project stakeholders believe is important about the model or predictions, which makes choosing model evaluation metrics challenging.</p><p>This challenge is made even more difficult when there is a skew in the class distribution. The reason for this is that many of the standard metrics become unreliable or even misleading when classes are imbalanced, or severely imbalanced, such as 1:100 or 1:1000 ratio between a minority and majority class.</p><blockquote><p><em>In the case of class imbalances, the problem is even more acute because the default, relatively robust procedures used for unskewed data can break down miserably when the data is skewed.</em></p></blockquote><p>— Page 187, <a href="https://amzn.to/32K9K6d" target="_blank" rel="noopener">Imbalanced Learning: Foundations, Algorithms, and Applications</a>, 2013.</p><p>For example, reporting classification accuracy for a severely imbalanced classification problem could be dangerously misleading. This is the case if project stakeholders use the results to draw conclusions or plan new projects.</p><blockquote><p><em>In fact, the use of common metrics in imbalanced domains can lead to sub-optimal classification models and might produce misleading conclusions since these measures are insensitive to skewed domains.</em></p></blockquote><p>— <a href="https://arxiv.org/abs/1505.01658" target="_blank" rel="noopener">A Survey of Predictive Modelling under Imbalanced Distributions</a>, 2015.</p><p>Importantly, different evaluation metrics are often required when working with imbalanced classification.</p><p>Unlike standard evaluation metrics that treat all classes as equally important, imbalanced classification problems typically rate classification errors with the minority class as more important than those with the majority class. As such performance metrics may be needed that focus on the minority class, which is made challenging because it is the minority class where we lack observations required to train an effective model.</p><blockquote><p><em>The main problem of imbalanced data sets lies on the fact that they are often associated with a user preference bias towards the performance on cases that are poorly represented in the available data sample.</em></p></blockquote><p>— <a href="https://arxiv.org/abs/1505.01658" target="_blank" rel="noopener">A Survey of Predictive Modelling under Imbalanced Distributions</a>, 2015.</p><p>Now that we are familiar with the challenge of choosing a model evaluation metric, let’s look at some examples of different metrics from which we might choose.</p><h2 id="Taxonomy-of-Classifier-Evaluation-Metrics"><a href="#Taxonomy-of-Classifier-Evaluation-Metrics" class="headerlink" title="Taxonomy of Classifier Evaluation Metrics"></a><strong>Taxonomy of Classifier Evaluation Metrics</strong></h2><p>There are tens of metrics to choose from when evaluating classifier models, and perhaps hundreds, if you consider all of the pet versions of metrics proposed by academics.</p><p>In order to get a handle on the metrics that you could choose from, we will use a taxonomy proposed by <a href="http://personales.upv.es/ceferra/" target="_blank" rel="noopener">Cesar Ferri</a>, et al. in their 2008 paper titled “<a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687" target="_blank" rel="noopener">An Experimental Comparison Of Performance Measures For Classification</a>.” It was also adopted in the 2013 book titled “<a href="https://amzn.to/32K9K6d" target="_blank" rel="noopener">Imbalanced Learning</a>” and I think proves useful.</p><p>We can divide evaluation metrics into three useful groups; they are:</p><ol><li>Threshold Metrics</li><li>Ranking Metrics</li><li>Probability Metrics.</li></ol><p>This division is useful because the top metrics used by practitioners for classifiers generally, and specifically imbalanced classification, fit into the taxonomy neatly.</p><blockquote><p><em>Several machine learning researchers have identified three families of evaluation metrics used in the context of classification. These are the threshold metrics (e.g., accuracy and F-measure), the ranking methods and metrics (e.g., receiver operating characteristics (ROC) analysis and AUC), and the probabilistic metrics (e.g., root-mean-squared error).</em></p></blockquote><p>— Page 189, <a href="https://amzn.to/32K9K6d" target="_blank" rel="noopener">Imbalanced Learning: Foundations, Algorithms, and Applications</a>, 2013.</p><p>Let’s take a closer look at each group in turn.</p><h3 id="Threshold-Metrics-for-Imbalanced-Classification"><a href="#Threshold-Metrics-for-Imbalanced-Classification" class="headerlink" title="Threshold Metrics for Imbalanced Classification"></a><strong>Threshold Metrics for Imbalanced Classification</strong></h3><p>Threshold metrics are those that quantify the classification prediction errors.</p><p>That is, they are designed to summarize the fraction, ratio, or rate of when a predicted class does not match the expected class in a holdout dataset.</p><blockquote><p><em>Metrics based on a threshold and a qualitative understanding of error […] These measures are used when we want a model to minimise the number of errors.</em></p></blockquote><p>— <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687" target="_blank" rel="noopener">An Experimental Comparison Of Performance Measures For Classification</a>, 2008.</p><p>Perhaps the most widely used threshold metric is classification accuracy.</p><ul><li><strong>Accuracy</strong> = Correct Predictions / Total Predictions</li></ul><p>And the complement of classification accuracy called classification error.</p><ul><li><strong>Error</strong> = Incorrect Predictions / Total Predictions</li></ul><p>Although widely used, classification accuracy is almost universally inappropriate for imbalanced classification. The reason is, a high accuracy (or low error) is achievable by a no skill model that only predicts the majority class.</p><p>For more on the failure of classification accuracy, see the tutorial:</p><ul><li><a href="https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/" target="_blank" rel="noopener">Failure of Classification Accuracy for Imbalanced Class Distributions</a></li></ul><p>For imbalanced classification problems, the majority class is typically referred to as the negative outcome (e.g. such as “<em>no change</em>” or “<em>negative test result</em>“), and the minority class is typically referred to as the positive outcome (e.g. “<em>change</em>” or “<em>positive test result</em>“).</p><ul><li><strong>Majority Class</strong>: Negative outcome, class 0.</li><li><strong>Minority Class</strong>: Positive outcome, class 1.</li></ul><p>Most threshold metrics can be best understood by the terms used in a confusion matrix for a binary (two-class) classification problem. This does not mean that the metrics are limited for use on binary classification; it is just an easy way to quickly understand what is being measured.</p><p>The confusion matrix provides more insight into not only the performance of a predictive model but also which classes are being predicted correctly, which incorrectly, and what type of errors are being made. In this type of confusion matrix, each cell in the table has a specific and well-understood name, summarized as follows:</p><div class="table-container"><table><thead><tr><th></th><th>Positive Prediction</th><th>Negative Prediction</th></tr></thead><tbody><tr><td>Positive Class</td><td>True Positive (TP)</td><td>False Negative (FN)</td></tr><tr><td>Negative Class</td><td>False Positive (FP)</td><td>True Negative (TN)</td></tr></tbody></table></div><p>There are two groups of metrics that may be useful for imbalanced classification because they focus on one class; they are sensitivity-specificity and precision-recall.</p><h4 id="Sensitivity-Specificity-Metrics"><a href="#Sensitivity-Specificity-Metrics" class="headerlink" title="Sensitivity-Specificity Metrics"></a><strong>Sensitivity-Specificity Metrics</strong></h4><p>Sensitivity refers to the true positive rate and summarizes how well the positive class was predicted.</p><ul><li><strong>Sensitivity</strong> = TruePositive / (TruePositive + FalseNegative)</li></ul><p>Specificity is the complement to sensitivity, or the true negative rate, and summarises how well the negative class was predicted.</p><ul><li><strong>Specificity</strong> = TrueNegative / (FalsePositive + TrueNegative)</li></ul><p>For imbalanced classification, the sensitivity might be more interesting than the specificity.</p><p>Sensitivity and Specificity can be combined into a single score that balances both concerns, called the <a href="https://machinelearningmastery.com/arithmetic-geometric-and-harmonic-means-for-machine-learning/" target="_blank" rel="noopener">geometric mean</a> or G-Mean.</p><ul><li><strong>G-Mean</strong> = sqrt(Sensitivity * Specificity)</li></ul><h4 id="Precision-Recall-Metrics"><a href="#Precision-Recall-Metrics" class="headerlink" title="Precision-Recall Metrics"></a><strong>Precision-Recall Metrics</strong></h4><p>Precision summarizes the fraction of examples assigned the positive class that belong to the positive class.</p><ul><li><strong>Precision</strong> = TruePositive / (TruePositive + FalsePositive)</li></ul><p>Recall summarizes how well the positive class was predicted and is the same calculation as sensitivity.</p><ul><li><strong>Recall</strong> = TruePositive / (TruePositive + FalseNegative)</li></ul><p>Precision and recall can be combined into a single score that seeks to balance both concerns, called the F-score or the F-measure.</p><ul><li><strong>F-Measure</strong> = (2 <em> Precision </em> Recall) / (Precision + Recall)</li></ul><p>The F-Measure is a popular metric for imbalanced classification.</p><p>The Fbeta-measure measure is an abstraction of the F-measure where the balance of precision and recall in the calculation of the <a href="https://machinelearningmastery.com/arithmetic-geometric-and-harmonic-means-for-machine-learning/" target="_blank" rel="noopener">harmonic mean</a> is controlled by a coefficient called <em>beta</em>.</p><ul><li><strong>Fbeta-Measure</strong> = ((1 + beta^2) <em> Precision </em> Recall) / (beta^2 * Precision + Recall)</li></ul><p>For more on precision, recall and F-measure for imbalanced classification, see the tutorial:</p><ul><li><a href="https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/" target="_blank" rel="noopener">How to Calculate Precision, Recall, and F-Measure for Imbalanced Classification</a></li></ul><h4 id="Additional-Threshold-Metrics"><a href="#Additional-Threshold-Metrics" class="headerlink" title="Additional Threshold Metrics"></a><strong>Additional Threshold Metrics</strong></h4><p>These are probably the most popular metrics to consider, although many others do exist. To give you a taste, these include Kappa, Macro-Average Accuracy, Mean-Class-Weighted Accuracy, Optimized Precision, Adjusted Geometric Mean, Balanced Accuracy, and more.</p><p>Threshold metrics are easy to calculate and easy to understand.</p><p>One limitation of these metrics is that they assume that the class distribution observed in the training dataset will match the distribution in the test set and in real data when the model is used to make predictions. This is often the case, but when it is not the case, the performance can be quite misleading.</p><blockquote><p><em>An important disadvantage of all the threshold metrics discussed in the previous section is that they assume full knowledge of the conditions under which the classifier will be deployed. In particular, they assume that the class imbalance present in the training set is the one that will be encountered throughout the operating life of the classifier</em></p></blockquote><p>— Page 196, <a href="https://amzn.to/32K9K6d" target="_blank" rel="noopener">Imbalanced Learning: Foundations, Algorithms, and Applications</a>, 2013.</p><p>Ranking metrics don’t make any assumptions about class distributions.</p><h3 id="Ranking-Metrics-for-Imbalanced-Classification"><a href="#Ranking-Metrics-for-Imbalanced-Classification" class="headerlink" title="Ranking Metrics for Imbalanced Classification"></a><strong>Ranking Metrics for Imbalanced Classification</strong></h3><p>Rank metrics are more concerned with evaluating classifiers based on how effective they are at separating classes.</p><blockquote><p><em>Metrics based on how well the model ranks the examples […] These are important for many applications […] where classifiers are used to select the best n instances of a set of data or when good class separation is crucial.</em></p></blockquote><p>— <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687" target="_blank" rel="noopener">An Experimental Comparison Of Performance Measures For Classification</a>, 2008.</p><p>These metrics require that a classifier predicts a score or a probability of class membership.</p><p>From this score, different thresholds can be applied to test the effectiveness of classifiers. Those models that maintain a good score across a range of thresholds will have good class separation and will be ranked higher.</p><blockquote><p><em>… consider a classifier that gives a numeric score for an instance to be classified in the positive class. Therefore, instead of a simple positive or negative prediction, the score introduces a level of granularity</em></p></blockquote><p>– Page 53, <a href="https://amzn.to/307Xlva" target="_blank" rel="noopener">Learning from Imbalanced Data Sets</a>, 2018.</p><p>The most commonly used ranking metric is the ROC Curve or ROC Analysis.</p><p>ROC is an acronym that means Receiver Operating Characteristic and summarizes a field of study for analyzing binary classifiers based on their ability to discriminate classes.</p><p>A ROC curve is a diagnostic plot for summarizing the behavior of a model by calculating the false positive rate and true positive rate for a set of predictions by the model under different thresholds.</p><p>The true positive rate is the recall or sensitivity.</p><ul><li><strong>TruePositiveRate</strong> = TruePositive / (TruePositive + FalseNegative)</li></ul><p>The false positive rate is calculated as:</p><ul><li><strong>FalsePositiveRate</strong> = FalsePositive / (FalsePositive + TrueNegative)</li></ul><p>Each threshold is a point on the plot and the points are connected to form a curve. A classifier that has no skill (e.g. predicts the majority class under all thresholds) will be represented by a diagonal line from the bottom left to the top right.</p><p>Any points below this line have worse than no skill. A perfect model will be a point in the top right of the plot.</p><p><img src="./uploads/figure_roc_cure" srcset="/img/loading.gif" alt="Depiction of a ROC Curve"></p><p>The ROC Curve is a helpful diagnostic for one model.</p><p>The area under the ROC curve can be calculated and provides a single score to summarize the plot that can be used to compare models.</p><p>A no skill classifier will have a score of 0.5, whereas a perfect classifier will have a score of 1.0.</p><ul><li><strong>ROC AUC</strong> = ROC Area Under Curve</li></ul><p>Although generally effective, the ROC Curve and ROC AUC can be optimistic under a severe class imbalance, especially when the number of examples in the minority class is small.</p><p>An alternative to the ROC Curve is the precision-recall curve that can be used in a similar way, although focuses on the performance of the classifier on the minority class.</p><p>Again, different thresholds are used on a set of predictions by a model, and in this case, the precision and recall are calculated. The points form a curve and classifiers that perform better under a range of different thresholds will be ranked higher.</p><p>A no-skill classifier will be a horizontal line on the plot with a precision that is proportional to the number of positive examples in the dataset. For a balanced dataset this will be 0.5. A perfect classifier is represented by a point in the top right.</p><p><img src="./uploads/precision_recall_curve" srcset="/img/loading.gif" alt="Depiction of a Precision-Recall Curve"></p><p>Like the ROC Curve, the Precision-Recall Curve is a helpful diagnostic tool for evaluating a single classifier but challenging for comparing classifiers.</p><p>And like the ROC AUC, we can calculate the area under the curve as a score and use that score to compare classifiers. In this case, the focus on the minority class makes the Precision-Recall AUC more useful for imbalanced classification problems.</p><ul><li><strong>PR AUC</strong> = Precision-Recall Area Under Curve</li></ul><p>There are other ranking metrics that are less widely used, such as modification to the ROC Curve for imbalanced classification and cost curves.</p><p>For more on ROC curves and precision-recall curves for imbalanced classification, see the tutorial:</p><ul><li><a href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/" target="_blank" rel="noopener">ROC Curves and Precision-Recall Curves for Imbalanced Classification</a></li></ul><h3 id="Probabilistic-Metrics-for-Imbalanced-Classification"><a href="#Probabilistic-Metrics-for-Imbalanced-Classification" class="headerlink" title="Probabilistic Metrics for Imbalanced Classification"></a><strong>Probabilistic Metrics for Imbalanced Classification</strong></h3><p>Probabilistic metrics are designed specifically to quantify the uncertainty in a classifier’s predictions.</p><p>These are useful for problems where we are less interested in incorrect vs. correct class predictions and more interested in the uncertainty the model has in predictions and penalizing those predictions that are wrong but highly confident.</p><blockquote><p><em>Metrics based on a probabilistic understanding of error, i.e. measuring the deviation from the true probability […] These measures are especially useful when we want an assessment of the reliability of the classifiers, not only measuring when they fail but whether they have selected the wrong class with a high or low probability.</em></p></blockquote><p>— <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687" target="_blank" rel="noopener">An Experimental Comparison Of Performance Measures For Classification</a>, 2008.</p><p>Evaluating a model based on the predicted probabilities requires that the probabilities are calibrated.</p><p>Some classifiers are trained using a probabilistic framework, such as maximum likelihood estimation, meaning that their probabilities are already calibrated. An example would be logistic regression.</p><p>Many nonlinear classifiers are not trained under a probabilistic framework and therefore require their probabilities to be calibrated against a dataset prior to being evaluated via a probabilistic metric. Examples might include support vector machines and k-nearest neighbors.</p><p>Perhaps the most common metric for evaluating predicted probabilities is log loss for binary classification (or the negative log likelihood), or known more generally as <a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" target="_blank" rel="noopener">cross-entropy</a>.</p><p>For a binary classification dataset where the expected values are y and the predicted values are yhat, this can be calculated as follows:</p><ul><li><strong>LogLoss</strong> = -((1 – y) <em> log(1 – yhat) + y </em> log(yhat))</li></ul><p>The score can be generalized to multiple classes by simply adding the terms; for example:</p><ul><li><strong>LogLoss</strong> = -( sum c in C y_c * log(yhat_c))</li></ul><p>The score summarizes the average difference between two probability distributions. A perfect classifier has a log loss of 0.0, with worse values being positive up to infinity.</p><p>Another popular score for predicted probabilities is the Brier score.</p><p>The benefit of the Brier score is that it is focused on the positive class, which for imbalanced classification is the minority class. This makes it more preferable than log loss, which is focused on the entire probability distribution.</p><p>The Brier score is calculated as the mean squared error between the expected probabilities for the positive class (e.g. 1.0) and the predicted probabilities. Recall that the mean squared error is the average of the squared differences between the values.</p><ul><li><strong>BrierScore</strong> = 1/N * Sum i to N (yhat_i – y_i)^2</li></ul><p>A perfect classifier has a Brier score of 0.0. Although typically described in terms of binary classification tasks, the Brier score can also be calculated for multiclass classification problems.</p><p>The differences in Brier score for different classifiers can be very small. In order to address this problem, the score can be scaled against a reference score, such as the score from a no skill classifier (e.g. predicting the probability distribution of the positive class in the training dataset).</p><p>Using the reference score, a Brier Skill Score, or BSS, can be calculated where 0.0 represents no skill, worse than no skill results are negative, and the perfect skill is represented by a value of 1.0.</p><ul><li><strong>BrierSkillScore</strong> = 1 – (BrierScore / BrierScore_ref)</li></ul><p>Although popular for balanced classification problems, probability scoring methods are less widely used for classification problems with a skewed class distribution.</p><p>For more on probabilistic metrics for imbalanced classification, see the tutorial:</p><ul><li><a href="https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/" target="_blank" rel="noopener">A Gentle Introduction to Probability Metrics for Imbalanced Classification</a></li></ul><h2 id="How-to-Choose-an-Evaluation-Metric"><a href="#How-to-Choose-an-Evaluation-Metric" class="headerlink" title="How to Choose an Evaluation Metric"></a><strong>How to Choose an Evaluation Metric</strong></h2><p>There is an enormous number of model evaluation metrics to choose from.</p><p>Given that choosing an evaluation metric is so important and there are tens or perhaps hundreds of metrics to choose from, what are you supposed to do?</p><blockquote><p><em>The correct evaluation of learned models is one of the most important issues in pattern recognition.</em></p></blockquote><p>— <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687" target="_blank" rel="noopener">An Experimental Comparison Of Performance Measures For Classification</a>, 2008.</p><p>Perhaps the best approach is to talk to project stakeholders and figure out what is important about a model or set of predictions. Then select a few metrics that seem to capture what is important, then test the metric with different scenarios.</p><p>A scenario might be a mock set of predictions for a test dataset with a skewed class distribution that matches your problem domain. You can test what happens to the metric if a model predicts all the majority class, all the minority class, does well, does poorly, and so on. A few small tests can rapidly help you get a feeling for how the metric might perform.</p><p>Another approach might be to perform a literature review and discover what metrics are most commonly used by other practitioners or academics working on the same general type of problem. This can often be insightful, but be warned that some fields of study may fall into groupthink and adopt a metric that might be excellent for comparing large numbers of models at scale, but terrible for model selection in practice.</p><p>Still have no idea?</p><p>Here are some first-order suggestions:</p><ul><li><strong>Are you predicting probabilities?</strong></li><li><ul><li><strong>Do you need class labels?</strong></li><li><ul><li><strong>Is the positive class more important?</strong></li><li><ul><li>Use Precision-Recall AUC</li></ul></li><li><strong>Are both classes important?</strong></li><li><ul><li>Use ROC AUC</li></ul></li></ul></li><li><strong>Do you need probabilities?</strong></li><li><ul><li>Use Brier Score and Brier Skill Score</li></ul></li></ul></li><li><strong>Are you predicting class labels?</strong></li><li><ul><li><strong>Is the positive class more important?</strong></li><li><ul><li><strong>Are False Negatives and False Positives Equally Important?</strong></li><li><ul><li>Use F1-Measure</li></ul></li><li><strong>Are False Negatives More Important?</strong></li><li><ul><li>Use F2-Measure</li></ul></li><li><strong>Are False Positives More Important?</strong></li><li><ul><li>Use F0.5-Measure</li></ul></li></ul></li><li><strong>Are both classes important?</strong></li><li><ul><li><strong>Do you have &lt; 80%-90% Examples for the Majority Class?</strong> </li><li><ul><li>Use Accuracy</li></ul></li><li><strong>Do you have &gt; 80%-90% Examples for the Majority Class?</strong> </li><li><ul><li>Use G-Mean</li></ul></li></ul></li></ul></li></ul><p>These suggestions take the important case into account where we might use models that predict probabilities, but require crisp class labels. This is an important class of problems that allow the operator or implementor to choose the threshold to trade-off misclassification errors. In this scenario, error metrics are required that consider all reasonable thresholds, hence the use of the area under curve metrics.</p><p>We can transform these suggestions into a helpful template.</p><p><img src="./uploads/how_to_choose_imbalanced" srcset="/img/loading.gif" alt="How to Choose a Metric for Imbalanced Classification"></p><h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a><strong>Further Reading</strong></h2><p>This section provides more resources on the topic if you are looking to go deeper.</p><h3 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a><strong>Papers</strong></h3><ul><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687" target="_blank" rel="noopener">An Experimental Comparison Of Performance Measures For Classification</a>, 2008.</li><li><a href="https://www.worldscientific.com/doi/abs/10.1142/S0218001409007326" target="_blank" rel="noopener">Classification Of Imbalanced Data: A Review</a>, 2009.</li><li><a href="https://arxiv.org/abs/1505.01658" target="_blank" rel="noopener">A Survey of Predictive Modelling under Imbalanced Distributions</a>, 2015.</li></ul><h3 id="Books"><a href="#Books" class="headerlink" title="Books"></a><strong>Books</strong></h3><ul><li>Chapter 8 Assessment Metrics For Imbalanced Learning, <a href="https://amzn.to/32K9K6d" target="_blank" rel="noopener">Imbalanced Learning: Foundations, Algorithms, and Applications</a>, 2013.</li><li><a href="https://amzn.to/307Xlva" target="_blank" rel="noopener">Chapter 3 Performance Measures, Learning from Imbalanced Data Sets</a>, 2018.</li></ul><h3 id="Articles"><a href="#Articles" class="headerlink" title="Articles"></a><strong>Articles</strong></h3><ul><li><a href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank" rel="noopener">Precision and recall, Wikipedia</a>.</li><li><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" target="_blank" rel="noopener">Sensitivity and specificity, Wikipedia</a>.</li><li><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">Receiver operating characteristic, Wikipedia</a>.</li><li><a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">Cross entropy, Wikipedia</a>.</li><li><a href="https://en.wikipedia.org/wiki/Brier_score" target="_blank" rel="noopener">Brier score, Wikipedia</a>.</li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h2><p>In this tutorial, you discovered metrics that you can use for imbalanced classification.</p><p>Specifically, you learned:</p><ul><li>About the challenge of choosing metrics for classification, and how it is particularly difficult when there is a skewed class distribution.</li><li>How there are three main types of metrics for evaluating classifier models, referred to as rank, threshold, and probability.</li><li>How to choose a metric for imbalanced classification if you don’t know where to start.</li></ul>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>imbalance</tag>
      
      <tag>metrics</tag>
      
      <tag>evaluation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>改善(今井正明)读书笔记</title>
    <link href="/2020/01/20/%E6%94%B9%E5%96%84(%E4%BB%8A%E4%BA%95%E6%AD%A3%E6%98%8E)%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <url>/2020/01/20/%E6%94%B9%E5%96%84(%E4%BB%8A%E4%BA%95%E6%AD%A3%E6%98%8E)%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>改善（KaiZen）是世界著名的质量管理大师，改善思想之父今井正明的代表作，改善成为了一种文化，一种生活方式。它是积小成多，量变引起质变的一种体现。下面是整书的梳理并结合自己的思考梳理关于改善的思维导图：</p><p><img src="/uploads/gaizen_mind_pic.png" srcset="/img/loading.gif" alt="改善整书的思维导图"></p><p>改善是一种理念，它包含了突变，渐变的过程，是基于现状之上源源不断的进行微小的改进，改善不仅包括了改进，同时也包括创新。改善是永恒的存在，因此是永无止境的。改善在不同的岗位上存在一定的差异，如普通员工，管理人员在改善上可以是下面的方式，但是改善始终是自下而上和自上而下的双向过程。<a id="more"></a></p><p><img src="/uploads/改善.png" srcset="/img/loading.gif" alt="各个岗位改善实施的不同比例" style="zoom:80%;" /></p><p>竞争的驱动力是价格，质量和服务，但是在日本，竞争的本质还是竞争本身。如果将利润看作企业成功的准则，那么就不难理解一个公司几十年没有任何变化。如果公司之间激烈的竞争改善的实力，那么这样的公司会持续的改进，处于永远进取的状态。</p><p>改善和创新是两种不同的思维导向，改善是以过程为导向，而创新是以目标为导向。也是因为目标导向的不同，两者的思想不同，结果不同，发展不同。西方管理者是信仰创新的，因此很多新的技术，产品都出自于西方技术。 创新被看成是由技术突破，应用的最新管理理念而发生的中带改变。创新是迅速的，非常大的变动；而改善是温和的，微妙的，连续的过程。创新是跳跃性的发展，同时也是不稳定的；而改善是持续性的，稳定的。创新是严重的个人主义，个人思想；而改善是团队型的，系统的方式。改善的针对的方向是人，而创新主要是针对某个技术。</p><p>在作者的观点中，任何事物或系统在存在开始，就开始衰败（帕金森定律），因此任何时候都应该维持努力，改善；即使是创新之后都应该用一系列改善去维护和改进创新的成果。非常明显的是建立标准-实施-改善标准-再实施的过程，在这不断迭代的过程中，相互优化，最终不断的增强自身的竞争力以及产品质量等。</p><p>改善和创新两种思维在实用性上也有一定的区别，改善哲学适用于增长较慢的经济；而创新适合增长较快的经济。比如当我门增加销量10%时非常苦难，但是我们减少10%的成本可能更加容易，这时改善在整个过程中是非常重要的。</p><p><strong>最后总结一句话</strong>：每个人应该试着改变自己，虽然不是每一个人都是天才，但是每个人都可以逐渐的强大，即使是一个普通的人。</p>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>思考</tag>
      
      <tag>note</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>生命形态的思考</title>
    <link href="/2019/10/21/%E7%94%9F%E5%91%BD%E5%BD%A2%E6%80%81%E7%9A%84%E6%80%9D%E8%80%83/"/>
    <url>/2019/10/21/%E7%94%9F%E5%91%BD%E5%BD%A2%E6%80%81%E7%9A%84%E6%80%9D%E8%80%83/</url>
    
    <content type="html"><![CDATA[<p>生命的定义在我们的意识中非常简单，生命是基于生物的，生物又必须依赖于生命的存在，从百科上主要解释为2点：1.生物生存，生物所具备活动，意识等的能力；2.指事物皆以生存的根本条件。如果从严格上来讲，目前的定较为官方的应该是下面的定义：</p><blockquote><p><strong>生命</strong>泛指一类具有稳定的<a href="https://zh.wikipedia.org/wiki/物质" target="_blank" rel="noopener">物质</a>和<a href="https://zh.wikipedia.org/wiki/能量" target="_blank" rel="noopener">能量</a><a href="https://zh.wikipedia.org/wiki/代谢" target="_blank" rel="noopener">代谢</a>现象并且能回应<a href="https://zh.wikipedia.org/wiki/刺激" target="_blank" rel="noopener">刺激</a>、能进行自我复制（<a href="https://zh.wikipedia.org/wiki/繁殖" target="_blank" rel="noopener">繁殖</a>）的半开放物质系统。简单来说，也就是具有生命机制的物体。生命个体一定会经历<a href="https://zh.wikipedia.org/wiki/出生" target="_blank" rel="noopener">出生</a>、<a href="https://zh.wikipedia.org/wiki/成长" target="_blank" rel="noopener">成长</a>、<a href="https://zh.wikipedia.org/wiki/衰老" target="_blank" rel="noopener">衰老</a>和<a href="https://zh.wikipedia.org/wiki/死亡" target="_blank" rel="noopener">死亡</a>。</p></blockquote><p>其实，生命是没有准确定义的，没有公认的定义，不同的科学家都有自己的各种定义。那么生命的形态也就存在了很多的不同的理解！目前我们一般认识中，生命的形态有🐒动物，🌳植物，👬人类，细菌🦠，微生物，病毒🦠等这些均认为是有生命的。那么生命有其他形态吗？</p><h2 id="非传统生命"><a href="#非传统生命" class="headerlink" title="非传统生命"></a>非传统生命</h2><p>如今科技发非常先进，人工智能已经逐渐在我们的生活中接触或使用。比如一个人工智能的机器人，它是一个具有生命的“生物”吗？如果从传统的定义上来说好像是，也好像不是。但是一般来说具有自我意识的一个机器人，大家可能会当作一个生命。它能，自我思考，自我进化，也会出生（人类创造了它），可能也会死亡（毁灭它），那么它应该算作一个生命。</p><p>在我们的工作中，把很多事物都赋予了生命，比如自己的产品，自己的作品等，它（产品或作品，甚至其他）的过程是通过我们自己来控制，它也存在出生和死亡，成长和衰退等。</p><p>在我们的生活中，类似这样“生命”很多，但是这些都还是在我们意识中的生命形态。并没有跳出传统的生命理念，对生命的认识还是较为固化。可以假设外星生命的形态是什么样子呢？它们一定是有原子，分子组成吗？它们一定是会活动吗？它们的意识和我们所认识的“意识”是一样的吗？</p><h2 id="畅想生命形态"><a href="#畅想生命形态" class="headerlink" title="畅想生命形态"></a>畅想生命形态</h2><p>其实我们可以大胆的想象一下，生命形态不一定一定是目前我们所认识的“生命体”。在现实中，我们是观察不到部分生命的，如细菌。那么是不是也存在一些生命形态，我们是无法感受到或观察到的呢？特别对于外星生命，其他星球的环境和我们所在的地球不同，我们是在地球这种环境下出现的生命体，其他星球的的环境下可能也会出现所属那个星球的生命体。它们所处的环境造就了它们是以另外一种形态存在，只是我们不能感受到它们或它们也无法感受到我们，它们与我们的生命形态完全不同，或许它们（外星生命体）并不是由原子，分子组成，生命的形式和我们大相径庭。如果是这样，那么必然每个星球的生命形态都不一样，彼此隔离，似乎像平行空间 的我们，彼此没有交集。</p><p>上面只是我的假象，但是也不是没有这种可能。目前我们在宇宙的探索中，我们都是基于地球生命的历史和形态来找到和我们类似的生命。甚至对于地球的研究，来探索其他星球的构成，环境等。宇宙中这么多大小星球，它们存在的意义是什么呢？还是根本没有意义？而这些星球是一个生命体吗？我想这个问题科学家们或许能回答。</p><h2 id="宇宙生命"><a href="#宇宙生命" class="headerlink" title="宇宙生命"></a>宇宙生命</h2><p>生命肯定是多种形态的，但宇宙中的星球是否是一个生命体我们不得而知。假设各个星球也是一个生命体，按照我们对生命的定义，生命有出生（起源），有成长过程，有死亡（毁灭），应用到星球上好像也是正确的。但是星球有无意识，我们不得而知，或者说它们的意识和我们的意识完全不同一个形态。如果星球也是有生命的，那我们只是地球这个生命体内的一个非常小的“微生物”，我们的生命过程或许只是为它（地球）提供着什么？这样去想象的话，那么整个宇宙都是有生命的。只是每个生命体的形态不同，我们只是宇宙生命体中的附着在星球生命体上的某个异形态生命体。就像微生物附着在人体内一样。那么，微生物能否理解和它的宿主就像我们能否理解地球一样，我们可以研究它，破坏它，是否能够完全理解它的生命形态呢🤔？</p><p>都说每个人都有存在的意义，我认为在宇宙中也同样适用。每个星球，星球上每个个体都有它存在的意义。那我们对于宇宙和地球存在的意义又是什么呢？</p>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>思考</tag>
      
      <tag>生命形态</tag>
      
      <tag>意识</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>学会静(Peace,安静,平静)</title>
    <link href="/2019/08/20/%E5%AD%A6%E4%BC%9A%E9%9D%99(Peace,%20%E5%AE%89%E9%9D%99,%20%E5%B9%B3%E9%9D%99)/"/>
    <url>/2019/08/20/%E5%AD%A6%E4%BC%9A%E9%9D%99(Peace,%20%E5%AE%89%E9%9D%99,%20%E5%B9%B3%E9%9D%99)/</url>
    
    <content type="html"><![CDATA[<blockquote><p>佛说，静能生慧。<br>诗人说，沉静最美。<br>情种说，岁月静好。<br>智者说，静水流深。<br>甘地说，寂静是我心底最强大的声音。</p></blockquote><p>在当下，各方面都是迅速的。科技发展迅速，生活节奏加快，社会变得浮躁起来，在这样的大环境下，太多的人都在这样快节奏下生活。然而，我们更需要“缓慢”下来，不断的平静，思考，总结，学会在快节奏中的慢生活，快节奏下的慢策略等，将有限的时间用在重要，更具有价值的事情上。而静的人生哲学可以帮助我们。</p><p>静是一种心境，一些人在一定程度上寻求内心的平静，在动的过程中寻求静是一种多么高的境界（动与静的哲学），他们无时无刻不在思考，行动，改变；就像上面所说的静能生慧，再明确不过了。非淡泊无以明志，非宁静无以志远，不就是说的静（宁静，安静）的心态是如此重要吗！</p><p>静是一种品质，静不是平淡，更不是平庸，是具有一种非常丰富内涵品质的幽远。犹如山之宏伟，犹如特立独行，犹如泰然自若的至高境界，它是人一生中高贵的品质。</p><p>静是一种生活，不急，不燥，不争。在恬静的时间里，静修，沁养，生活更有格调，更具有自己的生活特色。它也是极简的一种生活方式，让时间缓慢下来，充分的享受生活。它也是慢生活的一种体现，并不是让我们放弃什么或消极的态度，反而是更加的积极奋斗，非常健康的心态，是另外一种生活、心灵旅行的方式。</p><p>静可以让我们更理性的思考。一般，学习时都会在一个比较安静，平静的环境下进行，因为嘈杂的环境不利于我们思考；而在社会中亦是如此。一个浮躁的人怎么能够认真思考！而如今浮躁的事物太多，比如网络直播，小视频，各种朋友圈，毒“鸡汤”等，无时无刻不影响着我们。如果能够控制自己，使得自己能静下来，思考一部分信息的真实性，它的价值等，这样对自己非常有益。</p><p>静可以是我们更具有创造力，有时静（平静）可以让我们更高效。如上所述，静可以让我们更理性的思考，因此在工作或生活中，突然遇到某个问题，需要安静的思考，而不是马上进行着手行动。比如，在工作中接到某个项目，需要冷静的思考项目的整个方案，架构，实施，计划等各个方面的问题，而不是浮躁的，无全局性和无计划性的马上开展。特别是现在互联网盛行的情况下，我们更多接触的是一些零碎化，片段化的信息；某些小的问题在网络上可能有“答案”。因此，我们常常借助互联网，而忽略了整体性思维和思考的过程，特别是对学生（小学，中学甚至大学）的害处特别大。我在工作中遇到很多次这样的情况，比如某个程序的bug或错误，需要自己反复的思考，而不是马上借助互联网来寻求解决方案；再比如，学习某个新的技术，更好的方式先看一本该技术相关纸质的书，对局部或深入的问题可以通过互联网来参考或解决，书有自己的整体思维结构，能帮助我们更加整体性的了解该技术。</p><p>一个好的环境才能产生好产品，事物。丰田现有的成就绝不是在浮躁，躁动的环境下产生；而浮躁的环境也不会出现像爱因斯坦，霍金等这类人物。</p><p>好的环境是如此的重要，即使在浮躁的环境下，静（安静，平静）也能使得我们达到事半功倍的效果，使得我们做事更加的高效，也使得我们更具有创造力。</p><p>静可以认识自己，关注自己有什么，而不是只关注别人有什么。当一个人静下来的时候，多多少少会思考自己的一言一行，对自己的内心拷问，反思。这样可以更加的了解自己需要什么，从而改变自己，达到目标。如今流行的朋友圈，都是光鲜亮丽一面，往往我们只关注了别人有什么，做什么，而忽略了自己有什么。无形之中，生活在了别人的生活里，缺乏了自己的光环。若若人人都能对自己反思，对整个社会反思，那么是一个多么“可怕”的情况，在反思中不段改善，改变。</p><p>人生主要忙于两件事，忙着生和忙着死（肖生克的救赎）。正确的认识自己是多重要，多么有价值。静不就是我们最好的方式吗！</p><p>当然，静也是要取决于场景和方式的。静更多是思想上的，在行为上和静没有关系。不是所有的场景都需要静，需要视状况而定。<a id="more"></a></p>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>静</tag>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>boosting系列之CatBoost算法详解</title>
    <link href="/2019/08/04/Boosting%E7%AE%97%E6%B3%95%E4%B9%8BCatBoost%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <url>/2019/08/04/Boosting%E7%AE%97%E6%B3%95%E4%B9%8BCatBoost%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>CatBoost是由俄罗斯的搜索公司Yandex提出的一类boosting集成学习算法，该算法主要是解决类别型特征的学习，可以直接对字符型类别特征进行处理和学习，使用非常简单，实用。因此算法名称也叫做CatBoost（categorical和boosting两者的结合）,该算法支持GPU，以及训练过程可视化等，支持Python，R等建模，也支持其他语言的部署。官方的论文结构如下，本文主要对3，4两个部分进行说明。</p><pre><code class="lang-tex">---- 1.Introduction---- 2.Background---- 3.Categorical feature--------Target statistics: Greedy TS, Holdout TS, Leave-one-out TS, Ordered TS---- 4.Prediction shift and ordered boosting--------Prediction shift--------Ordered boosting---- 5.Practical implementation of ordered boosting</code></pre><h2 id="Categorical-features-类别特征"><a href="#Categorical-features-类别特征" class="headerlink" title="Categorical features(类别特征)"></a>Categorical features(类别特征)</h2><p>这部分主要是对类别特征转化为数值型特征，一般类别特征的最简单的方式就是通过对应标签的平均值来代替，而CatBoost则使用了自己设定的方式来进行转换，而且会进行特征组合，生成新的特征。</p><h3 id="Greedy-TS"><a href="#Greedy-TS" class="headerlink" title="Greedy TS"></a>Greedy TS</h3><p>类别特征不能直接在boosting中使用，因此需要将类别特征转换为数值型特征，CatBoost处理Categorical features的方式有别于其他方式，首先会计算TS(target statistics)，而传统的方式是：</p><script type="math/tex; mode=display">\hat{x}_{k}^{i}=\frac{\sum_{j=1}^{n}\left[x_{j, k}=x_{i, k}\right] \cdot Y_{i}}{\sum_{j=1}^{n}\left[x_{j, k}=x_{i, k}\right]}</script><p>CatBoost对该方法进行了优化，采用<strong>Greedy TS</strong>，添加先验分布项，这样可以减少噪声和低频率数据对于数据分布的影响（这个公式写的很奇怪，可能是俄罗斯人思维方式和我们的差异）：</p><script type="math/tex; mode=display">\hat{x}_{k}^{i}=\frac{\sum_{j=1}^{p-1}\left[x_{\sigma_{j}, k}=x_{\sigma_{p}, k}\right] Y_{\sigma, j}+a \cdot P}{\sum_{j=1}^{p-1}\left[x_{\sigma_{j}, k}=x_{\sigma_{p}, k}\right]}</script><p>其中，$a$为大于0的参数，$P$一般是target值得均值(添加的先验项)。其中方括弧中即满足条件为1，否则为0。但是上面得式子会出现一个问题就是greedy导致target leakage，$\hat{x}_{k}^{i}$的计算需要使用$y_k$和$x_k$，这样会出现<strong>target leakage</strong>，即$E\left(\hat{x}^{i} | y=v\right)=E\left(\hat{x}_{k}^{i} | y_{k}=v\right)$不成立。解决办法就是去掉$x_k$：<script type="math/tex">D\_{k} \subset D-\left\{x\_{k}\right\}</script>的样本来估计$\hat{x}_{k}^{i}$的值，不使用全部数据集合$D$。<a id="more"></a></p><h3 id="Holdout-TS"><a href="#Holdout-TS" class="headerlink" title="Holdout TS"></a>Holdout TS</h3><p>这种方法是将数据集分为两部分，D0和D1，D0用来计算TS，而D1用来训练，这种方式减少了用于训练的数据量和用来计算TS的的数据量，但是也存在问题：并未使用到所有的数据集来计算TS特征和学习模型。</p><h3 id="Leave-one-out-TS"><a href="#Leave-one-out-TS" class="headerlink" title="Leave-one-out TS"></a>Leave-one-out TS</h3><p>为了平衡训练和TS的计算，leave-one-out 的方式可能会更好，文中对该方式进行了简要的说明。</p><h3 id="Ordered-TS"><a href="#Ordered-TS" class="headerlink" title="Ordered TS"></a>Ordered TS</h3><p>为了将所有样本用于训练，CatBoost给出了一种解决方案，这受在线学习算法的启发（在线学习算法按照时序来获取训练数据），即首先对所有样本进行随机排序，然后针对类别型特征中的某个取值，每个样本的该特征转为数值型时都是基于排在该样本之前的类别标签取均值，同时加入了优先级和优先级的权重系数。CatBoost在不同的梯度提升步中使用不同的排列。</p><h2 id="Order-Boosting"><a href="#Order-Boosting" class="headerlink" title="Order Boosting"></a>Order Boosting</h2><h3 id="Prediction-shift"><a href="#Prediction-shift" class="headerlink" title="Prediction shift"></a>Prediction shift</h3><p>在梯度提升的过程中，存在<code>Prediction Shift</code>的问题，主要是应为特征的target leakage导致，处理的方式还是和<code>ordered ts</code>处理方式相同。</p><script type="math/tex; mode=display">h_{t}=\operatorname{argmin}_{\{h \in H\}} \frac{1}{n} \sum_{k=1}^{n}\left(-g^{t}\left(x_{k}, y_{k}\right)-h\left(x_{k}\right)\right)</script><p>论文中对链式偏移（chain of shifts）进行了描述：</p><p>（1）梯度的条件分布和测试数据的分布存在偏移。</p><p>（2）$h_t$的估计和式子$h^{t}=\underset{h \in H}{\arg \min } \mathbb{E}\left(-g^{t}(\mathbf{x}, y)-h(\mathbf{x})\right)^{2}$存在偏差。</p><p>（3）最终会影响学习到的函数$F^t$的泛化能力。</p><h3 id="Order-Boosting-1"><a href="#Order-Boosting-1" class="headerlink" title="Order Boosting"></a>Order Boosting</h3><p>为了解决上面的问题，CatBoost提出了order boosting的方案，具体算法如下图所示，首先是生成随机排列$\sigma$，随机配列用于下面的模型训练。在计算第$M_i$个模型时，使用排列中的前$i$个样本训练模型。在迭代（每次迭代使用的排列都不相同）的过程中，为了得到第$j$个样本的残差估计值，使用$M_{j-1}$模型进行估计。</p><p><img src="/uploads/order-boosting.png" srcset="/img/loading.gif" alt="order_boosting"></p><p>下图中对模型构建过程进行了简要可视化：</p><p><img src="/uploads/order_boost_compute.png" srcset="/img/loading.gif" alt="order_boost_compute"></p><p>论文中也分析了模型训练过程中的时间和空间复杂度，总体而言时间复杂度较高，因此，boosting tree构建过程必须进行改进和优化。</p><h2 id="主要实现"><a href="#主要实现" class="headerlink" title="主要实现"></a>主要实现</h2><h3 id="树的构建"><a href="#树的构建" class="headerlink" title="树的构建"></a>树的构建</h3><p>CatBoost 使用决策树，每次分裂节点均使用相同的策略。在Ordered Mode下，在计算梯度或者计算某个叶子节点平均梯度值，均只利用前 $i$个样本的信息。而在Plain Mode下，流程和GBDT算法类似，但是对于分类变量，计算其TS时仍会使用带permutation的模式（使用随机排列$\sigma^0$)。CatBoost实现了 gradient boosting，完全使用二叉树作为基础树形结构，而且是对称的二叉树。论文中认为使用这种树为了避免过拟合。</p><h3 id="叶子节点值"><a href="#叶子节点值" class="headerlink" title="叶子节点值"></a>叶子节点值</h3><p>在所有树结构都建立好的情况下，最终模型 $F$的叶子节点值是通过标准的梯度提升处理得到的，对于分类变量就需要使用随机排列 $\sigma^o$ 计算TS值。</p><p>具体的构建树的算法过程就不详细说明了，可以参考<a href="https://arxiv.org/pdf/1706.09516.pdf" target="_blank" rel="noopener">官方论文</a>中伪代码。</p><h2 id="CatBoost优点"><a href="#CatBoost优点" class="headerlink" title="CatBoost优点"></a>CatBoost优点</h2><ul><li>性能卓越：在性能方面可以匹敌任何先进的机器学习算法</li><li>鲁棒性/强健性：它减少了对很多超参数调优的需求，并降低了过度拟合的机会，这也使得模型变得更加具有通用性</li><li>易于使用：提供与scikit集成的Python接口，以及R和命令行界面</li><li>实用：可以处理类别型、数值型特征</li><li>可扩展：支持自定义损失函数</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><a href="https://github.com/catboost/catboost" target="_blank" rel="noopener">CatBoost github main page</a></li><li><a href="https://catboost.ai/" target="_blank" rel="noopener">CatBoost AI Documentation</a></li><li><a href="https://arxiv.org/pdf/1706.09516.pdf" target="_blank" rel="noopener">CatBoost原始Paper地址</a></li><li><a href="https://blog.csdn.net/u014686462/article/details/83543609" target="_blank" rel="noopener">论文笔记-1</a></li><li><a href="https://www.jianshu.com/p/49ab87122562" target="_blank" rel="noopener">CatBoost一个简单实用的boosting算法</a></li><li><a href="https://blog.csdn.net/appleyuchi/article/details/85413352" target="_blank" rel="noopener">CatBoost原文解读</a></li><li><a href="https://lonepatient.top/2017/03/18/boosting.html" target="_blank" rel="noopener">XGBoost、LightGBM和CatBoost的同与不同</a></li><li><a href="https://www.biaodianfu.com/catboost.html" target="_blank" rel="noopener">机器学习算法之Catboost</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>boosting</tag>
      
      <tag>target-statistics</tag>
      
      <tag>catboost</tag>
      
      <tag>categorical</tag>
      
      <tag>feature</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于轨迹的数据挖掘相关技术总结</title>
    <link href="/2019/07/20/%E8%BD%A8%E8%BF%B9%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"/>
    <url>/2019/07/20/%E8%BD%A8%E8%BF%B9%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>之前项目中接触到了轨迹数据挖掘领域，始终都没有时间对轨迹数据挖掘进行整体的梳理和总结，现在抽几天时间来进行总结并进行记录。</p><p>目前非常多的数据都是轨迹数据，如共享单车骑行的数据，手机GPS导航的数据，定位数据，一些传感器数据等等。在未来5G的发展下，相信轨迹数据或时空类型的数据会更多，而且会更加的复杂。</p><p>以前接触的数据大多没有空间的概念，都是一些普通的数据样本。这些数据可以认为是空间独立的，因此在数据分析或挖掘时，使用起来更加的方便，其技术也相对于成熟。而轨迹数据不同，它同时具有时间和空间双维度，在建模的时候很多传统算法在应用时都存在一定的问题，如轨迹的距离度量，这么一个简单问题，在轨迹里面都会变得复杂。</p><p>通过本次项目的实施，也对轨迹数据涉及到的主要问题进行了梳理，一些涉及到的技术栈进行了总结，便于后面项目中遇到该类问题时进查阅，同时增加自己的理解。</p><h2 id="轨迹数据涉及的技术栈"><a href="#轨迹数据涉及的技术栈" class="headerlink" title="轨迹数据涉及的技术栈"></a>轨迹数据涉及的技术栈</h2><p>轨迹数据的技术栈基本和普通的技术栈涉及到的过程一致，但是过程中某个部分具有很大的差异，下图是经过梳理得到的整个轨迹数据的涉及到的技术栈，其中每个部分都含有很多部分。具体如下图所示：</p><p><img src="/uploads/轨迹数据挖掘技术栈.png" srcset="/img/loading.gif" alt="轨迹数据挖掘技术栈"><a id="more"></a></p><h3 id="轨迹数据存储"><a href="#轨迹数据存储" class="headerlink" title="轨迹数据存储"></a>轨迹数据存储</h3><p>轨迹数据的存储和普通的数据存储不同，轨迹数据属于GIS数据，包含了位置数据，而且轨迹的表示为几何图形中的<code>Line</code>格式。因此一般采用GIS数据库进行存储，目前比较流行存储数据对比如下表所示：</p><div class="table-container"><table><thead><tr><th>数据库名</th><th>分布式</th><th>空间对象</th><th>HDFS</th></tr></thead><tbody><tr><td><a href="https://en.wikipedia.org/wiki/Environmental_Systems_Research_Institute" target="_blank" rel="noopener">ESRI</a> GIS Tools for <a href="https://en.wikipedia.org/wiki/Hadoop" target="_blank" rel="noopener">Hadoop</a></td><td>yes</td><td>yes (own specific API)</td><td>yes</td></tr><tr><td><a href="https://en.wikipedia.org/wiki/GeoMesa" target="_blank" rel="noopener">GeoMesa</a></td><td>yes</td><td>yes (<a href="https://en.wikipedia.org/wiki/Simple_Features" target="_blank" rel="noopener">Simple Features</a>)</td><td>yes</td></tr><tr><td><a href="https://en.wikipedia.org/wiki/H2_(DBMS" target="_blank" rel="noopener">H2</a>) (H2GIS)</td><td>no</td><td>yes (custom, no raster)</td><td>no</td></tr><tr><td><a href="https://en.wikipedia.org/wiki/Ingres_(database" target="_blank" rel="noopener">Ingres</a>)</td><td>yes (if extension is installed)</td><td>yes (custom, no raster)</td><td>no</td></tr><tr><td><a href="https://en.wikipedia.org/wiki/Neo4j" target="_blank" rel="noopener">Neo4J</a>-spatial<a href="https://en.wikipedia.org/wiki/Spatial_database#cite_note-15" target="_blank" rel="noopener">[15]</a></td><td>no</td><td>yes (<a href="https://en.wikipedia.org/wiki/Simple_Features" target="_blank" rel="noopener">Simple Features</a>)</td><td>no</td></tr><tr><td><a href="https://en.wikipedia.org/wiki/PostgreSQL" target="_blank" rel="noopener">PostgreSQL</a>with <a href="https://en.wikipedia.org/wiki/PostGIS" target="_blank" rel="noopener">PostGIS</a></td><td>no</td><td>yes (<a href="https://en.wikipedia.org/wiki/Simple_Features" target="_blank" rel="noopener">Simple Features</a>and raster)</td><td>no</td></tr><tr><td><a href="https://en.wikipedia.org/wiki/Postgres-XL" target="_blank" rel="noopener">Postgres-XL</a> with <a href="https://en.wikipedia.org/wiki/PostGIS" target="_blank" rel="noopener">PostGIS</a></td><td>yes</td><td>yes (<a href="https://en.wikipedia.org/wiki/Simple_Features" target="_blank" rel="noopener">Simple Features</a>and raster)</td><td>no</td></tr><tr><td><a href="https://en.wikipedia.org/wiki/Rasdaman" target="_blank" rel="noopener">Rasdaman</a></td><td>yes</td><td>just raster</td><td>no</td></tr><tr><td><a href="https://en.wikipedia.org/wiki/RethinkDB" target="_blank" rel="noopener">RethinkDB</a></td><td>yes</td><td>yes</td><td>no</td></tr></tbody></table></div><p>目前比较流行的为<code>PostgreSQL with PostGIS</code>，<code>GeoMesa</code>数据库，支持<code>Point, Line, Poly</code>等几何图形的存储，而且包含了很多GIS相关的功能函数使用。目前，MySQL，MongoDB, Redis等都支持GEO，但是对轨迹数据支持不完整，阿里，腾讯都有自己的时空数据的数据库。</p><h3 id="轨迹数据的处理"><a href="#轨迹数据的处理" class="headerlink" title="轨迹数据的处理"></a>轨迹数据的处理</h3><p>轨迹数据处理是非常重要的一环，数据处理包括，轨迹的压缩，噪声的过滤，轨迹分段等等。数据处理不仅会影响后面建模，对特征提取，后期应用都会有一定的影响。</p><h4 id="轨迹压缩"><a href="#轨迹压缩" class="headerlink" title="轨迹压缩"></a>轨迹压缩</h4><p>轨迹数据可能存在的主要问题有：数据量大，噪声多，定位不准，位置偏移等，而数据压缩可以在一定程度上解决这些问题，比如减少数据量，在某些应用场景下，不需要非常密集(时间)的轨迹数据，那么数据压缩后可以减小数据量。数据压缩主要有离线压缩（批量）和在线压缩。</p><ul><li>离线压缩：Douglas-Peucker (DP)，Top-Down Time Ratio（TDTR）等算法</li><li>在线压缩：Reservoir Sampling，Slicing Window，open window等算法</li></ul><h4 id="轨迹平滑"><a href="#轨迹平滑" class="headerlink" title="轨迹平滑"></a>轨迹平滑</h4><p>空间轨迹是一个$(x,y)$点的序列,每个点都有一个时间戳。因为轨迹通常是由传感器测量的，所以它们不可避免地会出现一些错误，需要对数据进行平滑化处理。主要方法有：</p><ul><li>均值滤波</li><li>中值滤波</li><li>卡尔曼滤波（Kalman filter）</li><li>粒子滤波</li></ul><h4 id="轨迹分割"><a href="#轨迹分割" class="headerlink" title="轨迹分割"></a>轨迹分割</h4><p>轨迹分割主要是对一个长轨迹分割为各个短的轨迹，将轨迹进行分解为不同段的轨迹。在很多应用场景下都会使用轨迹分割技术，目前也有很多轨迹分割算法，主要有：</p><ul><li>通过<strong>驻留点或停留点（<code>stay point</code>）</strong>检测进行轨迹分割</li><li>根据<strong>时间间隔</strong>进行轨迹分割</li><li>根据<strong>轨迹点密度</strong>进行轨迹分割</li><li>根据速度来进行轨迹分割</li></ul><p>其中轨迹密度可以有很多方法进行检测，一般都基于密度聚类算法的改进算法来进行检测，如：K-中值算法，<code>DJ-Cluster</code>算法，<code>cb_swot</code>算法，MSN(Move-Stop-Noise)算法等。</p><h3 id="数据管理"><a href="#数据管理" class="headerlink" title="数据管理"></a>数据管理</h3><p>时空数据管理或轨迹数据管理，涉及到近邻查询，时间和空间维度的快速查询，空间索引等相关技术。而这一部分，一般GIS数据库都包含了相关的功能，只需要在创建表时，增加索引或使用相关功能函数即可。但是有时在工程应用时也会涉及到空间近邻匹配查询。</p><h4 id="空间索引"><a href="#空间索引" class="headerlink" title="空间索引"></a>空间索引</h4><p>空间索引的方法有很多，目前经常使用的有KD树，R树，四叉树及图等相关结构。但是目前效率较高还是基于图的索引，特别是在高维空间时。这里有一个关于NN索引查询效率对比：<a href="http://ann-benchmarks.com/" target="_blank" rel="noopener">http://ann-benchmarks.com/</a></p><h3 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h3><p>轨迹数据的挖掘包含很多方面，基本传统机器学习的包含的，轨迹数据都包含在内。但是主要的一些方向有轨迹的分类，轨迹聚类，轨迹异常检测，轨迹推荐，轨迹的可视化等待。</p><h2 id="论文解读-Trajectory-Clustering-A-Partition-and-Group-Framework"><a href="#论文解读-Trajectory-Clustering-A-Partition-and-Group-Framework" class="headerlink" title="论文解读-Trajectory Clustering: A Partition-and-Group Framework"></a>论文解读-Trajectory Clustering: A Partition-and-Group Framework</h2><p>这篇论文涉及到了轨迹数据挖掘过程的很多问题，该论文有很多值得借鉴的地方，因此对改论文进行说明，并对该论文中的所有短发框架进行了实现(python)。该论文涉及到了轨迹的数据处理，轨迹分割，轨迹距离度量，轨迹聚类，主要轨迹提取等待。</p><h3 id="问题的提出"><a href="#问题的提出" class="headerlink" title="问题的提出"></a>问题的提出</h3><p><a href="http://hanj.cs.illinois.edu/pdf/sigmod07_jglee.pdf" target="_blank" rel="noopener">该论文</a>主要从众多的轨迹中找出聚集在一期的分段轨迹部分，并合成新的轨迹。这样可以找出公共轨迹部分，密集轨迹部分等。如下图所示，其中方框内的部分就行需要找出的部分。</p><p><img src="/uploads/1553750720955.png" srcset="/img/loading.gif" alt=""></p><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><h4 id="轨迹的距离度量"><a href="#轨迹的距离度量" class="headerlink" title="轨迹的距离度量"></a>轨迹的距离度量</h4><p>轨迹的距离从3个方面进行度量，两个轨迹之间<strong>最远点和最短点之间的距离</strong>，轨迹的<strong>长度</strong>，轨迹的<strong>角度</strong>。三者的度量公式如下图所示：</p><p><img src="/uploads/traj_distance.png" srcset="/img/loading.gif" alt="trajectory distance"></p><p>最终的总距离为：</p><script type="math/tex; mode=display">dist(L_i;L_j) = w_{\bot} d_{\bot}(Li;Lj)+w_{\parallel} d_{\parallel}(Li;Lj)+w_{\theta} d_{\theta}(Li;Lj )</script><p>其中权重为默认均为1，根据不同的应用场景可以修改权重。</p><h4 id="Partition处理"><a href="#Partition处理" class="headerlink" title="Partition处理"></a>Partition处理</h4><p>轨迹的分段主要是通过MDL（最小描述长度）算法。通过找到轨迹的特征点来确定partition，通过计算两个点之间存在partition特征点时的MDL开销和不存在partition特征点时的开销，进行对比，从而确定特征点，并标记出来，算法相对较简单。最终将轨迹划分为了Segment，具体算法过程如下图所示：</p><p><img src="/uploads/traj_partition.png" srcset="/img/loading.gif" alt="trajectory partition"></p><p>通过python实现后得到的测试结果如下图：</p><p><img src="/uploads/trajectory_partition_theta_5.png" srcset="/img/loading.gif" alt="trajectory partition result"></p><h4 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h4><p>基本思想和DBSCAN算法一致，给定所有轨迹的Segment集合，输出聚类的集合。算法主要分为3个步骤，下面对这3个步骤进行简要说明：</p><ol><li>对于每一条未分类的线段L，算法计算其的$\epsilon$邻域以判断该线段是否为核心线段。若L为核心线段，则程序跳转第2步。 </li><li>计算核心线段的密度相连集合并把其加入该核心线段组成的簇中。如果新加入的线段未被分类，则把其加入队列Q中以做进一步扩展，因为该线段可能是核心线段；若新加入的线段不是核心线段，则不加入队列Q中。 </li><li>计算每个簇的基数，若其值小于阈值，则算法将该簇淘汰，因为其不够密集。 </li></ol><p>可视化部分就不详细说明，可以参考论文。下面是将整个算法实现后，使用测试数据测试后的效果，算法可以设置不同的参数，得到的效果不同，如下图所示：</p><p><img src="/uploads/trajectory-major.png" srcset="/img/loading.gif" alt="聚类和可视化效果"></p><h3 id="代码开源"><a href="#代码开源" class="headerlink" title="代码开源"></a>代码开源</h3><p>GitHub: trajectory-cluster-<a href="https://github.com/MillerWu2014/trajectory-cluster" target="_blank" rel="noopener">https://github.com/MillerWu2014/trajectory-cluster</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>trajectory</tag>
      
      <tag>cluster</tag>
      
      <tag>partition</tag>
      
      <tag>spatial</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>boosting系列之LightGBM算法详解</title>
    <link href="/2019/07/16/boosting%E7%B3%BB%E5%88%97%E4%B9%8BLightGBM%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <url>/2019/07/16/boosting%E7%B3%BB%E5%88%97%E4%B9%8BLightGBM%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="LightGBM简介"><a href="#LightGBM简介" class="headerlink" title="LightGBM简介"></a>LightGBM简介</h2><p><code>LightGBM</code>是由微软团队基于<code>xgboost</code>的缺点新的实现，主要思想还是基于<code>GBDT</code>算法，在细节上进行了优化，在效率和精度方面都超过了<code>xgboost</code>的实现。并且支持分布式并行，包括特征并行，数据并行以及GPU并行计算。下面就对该算法的原理和细节进行详细说明。</p><p><code>LightGBM</code>主要针对<code>xgboost</code>或<code>gbdt</code>中特征分裂进行了优化，<code>xgb</code>和其他<code>gbdt</code>的实现中都采用了<code>pre-sort</code>，这种方式对内存消耗和训练速度上效率都非常低，还有另外一种实现就是通过直方图算法（histogram-based algorithm）因此，<code>LightGBM</code>采用<code>histogram</code>算法来对每个特征的分裂进行优化。</p><p>另外，为了减小训练数据集，一般是通过下采用，或者通过阈值的方式来过滤掉权重较小的数据。同样，可以通过过滤掉弱特征来减少特征量，从而在每次训练弱分类器的过程中提升模型的训练效率。但是<code>GBDT</code>中并没有样本权重的指标，下面就基于<code>LightGBM</code>论文中提出的点进行详细描述。<br><a id="more"></a></p><h2 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h2><p>主要对<code>LigthGBM</code>中的直方图算法，<code>GOSS</code>算法以及<code>EFB</code>进行详细的说明和算法过程描述，这三个点是该算法中最核心的优化和细节实现。并对这三个点的作用进行说明。</p><h3 id="histogram算法"><a href="#histogram算法" class="headerlink" title="histogram算法"></a>histogram算法</h3><p>直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。下面是<code>histogram</code>的过程：</p><p><img src="/uploads/1563269661109.png" srcset="/img/loading.gif" alt="1563269661109"></p><p>从上面的直方图算法流程中可以看出，主要对特征进行离散化，转化为bin value。通过分段函数将特征取值划分到某个bin中。上面的流程中，第二个<code>for</code>循环开始，是遍历<code>node</code>集合，即创建的每个树模型；第三个<code>for</code>循环开始遍历所有的特征（m个特征），为每个特征创建一个<code>Histogram</code>对象；在第四个<code>for</code>循环开始遍历所有的样本，来构建直方图，在这里主要做了2个操作，一个是统计每个bin下样本的个数，不断更新每个bin下的样本数，另外一个操作就是不断更新每个bin下梯度和，计算每个bin中样本的梯度和。后面开始基于bin来找最佳分裂。</p><h3 id="Gradient-based-One-Side-Sampling-GOSS"><a href="#Gradient-based-One-Side-Sampling-GOSS" class="headerlink" title="Gradient-based One-Side Sampling (GOSS)"></a>Gradient-based One-Side Sampling (GOSS)</h3><p>在<code>AdaBoost</code>中，样本权重是数据样本的重要性指标，但是在<code>GBDT</code>中并没有样本权重，不能通过权重来对样本进行采样。不过在<code>GBDT</code>中存在每个数据不同的梯度值，而这个信息对采用非常有用，<strong>即实例的梯度小，实例训练误差也就较小，已经被学习得很好了，直接想法就是丢掉这部分梯度小的数据</strong>，但是直接这样做会导致数据的分布被改变，从而影响训练模型的精度，而GOSS即是解决该问题而提出。</p><p><code>GOSS</code>简单的描述就是保留梯度较大的样本，在梯度较小的实例上进行随机采样。为了抵消对数据分布的影响，计算信息增益的时候，<code>GOSS</code>对小梯度的数据引入常量乘数。<code>GOSS</code>首先根据数据的梯度绝对值排序，选取<code>top a</code>个实例。然后在剩余的数据中随机采样<code>b</code>个实例。在计算信息增益时为采样出的小梯度样本乘以$\frac {1-a} {b}$的权重，这样在训练过程中模型会更关注训练不足的实例，而不会过多改变原数据集的分布。具体过程如下:</p><p><img src="/uploads/1563331674809.png" srcset="/img/loading.gif" alt="1563331674809"></p><p>在迭代（每个迭代就是一个弱分类器）的过程中，使用模型预测训练集的目标值，再计算<code>Loss</code>的梯度值，并更新每个样本的梯度<code>g</code>和每个样本的权重<code>w</code>，再对所有样本的梯度值进行排序，选取梯度值大的<code>TOP N</code>。在剩下的样本中在随机抽取<code>TOP N</code>个样本，组合为新的训练集。并再来更新新的样本权重（小梯度样本乘以<code>frac</code>），再训练模型。</p><h3 id="Exclusive-Feature-Bundling-EFB"><a href="#Exclusive-Feature-Bundling-EFB" class="headerlink" title="Exclusive Feature Bundling (EFB)"></a><strong>Exclusive Feature Bundling (EFB)</strong></h3><p>上面的GOSS主要是减少样本数量来提升效率。而EFB主要可以理解为互斥特征绑定，即减少特征的数量，从而来提升训练效率。一般在建模过程中，高纬度的数据是稀疏的，既然是稀疏的，那么能通过一种方式来减少特征维度么？在高维和稀疏的特征空间中，很多特征是互斥的，即不同时为非0值。那么，就提出了一种将互斥特征进行绑定为一个单一的特征，来减少特征维度。微软研究团队设计了一种将互斥特征捆绑后的特征构建与单个特征相同的直方图，这样构建直方图的复杂度从<code>O(#data*#feature)</code>变为<code>O(#data*#bundle)</code>，这样提升了效率，而不降低模型精度。但是在这个过程中，存在2个关键的问题：</p><ol><li>如何判断哪些特征应该绑定在一起呢？（上面时候进行绑定）</li><li>怎么将特征绑定在一起呢？（如何绑定）</li></ol><h4 id="特征绑定-bundle"><a href="#特征绑定-bundle" class="headerlink" title="特征绑定(bundle)"></a>特征绑定(bundle)</h4><p><code>LightGBM</code>算法中采用了”图着色”一类问题进行解决，首先创建一个图G(v, e)，其中v就是特征，为图中的节点，e为G中的边，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。具体算法过程如下：</p><p><img src="/uploads/1563334789846.png" srcset="/img/loading.gif" alt="1563334789846"></p><h4 id="特征合并-merging-feature"><a href="#特征合并-merging-feature" class="headerlink" title="特征合并(merging feature)"></a>特征合并(merging feature)</h4><p>该过程主要是<strong>关键在于原始特征值可以从<code>bundle</code>中区分出来</strong>，即绑定几个特征在同一个bundle里需要保证绑定前的原始特征的值可以在bundle中识别，考虑到<code>histogram-based</code>算法将连续的值保存为离散的<code>bins</code>，我们可以使得不同特征的值分到<code>bundle</code>中的不同<code>bins</code>中，这可以通过在特征值中加一个偏置常量来解决，比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间[0,10)，B特征的原始取值为区间[0,20），我们可以在B特征的取值上加一个偏置常量10，将其取值范围变为[10,30），这样就可以放心的融合特征A和B了，因为在树模型中对于每一个特征都会计算分裂节点的，也就是通过将他们的取值范围限定在不同的<code>bins</code>中，在分裂时可以将不同特征很好的分裂到树的不同分支中去。具体算法如下：</p><p><img src="/uploads/1563335082001.png" srcset="/img/loading.gif" alt="1563335082001"></p><h2 id="参考指南"><a href="#参考指南" class="headerlink" title="参考指南"></a>参考指南</h2><ol><li><a href="https://lightgbm.apachecn.org/#/" target="_blank" rel="noopener">LightGBM中文文档</a></li><li><a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" target="_blank" rel="noopener">官方论文地址</a></li><li><a href="https://blog.csdn.net/shine19930820/article/details/79123216" target="_blank" rel="noopener">LightGBM原理-LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a></li><li><a href="https://blog.csdn.net/anshuai_aw1/article/details/83048709" target="_blank" rel="noopener">Lightgbm源论文解析：LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a></li><li><a href="https://www.msra.cn/zh-cn/news/features/lightgbm-20170105" target="_blank" rel="noopener">LightGBM模型中的改进细节</a></li><li><a href="https://blog.csdn.net/anshuai_aw1/article/details/83040541" target="_blank" rel="noopener">Lightgbm 直方图优化算法深入理解</a></li><li><a href="https://www.deeplearn.me/2315.html" target="_blank" rel="noopener">LightGbm之直方图优化理解</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>boosting</tag>
      
      <tag>gbdt</tag>
      
      <tag>lightgbm</tag>
      
      <tag>goss</tag>
      
      <tag>histogram</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python中实现多线程(真正的)的探索</title>
    <link href="/2019/06/25/Python%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BA%BF%E7%A8%8B(%E7%9C%9F%E6%AD%A3%E7%9A%84)%E7%9A%84%E6%8E%A2%E7%B4%A2/"/>
    <url>/2019/06/25/Python%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BA%BF%E7%A8%8B(%E7%9C%9F%E6%AD%A3%E7%9A%84)%E7%9A%84%E6%8E%A2%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<p>使用Python多线程的人都知道，Python中由于GIL(全局解释锁：<code>Global Interpreter Lock</code>)的存在，在多线程时并没有真正的进行多线程计算。知道Python历史的就知道Python在1989年发布，当时计算机全是基于单核，知道2000多年多核处理器才发布。因此，GIL一致沿用到现在，而且没有非常好的替代方式。如果涉及到计算密集型的任务，在多核CPU的加持下使得效率急速上升，如果是IO密集型的效果可能没有那么明显。这个Python的官网也进行了说明。为了说明Python的多线程问题，下面可以通过一些代码来实现Python多线程的验证。</p><h2 id="Python和C-多线程的对比"><a href="#Python和C-多线程的对比" class="headerlink" title="Python和C++多线程的对比"></a>Python和C++多线程的对比</h2><p>首先使用Python中的多线程运行，观察CPU的情况，下面定义了一个函数，不进行任何计算，但是一致保持运行即可，再查看CPU的情况，代码如下：</p><pre><code class="lang-python">from concurrent.futures import ThreadPoolExecutordef f(a):    while 1:        passif __name__ == &#39;__main__&#39;:    pool = ThreadPoolExecutor()    pool.map(f, range(100))</code></pre><p>CPU的情况如下,乐意看出只有其中一个CPU的使用率为100%以上，其他没有使用，而且每次只运行一个线程，所以可以确定Python的多线程并没有利用多核运行。    <img src="/uploads/1561516460850.png" srcset="/img/loading.gif" alt="1561516460850"><br><a id="more"></a><br>下面我们使用C++的多线程来进行测试，使用同样的方式，下面是c++多线程的简单实现，f函数也是一直保持运行，便于后面的CPU使用情况观察：</p><pre><code class="lang-c++">#include &lt;thread&gt;#define NUM_THREADS 50using namespace std;void f(){    while(1){        continue;    };}void run_thread(){    std::thread threads[NUM_THREADS];    for(int i = 0; i &lt; NUM_THREADS; ++i)    {        threads[i] = std::thread(f);    }    for (int i = 0; i &lt; NUM_THREADS; ++i) {        threads[i].join();    }};int main(void){    run_thread();}</code></pre><p>经过编译后并运行：<code>g++ -pthread -std=c++11 ./main_thread_cpp.cpp &amp;&amp; ./a.out</code>，CPU的使用情况如下，可以看出CPU的使用率为1200%，12核全部为100%的使用率，多线程真正使用了多核。那么我们<strong>能将 python的多线程改为多核的多线程么</strong>？</p><p><img src="/uploads/1561516889704.png" srcset="/img/loading.gif" alt="1561516889704"></p><h2 id="Python多线程并行计算探索"><a href="#Python多线程并行计算探索" class="headerlink" title="Python多线程并行计算探索"></a>Python多线程并行计算探索</h2><p>我们知道Python中的多线程主要是由于GIL的存在，多线程之间相互竞争，而Python的官网文档<code>C API Reference Manua</code>对GIL进行了详细的说明。GIL只用来限制<code>pure python code</code>，对其他的没有限制。因此，我们可以通过一些测试来验证这个idea。下面就进行验证。</p><h3 id="Python多线程调用C-中的多线程"><a href="#Python多线程调用C-中的多线程" class="headerlink" title="Python多线程调用C++中的多线程"></a>Python多线程调用C++中的多线程</h3><p>首先我们使用python来调用C++的多线程代码，来观察CPU的使用情况，来验证之前的结论，这里使用pybind11来进行混合编程，下面是c++代码，多线程来直线任务：</p><pre><code class="lang-c++">#include &lt;thread&gt;#include &quot;pybind11/pybind11.h&quot;#define NUM_THREADS 50using namespace std;namespace py = pybind11;void f(){    while(1){        continue;    }}py::none nothing(){    std::thread threads[NUM_THREADS];    for(int i = 0; i &lt; NUM_THREADS; ++i)    {        threads[i] = std::thread(f);    }    for (int i = 0; i &lt; NUM_THREADS; ++i) {        threads[i].join();    }    return py::none();}PYBIND11_MODULE(example, m){    m.def(&quot;nothing&quot;, &amp;nothing, &quot;do nothing&quot;);}</code></pre><p>通过下面的编译，得到一个example的<code>.so</code>文件的包，再使用python来调用包里面的<code>nothing</code>函数，再观察CPU的运行情况，如下图所示.</p><pre><code class="lang-shell">g++ -O3 -Wall -shared -std=c++11 -fPICpython3 -m pybind11 --includesmix_thread.cpp -o examplepython3-config --extension-suffix`</code></pre><pre><code class="lang-python">from example import nothingfrom concurrent.futures import ThreadPoolExecutorpool = ThreadPoolExecutor()for i in range(12):    pool.submit(nothing)</code></pre><p><img src="/uploads/1561517891382.png" srcset="/img/loading.gif" alt="1561517891382"></p><p>从上图中的执行可以看出，缺失利用到了多核，实现了多线程的并行计算。但是这个多线程实际上是在C++中实现的，Python只是作为接口调用了C++中的函数而已。这样还不是Python的多线程。下面我们继续对代码进行修改。</p><h3 id="Python多线程调用C-中的普通函数"><a href="#Python多线程调用C-中的普通函数" class="headerlink" title="Python多线程调用C++中的普通函数"></a>Python多线程调用C++中的普通函数</h3><p>首先我们去掉C++中的多线程，通过Python的多线程直接调用C++的函数来看是什么情况呢？下面的代码来进行测试，将多线程去掉后，在Python中调用<code>nothing</code>函数来进行测试。</p><pre><code class="lang-c++">#include &lt;thread&gt;#include &quot;Python.h&quot;#include &quot;pybind11/pybind11.h&quot;#define NUM_THREADS 50using namespace std;namespace py = pybind11;int f(){    int n = 1000000000;    while(n &gt; 0){        n--;        sleep(0.2);    }    return n;}py::int_ nothing(){    int y;    y = f();    return py::int_(y);}PYBIND11_MODULE(example1, m){    m.def(&quot;nothing&quot;, &amp;nothing, &quot;do nothing&quot;);}</code></pre><p>编译，生成<code>example1</code>的<code>.so</code>文件，在使用Python直接调用该包即可：</p><pre><code class="lang-shell">g++ -O3 -Wall -shared -std=c++11 -fPIC `python3 -m pybind11 --includes` main_multi_nothread.cpp -o example1`python3-config --extension-suffix`</code></pre><pre><code class="lang-python">from example1 import nothingfrom concurrent import futuresfrom concurrent.futures import ThreadPoolExecutorpool = ThreadPoolExecutor()future_list = []for i in range(12):    future = pool.submit(nothing)    future_list.append(future)for future in futures.as_completed(future_list):    res = future.result()    print(res)</code></pre><p>经过编译，再在Python中进行调用得到如下图所示，每次只有一个线程在运行，因此并没有使用多线程进行并行计算。如下图所示，但是我们能实现前面所说的多线程真正的并行计算吗？</p><p><img src="/uploads/1561518345849.png" srcset="/img/loading.gif" alt="1561518345849"></p><h3 id="Python多线程调用C-中的释放GIL的函数"><a href="#Python多线程调用C-中的释放GIL的函数" class="headerlink" title="Python多线程调用C++中的释放GIL的函数"></a>Python多线程调用C++中的释放GIL的函数</h3><p>经过一番<code>Google</code>和官方文档的查询之后，基本了解，我们上面的代码中，在Python执行时会自动加GIL，因此我们需要手动的将GIL进行释放，将上面的代码进行修改，在调用函数<code>f</code>的前面加入<code>Py_BEGIN_ALLOW_THREADS</code>，后面加入<code>Py_END_ALLOW_THREADS</code>。再和上面相同的步骤运行，来观察CPU的使用情况。</p><pre><code class="lang-c++">#include &quot;Python.h&quot;#include &quot;pybind11/pybind11.h&quot;#define NUM_THREADS 50using namespace std;namespace py = pybind11;int f(){    int n = 1000000000;    while(n &gt; 0){        n--;        sleep(0.2);    }    return n;}py::int_ nothing(){    int y;    Py_BEGIN_ALLOW_THREADS    y = f();    Py_END_ALLOW_THREADS    return py::int_(y);}PYBIND11_MODULE(example1, m){    m.def(&quot;nothing&quot;, &amp;nothing, &quot;do nothing&quot;);}</code></pre><p>下面是python多线程运行时CPU使用情况，可以看到，所有的CPU均为100%，而且同时12个线程在运行。现在可以确定我们的猜想是正确的。整个python多线程最核心的就是GIL的限制，但是可以手动来释放GIL，来进行多线程。上面的代码，就是Python真正的多线程实现方式了。</p><p><img src="/uploads/1561519146831.png" srcset="/img/loading.gif" alt="1561519146831"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>根据上面的过程，我们可以确定得到结论：（1）Python中的GIL只作用于<code>pure python code</code>，其他的GIL限制不了也不会限制；（2）我们可以手动管理GIL来实现Python下的多线程；（3）GIL其实就是一把锁而已，并不需要对GIL进行困惑，当面对GIL时，需要打开”锁”。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ol><li><p>Python C API中提供了<code>Py_BEGIN_ALLOW_THREADS</code>和<code>Py_END_ALLOW_THREADS</code>宏来释放GIL，这两个宏的原型定义如下：<code>{PyThreadState *_save; _save = PyEval_SaveThread();</code>和<code>PyEval_RestoreThread(_save); }</code>,可以参考官方文档。</p></li><li><p>pybind11中也提供了释放GIL的方法：<code>py::gil_scoped_acquire acquire;</code>该方法是通过一个class来实现，类似于下面的代码：</p><pre><code class="lang-c++">class gil_scoped_acquire{public:    inline ScopedGILRelease() {        m_thread_state = PyEval_SaveThread();    }    inline ~ScopedGILRelease() {        PyEval_RestoreThread(m_thread_state);        m_thread_state = NULL;    }private:    PyThreadState * m_thread_state;};</code></pre></li><li><p>只要使用c/c++/cython等来释放GIL的函数调用就可以实现Python下的多线程，C使用1中的方式，C++使用1，2中的方式均可，Cython使用<code>with nogil</code>可以实现。</p></li></ol><p>参考文档：</p><ol><li><a href="https://cyrusin.github.io/2016/04/27/python-gil-implementaion/" target="_blank" rel="noopener">GIL实现细节解析</a></li><li><a href="http://blog.zhanglun.me/2018/09/15/触摸Python的GIL/" target="_blank" rel="noopener">触摸Python的GIL</a></li><li><a href="[https://closure11.com/%E9%80%9A%E8%BF%87cc%E6%89%A9%E5%B1%95%E7%9A%84%E6%96%B9%E5%BC%8F%E5%9C%A8python%E4%B8%AD%E8%BF%9B%E8%A1%8C%E5%B9%B6%E5%8F%91/](https://closure11.com/通过cc扩展的方式在python中进行并发/">通过C/C++扩展的方式在python中进行并发</a>)</li><li><a href="https://docs.python.org/3/c-api/init.html" target="_blank" rel="noopener">Initialization, Finalization, and Threads</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>c/c++</tag>
      
      <tag>thread</tag>
      
      <tag>pybind11</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高斯过程（Gaussian Process）理解</title>
    <link href="/2019/05/01/%E9%AB%98%E6%96%AF%E8%BF%87(Gaussian%20Process)%E7%A8%8B%E7%90%86%E8%A7%A3/"/>
    <url>/2019/05/01/%E9%AB%98%E6%96%AF%E8%BF%87(Gaussian%20Process)%E7%A8%8B%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>对于高斯过程（Gaussian Process）已经心仪已久，但是之前大概看了一些文章，没有完全理解，本身这个模型是比较理解的，因此现在抽出时间来研究一番，本文中不会进行复杂的公式推导，主要对高斯过程进行理解。</p><p>高斯过程是机器学习中 一种或者说一类的建模方法。这种建模方法得出的模型统称为非参数模型，比如：高斯过程，贝叶斯，KNN等（参数模型：回归，深度学习，逻辑回归）。高斯过程是从概率统计学的角度来解决问题，更确切的说应该是随机过程（Stochastic Process）种一个特殊的例子，下面是WIKI上的定义：</p><blockquote><p><strong>高斯过程</strong>（<strong>Gaussian process</strong>）是<a href="https://zh.wikipedia.org/wiki/随机变量" target="_blank" rel="noopener">观测值</a>出现在一个连续域（例如时间或空间）的统计模型。在高斯过程中，连续输入空间中每个点都是与一个<a href="https://zh.wikipedia.org/wiki/正态分布" target="_blank" rel="noopener">正态分布</a>的<a href="https://zh.wikipedia.org/wiki/随机变量" target="_blank" rel="noopener">随机变量</a>相关联。此外，这些随机变量的每个有限集合都有一个<a href="https://zh.wikipedia.org/wiki/多元正态分布" target="_blank" rel="noopener">多元正态分布</a>。高斯过程的分布是所有那些（无限多个）随机变量的联合分布，正因如此，它是连续域（例如时间或空间）的分布。</p><p>高斯过程被认为是一种<a href="https://zh.wikipedia.org/wiki/机器学习" target="_blank" rel="noopener">机器学习</a>算法，是以<a href="https://zh.wikipedia.org/w/index.php?title=惰性學習&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">惰性学习</a>方式，利用点与点之间同质性的度量作为<a href="https://zh.wikipedia.org/w/index.php?title=核函數&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">核函数</a>，以从输入的训练数据预测未知点的值。其预测结果不仅包含该点的值，而同时包含不确定性的资料－它的一维高斯分布（即该点的<a href="https://zh.wikipedia.org/wiki/边际分布" target="_blank" rel="noopener">边际分布</a>）。</p></blockquote><p>下面主要对高斯过程从几个方面进行详细的讲解，首先我们对高斯过程有一个大概的认知，通过一种通俗易懂的方式来进行说明，再从理论的角度来对高斯过程进行详细的理论讲解，最后会对高斯过程种使用的协方差函数（核函数）进行列举说明。另外会对使用高斯过程应该考虑的问题进行说明。</p><h2 id="1-高斯过程的通俗理解（总体认知）"><a href="#1-高斯过程的通俗理解（总体认知）" class="headerlink" title="1. 高斯过程的通俗理解（总体认知）"></a>1. 高斯过程的通俗理解（总体认知）</h2><p>在本文中通过<strong>高斯过程回归</strong>（GPR）来帮助高斯过程的理解。下面我们从回归的角度，从解决问题的角度看GPR是怎么一步一步解决我们的问题。</p><p>对于回归我们是给出$X$和$Y$，再给出测试的$X_{test}$，要求预测得到$Y_{test}$。高斯过程的出发点就是：<strong>如果两个$x$比较接近，那么它们对应的$y$一定比较接近</strong>。对于给定的新$X_{test}$与之前给出的$X$有多近，从而就知道了要预测$Y$的$Y_{test}$有多近，来预测$Y_{test}$。那么如何度量Y之间有多近呢？高斯过程是通过协方差矩阵（相关系数矩阵），而这个协方差矩阵怎么计算呢？这儿将多维高斯分布作为先验，认为所有的y，服从一个多维的联合高斯分布，只需要根据$X$和$Y$来将协方差矩阵计算好，最后按照公式计算$Y_{test}$的边缘分布就可以了。</p><p>上面解决了$Y$之间“近”的度量，那么$X$之间近怎么度量呢？那么核（kernel）就应该出现了，核不仅能刻画线性，而且能刻画非线性。目前整个问题就解决了：<strong>用$X$之间加kernel，来填充$Y$之间的协方差矩阵，然后计算$Y_{test}$的边缘分布。通过$X$和$Y$，给定$X_{test}$计算$Y_{test}$</strong>。</p><p>上面我们对高斯过程有了一个总体认识后，下面我们就结合理论来进行理解，知道理论种每一步的具体作用，帮助我们理解高斯过程。</p><h2 id="2-多元高斯分布"><a href="#2-多元高斯分布" class="headerlink" title="2. 多元高斯分布"></a>2. 多元高斯分布</h2><p>在理解高斯过程之前需要对多元高斯分布有初步的认识，高斯分布是高斯过程的基础部件，而多元高斯分布是高斯分布的基础。多元高斯分布定义：</p><blockquote><p>多变量正态分布亦称为多元高斯分布。它是单维高斯分布向多维的推广。它同<a href="https://zh.wikipedia.org/wiki/矩阵正态分布" target="_blank" rel="noopener">矩阵正态分布</a>有紧密的联系。 一般形式：</p><script type="math/tex; mode=display">X = N(\mu, \Sigma) \tag{1}</script><p>其中，$\mu$为向量，$\Sigma$为协方差矩阵，该分布为向量$\mu$为中心，协方差矩阵$\Sigma$决定其形状。N维随机向量 $X=[X_{1},\dots ,X_{N}]^{T}$如果服从多变量正态分布，必须满足下面的三个等价条件： </p><ol><li>任何线性组合$Y=a_{1}X_{1}+\cdots +a_{N}X_{N}$服从正态分布。 </li><li>存在随机向量 $Z=[Z_{1},\dots ,Z_{M}]^{T}$( 它的每个元素服从独立标准正态分布），向量 $\mu =[\mu_{1},\dots ,\mu_{N}]^{T}$及$N\times M$矩阵$A$满足$X=AZ+\mu $. </li><li>存在$\mu$ 和一个对称正定阵 $\Sigma$，满足$X$的特征函数$\phi_{X}\left(u;\mu ,\Sigma \right)=\exp \left(i\mu^{T}u-{\frac  {1}{2}}u^{T}\Sigma u\right)$，如果$\Sigma$是非奇异的，那么该分布可以由以下的PDF来描述：$f_{\mathbf {x} }(x_{1},\ldots ,x_{k})={\frac {1}{\sqrt {(2\pi )^{k}|{\boldsymbol {\Sigma }}|}}}\exp \left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm {T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu }})\right)$，注意这里的$|\Sigma |$是协方差矩阵的行列式。<a id="more"></a></li></ol></blockquote><p>下面是一个二维高斯分布的图像，可以看出是以（0， 0）为中心，1为峰值，图形的宽度，是通过$|\Sigma |$矩阵来定义的。具体如下图所示：</p><p><img src="/uploads/20180516094227255.jpg" srcset="/img/loading.gif" alt="gaussian curve with a 2-dimensional domain"></p><h2 id="3-高斯过程（Gaussian-Process）"><a href="#3-高斯过程（Gaussian-Process）" class="headerlink" title="3. 高斯过程（Gaussian Process）"></a>3. 高斯过程（Gaussian Process）</h2><p><code>Gaussian Process</code>即高斯和过程，高斯一般指高斯分布，多元高斯分布；过程，这里主要是指随机过程（wiki上的随机过程<strong>定义</strong>：在概率论概念中，<strong>随机过程</strong>是<strong>随机</strong>变量的集合。 若一<strong>随机</strong>系统的样本点是<strong>随机</strong>函数，则称此函数为样本函数，这一<strong>随机</strong>系统全部样本函数的集合是一个<strong>随机过程</strong>）。</p><p>本文还是基于回归的示例来理解高斯过程，假设：有$N$对样本$(x_i, y_i)_1^n$，一般是样本来拟合$x和y$之间的关系，并根据新的自变量$x$来预测$y$。但是高斯过程并非如此，而是利用先验知识，来拟合函数$f$的概率分布：</p><script type="math/tex; mode=display">f: R \rightarrow R = p(f | x, X, y) \tag{2}</script><p>首先进行假设，假设给定一些$X$的值，我们对$Y$建模，并假设对应的这些$Y$值 服从联合高斯分布。一个高斯过程是由均值函数$m(x)$和协方差函数$k(x, x^”)$（<strong>正因为有均值函数核协方差函数确定，所以高斯过程为非参数模型</strong>）确定的。它可理解成联合高斯分布的一个生成过程，那么联合高斯分布可以通过如下表示：</p><script type="math/tex; mode=display">\left[\begin{array}{ccc}Y_{1} \\\vdots \\Y_{n}\end{array}\right] = N(\left[\begin{array}{ccc}0 \\\vdots \\0\end{array}\right], \left[\begin{array}{ccc}K_{11} & \cdots & K_{1n} \\\vdots & \ddots & \vdots \\K_{n1} & \cdots & K_{nn}\end{array}\right]) \tag{3}</script><p>上面的形式就是多元高斯分布形式：$X = N(\mu, \Sigma)$，其中均值$\mu$不用担心，可以先假设为0（即使 $\mu≠0$，我们也可以在预测结束后把$\mu$加回到结果函数值中。所以配置$ \mu $非常简单），如上面的(3)所示。那么协方差矩阵怎么得到呢？</p><p>在上面通俗理解的段落中提到了，$X$比较相似，那么对应的$y$值的相关性也就较高。换言之，协方差矩阵是<strong>$X$的函数</strong>。（而不是$y$的函数）。也就是说，可以假设协方差矩阵的每个元素为对应的两个$x$值的一个相似性度量，相似性度量函数是<strong>核函数(Kernel Functions</strong>，高斯过程回归中常用的Kernel是<code>squared exponential kernel</code>，在<code>scikit-learn</code>中GP接口中默认使用的<code>kernel</code>就是该核函数)。</p><p>协方差矩阵不仅仅描述了这个分布的形状，也最终决定了我们想要预测的函数所具有的特性。我们通过求核函数$K$的值来生成协方差矩阵，这个核函数通常也被称为协方差函数，作用在<strong>两两成对的所有测试点</strong>上。核函数接收到的<strong>输入是两个点</strong>，，返回的是一个<strong>标量</strong>，表达了这<strong>两个点之间的相似度</strong>：</p><script type="math/tex; mode=display">K = \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}, \Sigma = Cov(X, X^")=k(x, x^") \tag{4}</script><p>协方差矩阵$\Sigma_{ij}$ 描述的是第 $i$个点和第$j$个点之间的相互影响，这和多元高斯分布的定义一致。在多元高斯分布的定义中，$\Sigma_{ij}$ 定义了第$i$个随机变量和第$j$个随机变量之间的相关性。由于核函数描述的输入值之间的相似度，它便控制了这个拟合函数可能拥有的形状。</p><h3 id="3-1-高斯过程回归（GPR）"><a href="#3-1-高斯过程回归（GPR）" class="headerlink" title="3.1 高斯过程回归（GPR）"></a>3.1 高斯过程回归（GPR）</h3><p>假设对输入的$x^”$对应的$y^”$进行预测，那么可以简述如下：</p><h4 id="3-1-1-先验分布（Prior-Distribution）"><a href="#3-1-1-先验分布（Prior-Distribution）" class="headerlink" title="3.1.1 先验分布（Prior Distribution）"></a>3.1.1 先验分布（Prior Distribution）</h4><p>训练样本：$x=[x_1, x_2, x_3, …, x_n], y=[y_1, y_2, y_3, …, y_n]$，测试样本：$x^”$,$y^”$。先验就是所有的随机变量遵循一个均值的多元高斯分布（假设随机变量已经零均值化，均值为0）：</p><script type="math/tex; mode=display">\left[\begin{array}{ccc}y \\y^{"}\end{array}\right] \sim N(0, \left[\begin{array}{ccc}K & K_{"}^T \\K_{"} & K^{""}\end{array}\right] ) \tag{5}</script><p>其中，GP定义了核函数$k$，就可以知道：</p><script type="math/tex; mode=display">K=\left[\begin{array}{ccc}k(x_1, x_1) & k(x_1, x_2) & \cdots & k(x_1, x_n)\\k(x_2, x_1) & k(x_2, x_2) & \cdots & k(x_2, x_n)\\\vdots & \vdots & \ddots & \vdots\\k(x_n, x_1) & k(x_n, x_2) & \cdots & k(x_n, x_n)\\\end{array}\right]  \tag{6}</script><script type="math/tex; mode=display">K_{"} = [k(x^", x_1), k(x^", x_2), \cdots, k(x^", x_n)]\\k^{""} = k(x^", x^") \tag{7}</script><p>通过训练样本就确定了先验分布。如果将$x^”$,$y^”$拓展到$n^”$维，我们希望得到 一个 $n^”$ 维的预测向量 $f^”$:</p><script type="math/tex; mode=display">\left[\begin{array}{ccc}y \\y^{"}\end{array}\right] \sim N(\left(\begin{array}{ccc}\mu\\\mu^{"}\end{array}\right), \left[\begin{array}{ccc}K & K_{"}^T \\K_{"} & K^{""}\end{array}\right] ) \tag{8}</script><p>其中，$K_{n \times n}=K(x, x)$，$K_{“ n^” \times n}$， $K^{“”}_{n^” \times n^”}$。</p><h4 id="3-1-2-后验分布（Posterior-Distribution-）"><a href="#3-1-2-后验分布（Posterior-Distribution-）" class="headerlink" title="3.1.2 后验分布（Posterior Distribution ）"></a>3.1.2 后验分布（Posterior Distribution ）</h4><p>后验分布利用了训练样本的集中信息，因为训练样本和测试样本都使用的同一高斯过程，所以训练样本$x$和测试样本的$x^”$比较接近，那么对应的$y$也是比较接近，根据多元高斯分布式的条件分布，就可以得到：</p><script type="math/tex; mode=display">y^"|y = N(K_"K^{-1}y, K^{""}-K_"K^{-1}K_"^T) \tag{9}</script><p>根据上述公式可知，测试集后验分布的期望值就是训练样本$y$的线性组合，而方差和训练样本的$y$无关，只与$x$有关。如果在拓展到多维时且均值不为0(基于3.1.1中的公式（8）的情况)，则根据贝叶斯回归得到：</p><script type="math/tex; mode=display">\begin{align*}&y^"|x^", x, y = N(\mu ^ {"}, z^")\\&\mu ^ {"} = \mu(x^") + K_"K^{-1}(y-u(x))\\&z^" = K^{""}-K_"K^{-1}K_"^T\end{align*}</script><h4 id="3-1-3-GPR训练"><a href="#3-1-3-GPR训练" class="headerlink" title="3.1.3 GPR训练"></a>3.1.3 GPR训练</h4><p>在GPR中训练集给定了，核函数(也叫协方差函数)给定了，基本都给定了，那结果也应该定了？虽然都给定了，但是<code>kernel</code>中的<strong>超参数</strong>并没有给定。所谓的训练就是训练得到这个超参数。只有当超参数确定后我们才确定了高斯过程得到的分布。因此，GPR的训练核心是学习<code>kernel</code>的超参数。</p><h3 id="3-2-小结"><a href="#3-2-小结" class="headerlink" title="3.2 小结"></a>3.2 小结</h3><ul><li>高斯过程可以看做是多维高斯分布向无限维的扩展，我们可以将$y=y_1,y_2,…,y_n$看作是从$n$维高斯分布中随机抽取的一个点</li><li>对高斯过程的刻画，如同高斯分布一样，也是用均值和方差来刻画。通常在应用高斯过程 $f∼GP(\mu,K)$的方法中，都是假设均值$\mu$为零，而协方差函数$K$则是根据具体应用而定。</li><li>高斯回归的本质其实就是通过一个映射把自变量从低维空间映射到高维空间（类似于支持向量机中的核函数将低维线性不可分映射为高维线性可分），只需找到合适的核函数，就可以知道 $p(f|x,X,y)$的分布，最常用的就是高斯核函数</li></ul><h2 id="4-机器学习中的高斯过程"><a href="#4-机器学习中的高斯过程" class="headerlink" title="4. 机器学习中的高斯过程"></a>4. 机器学习中的高斯过程</h2><p><strong>针对机器学习的高斯过程(Gaussian Processes for Machine Learning,即 GPML)</strong> 是一个通用的监督学习方法，主要被设计用来解决<em>回归</em>问题。 它也可以扩展为<em>概率分类(probabilistic classification)</em>，但是在当前的实现中，这只是<em>回归</em>学习的一个后续处理。</p><p>GPML的优势如下:</p><blockquote><ul><li>预测是对观察值的插值（至少在普通相关模型上是的）.</li><li>预测是带有概率的(Gaussian)。所以可以用来计算经验置信区间和超越概率 以便对感兴趣的区域重新拟合（在线拟合，自适应拟合）预测。</li><li>多样性: 可以指定不同的线性回归模型 <a href="http://sklearn.lzjqsdd.com/modules/linear_model.html#linear-model" target="_blank" rel="noopener">linear regression models</a> 和相关模型 <a href="http://sklearn.lzjqsdd.com/modules/gaussian_process.html#correlation-models" target="_blank" rel="noopener">correlation models</a> 。 它提供了普通模型，但也能指定其它静态的自定义模型</li></ul></blockquote><p>GPML的缺点如下:</p><blockquote><ul><li>不是稀疏的，它使用全部的样本/特征信息来做预测。</li><li>多维空间下会变得低效 – 即当特征的数量超过几十个,它可能确实会表现很差，而且计算效率下降。</li><li>分类只是一个后处理过程, 意味着要建模，首先需要提供试验的完整浮点精度标量输出 <img src="http://sklearn.lzjqsdd.com/_images/math/276f7e256cbddeb81eee42e1efc348f3cb4ab5f8.png" srcset="/img/loading.gif" alt="y"> 来解决回归问题。</li></ul></blockquote><h2 id="5-GP建模时的问题"><a href="#5-GP建模时的问题" class="headerlink" title="5. GP建模时的问题"></a>5. GP建模时的问题</h2><p>用GP对数据建模，首先得考虑下面几个问题：<br>（1）如何选择<strong>核函数</strong>？<br>        选什么核函数（这点非常重要）？选最常用的SE还是SMP还是混合（基于对数据的理解）？<br>（2）超参数<strong>物理含义</strong>是什么？<br>       训练完模型后，大部分情况下测试效果不会很好，再回头看看学到的超参数变化如何？那么问题来了，这些超参数又代表什么物理含义呢（基于对核函数的理解）？<br>（3）<strong>非高斯似然</strong>函数<br>       对常见的分类任务来说，似然非高斯分布，此时共轭优势不复存在，如何找到最有效逼近后验的方法（EP？VI？）<br>（4）<strong>异方差</strong><br>       事实上，很多标签都不是简单的高斯分布，甚至标签噪声都是异方差分布（比如标签值跨度超多几个数量级），又该如何解决（可参考warped GPs）？</p><ol><li><a href="https://blog.csdn.net/paulfeng20171114/article/details/80276061" target="_blank" rel="noopener">透彻理解高斯过程Gaussian Process (GP)</a></li><li><a href="https://cloud.tencent.com/developer/article/1353538" target="_blank" rel="noopener">[重点]机器学习-高斯过程和随机过程回归</a></li><li><a href="https://blog.csdn.net/lj6052317/article/details/78772494" target="_blank" rel="noopener">机器学习中的高斯过程</a></li><li><a href="https://www.bilibili.com/video/av35626047/" target="_blank" rel="noopener">高斯过程视频讲解</a></li><li><a href="https://www.youtube.com/watch?v=vU6AiEYED9E&amp;list=PLD0F06AA0D2E8FFBA&amp;index=151&amp;t=0s" target="_blank" rel="noopener">[视频]Gaussian processes - definition and first examples</a></li><li><a href="https://www.youtube.com/watch?v=4vGiHC35j9s" target="_blank" rel="noopener">[视频]Machine learning - Introduction to Gaussian processes</a></li><li><a href="https://www.zhihu.com/question/46631426" target="_blank" rel="noopener">如何通俗易懂的理解高斯过程</a></li><li><a href="[https://zh.wikipedia.org/zh/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B](https://zh.wikipedia.org/zh/高斯过程">高斯过程-维基百科</a>)</li><li><a href="https://www.jgoertler.com/visual-exploration-gaussian-processes/" target="_blank" rel="noopener">可视化理解高斯过程</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kernel</tag>
      
      <tag>bayes</tag>
      
      <tag>gaussian</tag>
      
      <tag>回归</tag>
      
      <tag>regression</tag>
      
      <tag>GPR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python下的c/c++扩展方式详解</title>
    <link href="/2019/04/18/%E5%9C%A8python%E4%B8%8B%E7%9A%84c%E6%88%96c++%E6%89%A9%E5%B1%95%E6%96%B9%E5%BC%8F%E8%AF%A6%E8%A7%A3/"/>
    <url>/2019/04/18/%E5%9C%A8python%E4%B8%8B%E7%9A%84c%E6%88%96c++%E6%89%A9%E5%B1%95%E6%96%B9%E5%BC%8F%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p><code>python</code>作为一种胶水语言，最重要的是非常容易的和<code>shell</code>，<code>c/c++</code>之间进行相互扩展，而本文主要是对<code>python</code>的<code>c/c++</code>扩展进行详细讲解，并作为笔记便于以后的使用。</p><h2 id="为什么要进行c-c-扩展"><a href="#为什么要进行c-c-扩展" class="headerlink" title="为什么要进行c/c++扩展"></a>为什么要进行c/c++扩展</h2><p><code>python</code>之所以使用广泛，主要是因为使用简单，第三方扩展库强大，对于某个问题，能迅速的验证自己的思路或某个原型。但是，<code>python</code>的效率是比较低的，因此很多涉及到效率的时候需要使用<code>c/c++</code>来实现，为<code>python</code>提供接口即可。这样混合编程后，能达到快速+效率的双重目的。</p><h2 id="python的c-c-扩展"><a href="#python的c-c-扩展" class="headerlink" title="python的c/c++扩展"></a>python的c/c++扩展</h2><p>在<code>python</code>下的<code>c/c+</code>扩展有很多方式，主要有<code>cffi/ctypes</code>，<code>cpython</code>和<code>cython</code>这几种方式，各个方式都有优势。下面对这几个方式进行详细的说明。</p><h3 id="cffi来进行c扩展"><a href="#cffi来进行c扩展" class="headerlink" title="cffi来进行c扩展"></a>cffi来进行c扩展</h3><p><code>cffi</code>是连接<code>c</code>和<code>python</code>的桥梁，是在<code>pypy</code>中分离出来的一个库，这个和<code>python</code>自带的<code>ctypes</code>模块很相似，但是<code>cffi</code>更加通用。下面我们定义了一个<code>pi.h</code>和<code>pi.c</code>文件：</p><pre><code class="lang-c">// file: pi.h#ifndef C_EXTEND_PYTHON_PI_H#define C_EXTEND_PYTHON_PI_Hfloat pi_approx(float n);#endif *//C_EXTEND_PYTHON_PI_H*</code></pre><pre><code class="lang-c">// file: pi.cfloat pi_approx(float n){    return 3.1415926 * n * n;}</code></pre><p>使用cffi模块来对这两个文件进行编译，会生成一个<code>c source</code>文件和<code>.so</code>或<code>.pyd</code>文件，可以在python下直接引用，进行使用，编译文件如下：</p><pre><code class="lang-python">from cffi import FFIffi_builder = FFI()ffi_builder.cdef(&quot;&quot;&quot;    float pi_approx(float n);&quot;&quot;&quot;)# This describes the extension module &quot;_pi_cffi&quot; to produce.ffi_builder.set_source(&quot;_pi_cffi&quot;,                       &quot;&quot;&quot;                           #include &quot;pi.c&quot;   // the C header of the library                       &quot;&quot;&quot;,                       libraries=[])  # library name, for the linkerif *__name__ == &quot;__main__&quot;:    ffi_builder.compile(verbose=True )</code></pre><p>直接运行该编译文件则可创建python的c扩展库：<code>.so</code>或<code>.pyd</code>文件；另外也会生成<code>_pi_cffi.c</code>的源码。将<code>.so</code>或<code>.pyd</code>文件放如对应的目录即可实现引用。这种方式比较方便，能迅速构建c扩展。<br><a id="more"></a></p><h3 id="cpython的方式"><a href="#cpython的方式" class="headerlink" title="cpython的方式"></a>cpython的方式</h3><p>这种方式，是利用Python的C-API接口来构建python模块。Python的每个版本官方都有相应的C-API文档，每个版本API可能存在一些不同。特别是Python2和Python3（本文是基于Python3的版本）之间。下面我们就通过代码来进行说明。主要实现一个列表中所有元素的和。代码如下：</p><pre><code class="lang-c">/* * #include &quot;/opt/anaconda3/include/python3.6m/Python.h&quot; * Python.h has all the required function definitions to manipulate the Python objects */#include &quot;D:\Anaconda3\include\Python.h&quot;//This is the function that is called from your python codestatic PyObject* addList_add(PyObject* self, PyObject* args){    PyObject * listObj; // 申请一个python对象来存list    // args参数解析, 并将值存入到listObj变量    if (! PyArg_ParseTuple( args, &quot;O&quot;, &amp;listObj ))        return NULL;    // 获取list的长度    long length = PyList_Size(listObj);    // 循环获取list中的值, 并求和    long i, sum =0;    for (i = 0; i &lt; length; i++) {        //get an element out of the list - the element is also a python objects        PyObject* temp = PyList_GetItem(listObj, i);         //we know that object represents an integer - so convert it into C long        long elem = PyLong_AsLong(temp);        sum += elem;    }    //value returned back to python code - another python object    //build value here converts the C long to a python integer    return Py_BuildValue(&quot;l&quot;, sum);}// 这里是为上面的函数定义一个docstringstatic char addList_docs[] =&quot;add(  ): add all elements of the list\n&quot;;/* * This table contains the relavent info mapping -&lt;function-name in python module&gt;, &lt; * actual-function&gt;,&lt;type-of-args the function expects&gt;, &lt;docstring associated with the function&gt; */static PyMethodDef addList_funcs[] = {    {&quot;add&quot;, (PyCFunction)addList_add, METH_VARARGS, addList_docs},    {NULL, NULL, 0, NULL}};/*defined PyModuleDef struct*/static struct PyModuleDef moduledef = {        PyModuleDef_HEAD_INIT,        &quot;addList&quot;,  // module name        NULL,        -1,        addList_funcs, // functions        NULL,        NULL,        NULL,        NULL};/* * addList is the module name, and this is the initialization block of the module. * &lt;desired module name&gt;, &lt;the-info-table&gt;, &lt;module&#39;s-docstring&gt; */static PyObject *SpamError = NULL;// 创建模块PyMODINIT_FUNC PyInit_addList(void){    PyObject *m;    m = PyModule_Create(&amp;moduledef);    if (m == NULL)        return NULL;    SpamError = PyErr_NewException(&quot;addList.error&quot;, NULL, NULL);    Py_INCREF(SpamError);    PyModule_AddObject(m, &quot;error&quot;, SpamError);    return m;}</code></pre><p>完成上面功能后，编写一个setup.py即可：</p><pre><code class="lang-python">from distutils.core import setup, Extensionsetup(name=&#39;addList&#39;, version=&#39;1.0&#39;, ext_modules=[Extension(&#39;addList&#39;, [&#39;addList.c&#39;])])</code></pre><p>运行<code>python setup.py build</code>即可进行编译。编译后悔生成相应的<code>.so</code>或<code>.pyd</code>文件文件：模块名称为addList。python可以直接进行引用该文件即可。<br>这种方法比较灵活，但是需要对python的C-API相对有了解。官方是推荐这种方法来进行扩展的，比其他方法都要原生，对c++也支持的非常好。</p><h3 id="CYTHON的方式进行扩展"><a href="#CYTHON的方式进行扩展" class="headerlink" title="CYTHON的方式进行扩展"></a>CYTHON的方式进行扩展</h3><p><code>cython</code>可以看成一个python的超集，不仅结合了c的语法，同时能使用c的东西。它的运行过程主要是将<code>cython</code>实现的代码转成c代码，并进行编译形成Python的扩展库。<code>cython</code>不仅仅进行了Python与C之间的转换，还对部分操作进行了优化。在<code>cython</code>可以使用c的任何东西, 同时支持Python的函数和类的方式编程。目前很多第三方库都有使用它，比如<code>scikit-learn</code>，<code>scipy</code>等等很多。具体的一些用法可以参考它的官方文档。下面我通过<code>cython</code>从不同的角度来展示它的用法。</p><h4 id="使用cython连接c文件和python"><a href="#使用cython连接c文件和python" class="headerlink" title="使用cython连接c文件和python"></a>使用cython连接c文件和python</h4><p>假设现在我写一个<code>c</code>的一些方法，包括了头文件和源文件：</p><pre><code>// file: csample.h#include &lt;stdio.h&gt;#include &lt;math.h&gt;int add(int x, int y);int sub(int x, int y);</code></pre><pre><code class="lang-c">// file: csample.cint add(int x, int y){    return x + y;}int sub(int x, int y){    return x - y;}</code></pre><p>现在我要用<code>cython</code>来使用上面定义的c方法，首先创建一个<code>cython</code>的头文件<code>csample.pxd</code>文件，文件中引用<code>csample.h</code>的方法,，并定义一个<code>Point</code>结构体：</p><pre><code class="lang-c">// file csample.pxdcdef extern from &quot;csample.h&quot;:    int add(int x, int y)    int sub(int x, int y)ctypedef struct Point:    double x    double y</code></pre><p>再新建一个<code>pyx</code>的文件，通过<code>cython</code>来定义<code>python</code>的接口：</p><pre><code class="lang-python">// file: sample.pyxcimport csamplefrom cpython.pycapsule cimport *from libc.stdlib cimport malloc, freedef add(x, y):    return csample.add(x, y)def sub(x, y):    return csample.sub(x, y)cdef delPoint(object obj):    pt = &lt;csample.Point *&gt; PyCapsule_GetPointer(obj, &quot;Point&quot;)    free(&lt;void *&gt; pt)def Point(x, y):    cdef csample.Point *p    p = &lt;csample.Point *&gt; malloc(sizeof(csample.Point))    if p == NULL:        raise MemoryError(&quot;No memoery to make a point!&quot;)    p.x = x    p.y = y    return PyCapsule_New(&lt;void *&gt; p, &quot;Point&quot;, &lt;PyCapsule_Destructor&gt;delPoint)</code></pre><p>这样整个过程就实现了，基本和上面<code>cpython</code>的过程类似，只是采用的方式不同。下面编写一个<code>setup.py</code>的文件即可进行编译<code>python setup.py build</code>。生成对一个的扩展包。</p><pre><code class="lang-python"># file: setup.pyfrom distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Distutils import build_extext_modules = [    Extension(&#39;sample&#39;,  # 扩展包名称              [&#39;sample.pyx&#39;, &#39;E:\\WorkSpace\\2019\CythonExample\\c_mix_cython\\csample.c&#39;], # 用到的源文件              libraries=[],  # 依赖的库              library_dirs=[&#39;.&#39;])]setup(    name=&#39;Sample extension module&#39;,    cmdclass={&#39;build_ext&#39;: build_ext},    ext_modules=ext_modules)</code></pre><h4 id="在cython中使用头文件-pxd"><a href="#在cython中使用头文件-pxd" class="headerlink" title="在cython中使用头文件(pxd)"></a>在cython中使用头文件(pxd)</h4><p>头文件的使用具有一定的好处，方便大量代码的维护等等。在<code>cython</code>中也可以使用头文件，在上面的例子中已经应用到了。下面再进行说明。首先创建一个头文件， 定义相应的方法，结构等：</p><pre><code class="lang-python"># sample.pxdcdef int function1(int a, int b=*)cdef double add_one(double x)cdef class A:    cdef public int a, b    cpdef void foo(self, double x)</code></pre><p>再根据头文件实现对应的方法：</p><pre><code class="lang-python"># file: sample.pyxcdef int function1(int a, int b=10):    cdef int x = a - b    return x + a * bcdef double add_one(double x):    return x + 1cdef class A:    def __init__(self, int b=0):        self.a = 13        self.b = b    cpdef void foo(self, double x):        print(x + add_one(10.25))</code></pre><p>下面编写<code>setup.py</code>进行编译即可：</p><pre><code class="lang-python"># setup.pyfrom distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Distutils import build_extext_modules = [    Extension(&#39;sample&#39;,              [&#39;sample.pyx&#39;],              libraries=[],              library_dirs=[&#39;.&#39;])]setup(    name=&#39;Sample extension module&#39;,    cmdclass={&#39;build_ext&#39;: build_ext},    ext_modules=ext_modules)</code></pre><h4 id="在cython中使用指针（pointer）"><a href="#在cython中使用指针（pointer）" class="headerlink" title="在cython中使用指针（pointer）"></a>在cython中使用指针（<code>pointer</code>）</h4><p>在cython中也是可以使用指针的，可以将python对象转化为指针在<code>cython</code>中使用，下面是具体的示例，代码中给了注释说明，这里就不详细介绍了。</p><pre><code class="lang-python">from cpython cimport arrayfrom cython.parallel import prangecimport numpy as npimport cythonimport numpy as npctypedef unsigned long long ullongcdef inline ullong add_list(unsigned long *array, unsigned long size):    &quot;&quot;&quot;注意数据类型,在c中, int, long, long long等不同的数据类型, 边界不同, 因此必须要满足所有的数据在最大边界内, 否则结果是不正确的&quot;&quot;&quot;    cdef ullong i = 0    cdef ullong sum_v = 0    for i in prange(size, nogil=True):        sum_v += array[i]    return sum_vdef addList(list_array, long s):    &quot;&quot;&quot;将一个python对象list传递给一个函数, 该函数的接收参数是一个指针(int *)类型, 通过指针来操作python的list对象.    reference: Cython: can&#39;t convert Python object to &#39;double *&#39;:        https://stackoverflow.com/questions/17014379/cython-cant-convert-python-object-to-double    &quot;&quot;&quot;    cdef array.array p_array =array.array(&#39;L&#39;, list_array)  # transform c array    return add_list(p_array.data.as_ulongs, s)def addList_numpy(list_array, long s):    &quot;&quot;&quot;通过numpy的ndarray将list对象转换为c的指针类型    reference: tutorials Numpy Pointer To C with Cython        https://github.com/cython/cython/wiki/tutorials-NumpyPointerToC    &quot;&quot;&quot;    cdef np.ndarray[long, ndim=1, mode=&#39;c&#39;] p_array = np.array(list_array)  # transform c memory type    return add_list(&lt;unsigned long *&gt; &amp;p_array[0], s)  # &amp;p_array[0] or &lt;int*&gt; p_array.data</code></pre><h4 id="在cython中使用结构体（struct）"><a href="#在cython中使用结构体（struct）" class="headerlink" title="在cython中使用结构体（struct）"></a>在cython中使用结构体（struct）</h4><p><code>cython</code>可以使用c的任何东西，因此结构体也是支持的，这里用一个小的例子来说明在<code>cython</code>中的使用。代码如下所示，如果在<code>cython</code>中返回结构体，对应到<code>python</code>中就是一个字典的对象。</p><pre><code class="lang-python">ctypedef struct Node:    double x    double ycdef Node nodedef setNode(double x, double y):    cdef Node *p_node = &amp;node    p_node.x = x    p_node.y = y    return p_node[0]  # 返回了字典</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面主要对<code>cffi</code>, <code>cpython</code>和<code>cython</code>的三种方式进行了详解。这三种方式也是主流的方式，其中比较常用的应该是<code>cython</code>的方式，特别是和<code>numpy</code>的结合。其次是<code>cffi</code>的方式，但是还是推荐<code>cpython</code>的方式，这种方式更加原生，和python结合的更好，但是，难度也是最高的。在工作应用中可以根据自己的需要选择不同的方式。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>c/c++</tag>
      
      <tag>numpy</tag>
      
      <tag>cython</tag>
      
      <tag>优化</tag>
      
      <tag>compile</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多级网络优化的规划问题简化-Python预演</title>
    <link href="/2019/04/11/%E5%A4%9A%E7%BA%A7%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E4%B8%AD%E7%9A%84%E8%A7%84%E5%88%92%E9%97%AE%E9%A2%98%E7%AE%80%E5%8C%96/"/>
    <url>/2019/04/11/%E5%A4%9A%E7%BA%A7%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E4%B8%AD%E7%9A%84%E8%A7%84%E5%88%92%E9%97%AE%E9%A2%98%E7%AE%80%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="背景和目标"><a href="#背景和目标" class="headerlink" title="背景和目标"></a>背景和目标</h2><p>在物流网络中，经常存在需要优化的网络结构，比如从生产中心到一级仓库 ，到二级仓库再到配送中心，这样形成一个多级的分仓体系。在实际执行过程中会遇到DC层或TC层的选址问题，一般会遇到优化问题，优化的目标主要考虑多个方面的成本，如运输成本，仓库成本，转运成本等等。如果按照多级网络的模式进行考虑，一般规划问题会成为非线性的整数规划问题，在求解上是非常难且建模也比较难。</p><p>现在对于这样的其中选址的问题进行思考：假设我们只是考虑运输成本或距离上的成本，DC层的仓库可以向周边的仓库进行供货，而且保证TC层的一个仓库只能唯一的DC层仓库供货。从常规的建模一般都是在DC层设置一个变量X， TC层设置一个变量Y，来计算目标，并设置约束，最终来进行求解，那么这个问题能简化么？</p><p>如果能简化那么这个问题就能推广到多层的选址问题，不需要非线性整数规划的方式进行求解。能大大的简化整个求解，可能不是最优的解，但是能得到较优的解。经过仔细的实验和思考，是能进行简化的。将DC层和TC层的地址看为一个变量，这样就减少一个变量，转化为了线性规划的问题（LP）。DC层对TC层为1对N的方式，但是TC只能属于一个DC。而且可以控制DC层的地点或TC层的地点的个数。解决了选址中遇到的所有问题。<br><img src="/uploads/wuliufencangtixi.jpg" srcset="/img/loading.gif" alt="物流的多级分仓体系"> </p><h2 id="数据的准备"><a href="#数据的准备" class="headerlink" title="数据的准备"></a>数据的准备</h2><p>这里我们准备了一些城市的数据，其中有DC，TC层和最终的地点。数据存储于mysql数据库中，下面定义一个数据抽取的class，来进行数据查询，并生成需要的距离矩阵。便于后续使用该矩阵来进行优化目标值的计算。<br><a id="more"></a></p><pre><code class="lang-python"># import packagesimport pymprogimport pymysqlimport pandas as pdimport numpy as npfrom collections import defaultdictclass MysqlClient(object):    &quot;&quot;&quot;    create mysql connect client    &quot;&quot;&quot;    def __init__(self, host, user, password, database=&quot;glc_lpp_dev&quot;, port=3306):        self.host = host or &quot;127.0.0.1&quot;        self.user = user        self.passwd = password        self.database = database        self.port = port        self.connection = self._get_connection()    def _get_connection(self):        &quot;&quot;&quot;get mysql connection client&quot;&quot;&quot;        try:            _connection = pymysql.connect(host=self.host, user=self.user, password=self.passwd, port=self.port,                                          database=self.database, charset=&quot;utf8&quot;)        except Exception as error:            print(&quot;Connect mysql server:%s has error, the error is: %s&quot; % (self.host, error))            _connection = None        return _connection    def query(self, sql_scripts):        &quot;&quot;&quot;        :param sql_scripts: string, use this sql scripts get data.        :return: tuple        &quot;&quot;&quot;        if self.connection is None:            raise Exception(&quot;Get mysql connection failed. The connection is NoneType object.&quot;)        cursor = self.connection.cursor()        cursor.execute(sql_scripts)        fetch_data = cursor.fetchall()        cursor.close()        return fetch_data    def close(self):        if self.connection is not None:            self.connection.commit()            self.connection.close()</code></pre><h2 id="规划问题的简化方案"><a href="#规划问题的简化方案" class="headerlink" title="规划问题的简化方案"></a>规划问题的简化方案</h2><p>在本次的实验过程中使用pymprog来进行LP求解，并定义整个网络模型，通过距离矩阵来定义目标和约束，具体如下面代码中的说明，先定义决策变量（这里定义为了0-1整数变量），再定义目标函数（本示例中主要定义了一个距离相关的优化目标），再定义约束条件，最终求解，并格式化输出。其中约束条件定义非常灵活：</p><ol><li>一个TC只能属于一个DC</li><li>一个DC可以有多个TC，这儿并定义了输入的DC都必须被选到</li></ol><p>示例代码如下：</p><pre><code class="lang-python">def solve_link(distance_matrix_frame, flag=1):    &quot;&quot;&quot;Find the link matrix by linear programming slove    parameters    ----------    distance_matrix_frame:         pandas.DataFrame, src positions to dst positions, index is src position.    return    ------        DataFrame, the link dataframe, the index is src positions and the columns is dst positions.    &quot;&quot;&quot;    t_frame = distance_matrix_frame.T.to_dict()    src_pos_, dst_pos_ = distance_matrix_frame.index, distance_matrix_frame.columns    constant = []    for s in src_pos_:        s_list = []        for d_ in dst_pos_:            s_list.append(t_frame[s][d_])        constant.append(s_list)     pymprog.begin(&#39;link&#39;)    index, columns = range(len(src_pos_)), range(len(dst_pos_))  # the number of variable    x = pymprog.var(&#39;x&#39;, pymprog.iprod(index, columns), bool)  # defined variable    min_value = sum([x[(i, j)] * constant[i][j] for i in index  for j in columns])    pymprog.minimize(min_value, &quot;min_distance&quot;)  # optimizer target: minimize    # ------------  defined constraints -------------- #    # defined next layer map one city    for j in columns:        st_init = 0        for i in index:            st_init += x[(i, j)]        pymprog.st(1 &lt;= st_init &lt;= 2)    # defined current layer at least one    if flag:        for i in index:            ct_init = 0            for j in columns:                ct_init += x[(i, j)]            pymprog.st(1 &lt;= ct_init)    # ------------  defined constraints -------------- #    # soleve linear programing    pymprog.solve()    # build up result by dataframe    output_dict = defaultdict(dict)    for i in index:        for j in columns:            output_dict[dst_pos_[j]][src_pos_[i]] = x[(i, j)].primal    # pymprog.sensitivity()    pymprog.end()    return pd.DataFrame(data=output_dict)</code></pre><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>通过一些数据进行试验上面的思路，下面生产中心选取了两个：天津和重庆，DC层有4个点，TC层 有7个点，配送中心有很多个城市，如下设置所示。通过sql查询得到数据库中的距离矩阵。后面将所有的中文地址转换为英文，主要便于后面的可视化。</p><pre><code class="lang-python"># built the network struct, src position, depot and dst positiondst_position = [&quot;宜昌&quot;, &quot;长沙&quot;, &quot;岳阳&quot;, &quot;无锡&quot;, &quot;合肥&quot;, &quot;徐州&quot;, &quot;常州&quot;,&quot;哈尔滨&quot;, &quot;长春&quot;, &quot;秦皇岛&quot;, &quot;呼和浩特&quot;, &quot;银川&quot;, &quot;西宁&quot;,                 &quot;酒泉&quot;, &quot;庆阳&quot;, &quot;许昌&quot;, &quot;吴忠&quot;, &quot;鄂尔多斯&quot;, &quot;广元&quot;, &quot;巴中&quot;, &quot;都江堰&quot;, &quot;南充&quot;, &quot;广安&quot;, &quot;乐山&quot;, &quot;宜宾&quot;, &quot;南阳&quot;,                &quot;衡阳&quot;, &quot;大同&quot;, &quot;丽江&quot;, &quot;攀枝花&quot;, &quot;玉林&quot;, &quot;柳州&quot;, &quot;桂林&quot;, &quot;中山&quot;, &quot;南平&quot;, &quot;泉州&quot;, &quot;雅安&quot;, &quot;怀化&quot;, &quot;荆州&quot;, &quot;黄山&quot;               ]src_position = [&quot;重庆&quot;, &quot;天津&quot;]depot_position = {    &quot;level_1&quot;: [&quot;北京&quot;, &quot;成都&quot;, &quot;西安&quot;, &quot;武汉&quot;],     &quot;level_2&quot;: [&quot;绵阳&quot;, &quot;沈阳&quot;, &quot;兰州&quot;, &quot;南昌&quot;, &quot;杭州&quot;, &quot;南宁&quot;, &quot;昆明&quot;]}</code></pre><pre><code class="lang-python">middle_layer = []for k, v in depot_position.items():    middle_layer.extend(v)node_list = dst_position + src_positionimport picklewith open(&quot;/opt/notebook/distance.pkl&quot;, &quot;rb&quot;) as picker:    data = pickle.load(picker)with open(&quot;/opt/notebook/layer_distance.pkl&quot;, &quot;rb&quot;) as layer:    layer_dist = pickle.load(layer)layer_dist</code></pre><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th style="text-align:right">绵阳</th><th style="text-align:right">沈阳</th><th style="text-align:right">兰州</th><th style="text-align:right">南昌</th><th style="text-align:right">杭州</th><th style="text-align:right">南宁</th><th style="text-align:right">昆明</th></tr></thead><tbody><tr><td style="text-align:left">北京</td><td style="text-align:right">1691</td><td style="text-align:right">685</td><td style="text-align:right">1617</td><td style="text-align:right">1470</td><td style="text-align:right">1329</td><td style="text-align:right">2426</td><td style="text-align:right">2834</td></tr><tr><td style="text-align:left">成都</td><td style="text-align:right">120</td><td style="text-align:right">2474</td><td style="text-align:right">1039</td><td style="text-align:right">1496</td><td style="text-align:right">1857</td><td style="text-align:right">1325</td><td style="text-align:right">1068</td></tr><tr><td style="text-align:left">西安</td><td style="text-align:right">606</td><td style="text-align:right">1775</td><td style="text-align:right">647</td><td style="text-align:right">1197</td><td style="text-align:right">1417</td><td style="text-align:right">1932</td><td style="text-align:right">1561</td></tr><tr><td style="text-align:left">武汉</td><td style="text-align:right">921</td><td style="text-align:right">1845</td><td style="text-align:right">1368</td><td style="text-align:right">370</td><td style="text-align:right">859</td><td style="text-align:right">1294</td><td style="text-align:right">1815</td></tr></tbody></table></div><p>通过线性规划求解得到了连接矩阵如下面结果所示，可以看出DC层下有多个TC层，但是TC只属于一个DC。<strong>本文中没有控制DC和TC的数量。实际是可以加入进去的</strong>。后续对得到结果通过networkx进行可视化。</p><pre><code class="lang-python">nodes_res = solve_link(data, flag=0)layer_res = solve_link(layer_dist)def cn2pingyin(s: str)-&gt;str:    from pypinyin import lazy_pinyin    return &quot;&quot;.join(lazy_pinyin(s))nodes_res.index = nodes_res.index.map(cn2pingyin)nodes_res.columns = nodes_res.columns.map(cn2pingyin)layer_res.index = layer_res.index.map(cn2pingyin)layer_res.columns = layer_res.columns.map(cn2pingyin)layer_res</code></pre><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th style="text-align:right">lanzhou</th><th style="text-align:right">nanning</th><th style="text-align:right">nanchang</th><th style="text-align:right">kunming</th><th style="text-align:right">hangzhou</th><th style="text-align:right">shenyang</th><th style="text-align:right">mianyang</th></tr></thead><tbody><tr><td style="text-align:left">beijing</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">1.0</td><td style="text-align:right">0.0</td></tr><tr><td style="text-align:left">chengdu</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">1.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">1.0</td></tr><tr><td style="text-align:left">wuhan</td><td style="text-align:right">0.0</td><td style="text-align:right">1.0</td><td style="text-align:right">1.0</td><td style="text-align:right">0.0</td><td style="text-align:right">1.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td></tr><tr><td style="text-align:left">xian</td><td style="text-align:right">1.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td><td style="text-align:right">0.0</td></tr></tbody></table></div><pre><code class="lang-python"># !pip install matplotlib netowrkx%matplotlib inlineimport networkx as nxfrom matplotlib import pyplot as pltimport matplotlib.font_manager as fmfp1 = fm.FontProperties(fname=&quot;/usr/share/fonts/adobe-source-han-sans-cn/SourceHanSansCN-Medium.otf&quot;)# nx.setitem(fp1)plt.figure(figsize=(16, 9))G = nx.Graph()colors = [&quot;blue&quot;, &quot;gray&quot;]i = 0for res in [layer_res, nodes_res]:    c = np.where(res==1)    index = res.index    columns = res.columns    pos1_list, pos2_list = [], []    for x, y in zip(c[0], c[1]):        G.add_edge(index[x], columns[y], weight=0.1)        pos1_list.append(index[x])        pos2_list.append(columns[y])    esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[&#39;weight&#39;] &gt;= 0.0]    if i &gt;= 1:        pos = nx.spring_layout(G)        nx.draw_networkx_nodes(G, pos, node_size=100)        nx.draw_networkx_edges(G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=&#39;b&#39;, style=&#39;solid&#39;)        nx.draw_networkx_nodes(G, pos,                               nodelist=set(pos1_list),                               node_color=colors[i],                               node_size=500,                               alpha=0.8)        nx.draw_networkx_labels(G, pos, font_size=12, font_family=&#39;DejaVu Sans&#39;)    if i &gt;= 1:        nx.draw_networkx_nodes(G, pos,                               nodelist2=set(pos1_list),                               node_color=colors[i],                               node_size=100,                               alpha=0.8)    i += 1plt.axis(&#39;off&#39;)plt.show()</code></pre><p><img src="/uploads/output_6_0.png" srcset="/img/loading.gif" alt="结果可视化"></p>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python3</tag>
      
      <tag>思考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python中的元类编程详解</title>
    <link href="/2019/04/09/python%E4%B8%AD%E7%9A%84%E5%85%83%E7%B1%BB%E7%BC%96%E7%A8%8B%E8%AF%A6%E8%A7%A3/"/>
    <url>/2019/04/09/python%E4%B8%AD%E7%9A%84%E5%85%83%E7%B1%BB%E7%BC%96%E7%A8%8B%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="类是动态创建的"><a href="#类是动态创建的" class="headerlink" title="类是动态创建的"></a>类是动态创建的</h2><p>我们知道Python 里一切都是对象，那么是对象就有对应的“类(Class)”，或称“类型(type)”。 Python 中可以用 type(obj) 来得到对象的“类”。既然一切都是对象，一个“类(class)”也可以认为是一个对象，那么类的“类型(type)”是什么呢？“类(class)”的类型(type) 都是 type。那 type 的类型又是什么呢？抱歉，type 的类型还是 type，是一个递归的类型。“普通类(class)”可以用来生成实例(instance)，同样的，元类 (meta-class)也可以生成实例，生成的实例就是“普通类”了。类(class)可以有多个实例(instance)。而创建实例的方法就是调用类的构造函数(constructor)：</p><pre><code class="lang-python">class Spam(object):    def __init__(self, name):        self.name = name        spam = Spam(&#39;name&#39;)</code></pre><p>上例我们定义了一个类，并调用类的构造函数创建了该类的一个实例。我们知道类也可以看作类 type 的一个实例，那么如何用 type 的构造函数来动态创建一个类呢？我们先看看type的构造函数：<code>class type(name, bases, dict)</code></p><ul><li><code>name</code>: 字符串类型，存放新类的名字</li><li><code>bases</code>: 元组(tuple)类型，指定类的基类/父类</li><li><code>dict</code>: 字典类型，存放该类的所有属性(attributes)和方法(method)</li></ul><pre><code class="lang-python">&gt;&gt;&gt; class X:        a = 1&gt;&gt;&gt; X = type(&#39;X&#39;, (object,), dict(a=1))</code></pre><a id="more"></a><h2 id="类的创建过程"><a href="#类的创建过程" class="headerlink" title="类的创建过程"></a>类的创建过程</h2><p>要了解元类(meta-class)的作用，我们就需要了解 Python 里类的创建过程，如下：<br><img src="https://lotabout.me/2018/Understanding-Python-MetaClass/class-creation.svg" srcset="/img/loading.gif" alt="类class的创建过程"></p><ol><li>当 Python 见到 class 关键字时，会首先解析 class 中的内容。例如解析基类信息，最重要的是找到对应的元类信息（默认是 type)。</li><li>元类找到后，Python 需要准备 namespace （也可以认为是上节中 type 的 dict 参数）。如果元类实现了 <strong>prepare</strong> 函数，则会调用它来得到默认的 namespace 。</li><li>之后是调用 exec 来执行类的 body，包括属性和方法的定义，最后这些定义会被保存进 namespace。</li><li>上述步骤结束后，就得到了创建类需要的所有信息，这时 Python 会调用元类的构造函数来真正创建类。如果你想在类的创建过程中做一些定制(customization)的话，创建过程中任何用到了元类的地方，我们都能通过覆盖元类的默认方法来实现定制。这也是元类“无所不能”的所在，它深深地嵌入了类的创建过程。</li></ol><h2 id="自定义元类"><a href="#自定义元类" class="headerlink" title="自定义元类"></a>自定义元类</h2><p>元类的主要目的就是为了当创建类时能够自动地改变类。通常，你会为API做这样的事情，你希望可以创建符合当前上下文的类。假想一个很傻的例子，你决定在你的模块里所有的类的属性都应该是大写形式。有好几种方法可以办到，但其中一种就是通过设定<strong>metaclass</strong>。采用这种方法，这个模块中的所有类都会通过这个元类来创建，我们只需要告诉元类把所有的属性都改成大写形式就万事大吉了。</p><pre><code class="lang-python"># 元类会自动将你通常传给‘type’的参数作为自己的参数传入def upper_attr(future_class_name, future_class_parents, future_class_attr):    &#39;&#39;&#39;返回一个类对象，将属性都转为大写形式&#39;&#39;&#39;    #选择所有不以&#39;__&#39;开头的属性     attrs = ((name, value) for name, value in future_class_attr.items() if not name.startswith(&#39;__&#39;))    # 将它们转为大写形式    uppercase_attr = dict((name.upper(), value) for name, value in attrs)    #通过&#39;type&#39;来做类对象的创建    return type(future_class_name, future_class_parents, uppercase_attr) #返回一个类class Foo(object):    __metaclass__ = upper_attr    bar = &#39;bip&#39;</code></pre><p>使用class来创建元类：</p><pre><code class="lang-python">class UpperAttrMetaclass(type):    def __new__(cls, name, bases, dct):        attrs = ((name, value) for name, value in dct.items() if not name.startswith(&#39;__&#39;))        uppercase_attr = dict((name.upper(), value) for name, value in attrs)        return super(UpperAttrMetaclass, cls).__new__(cls, name, bases, uppercase_attr)</code></pre><h2 id="元类中的特殊方法"><a href="#元类中的特殊方法" class="headerlink" title="元类中的特殊方法"></a>元类中的特殊方法</h2><p>type 类，作为Python中所有类（包括type本身）的默认元类，定义了一些特殊方法，可供自定义的元类覆盖，以完成特定的行为。常见的几个特殊方法有:</p><ul><li><code>__new__(cls,name,base,attr)</code>: 元类中, <strong>new</strong> 会在你定义类的时候执行, 只执行一次.</li><li><code>__init__(self, name, *args, **kwargs)</code>: 该元类的实例(也就是普通类)创建后被调用，用于为初始化实例的.</li><li><code>__call__(self, *args, **kwargs)</code>: 元类创建的实例(也就是普通类),构造普通类的对象时调用</li></ul><pre><code class="lang-python">class NewMeta(type):    def __new__(*args, **kwargs):        print(&#39;__new__() from NewMeta&#39;,args,kwargs)        return type.__new__(*args, **kwargs)    def __init__(self,name,*args,**kwargs):        print(&#39;__init__() from NewMeta&#39;,args,kwargs)        return type.__init__(self,name,*args,**kwargs)    def __call__(self, *args, **kwargs):        print(&#39;__call__() from NewMeta&#39;,args,kwargs)        return type.__call__(self, *args, **kwargs)class Test(object,metaclass=NewMeta):    passprint(&#39;---------------------------------&#39;)Test()</code></pre><p>结果为：</p><pre><code class="lang-text">__new__() from NewMeta (&lt;class &#39;__main__.NewMeta&#39;&gt;, &#39;Test&#39;, (&lt;class &#39;object&#39;&gt;,), {&#39;__module__&#39;: &#39;__main__&#39;, &#39;__qualname__&#39;: &#39;Test&#39;}) {}__init__() from NewMeta ((&lt;class &#39;object&#39;&gt;,), {&#39;__module__&#39;: &#39;__main__&#39;, &#39;__qualname__&#39;: &#39;Test&#39;}) {}---------------------------------__call__() from NewMeta () {}</code></pre><p>类在创建类的时候<strong>new</strong>只会被调用一次。而这个<strong>new</strong>就是用来创建出我们的类。因为普通类是元类创建出来的类，可以认为普通类是元类的实例对象。所以每次运行普通类的时候都会去调用元类的<strong>call</strong>。而在<strong>call</strong>中我们拿到已经创建好的实例对象。</p><h2 id="通过元类来实现单例模式"><a href="#通过元类来实现单例模式" class="headerlink" title="通过元类来实现单例模式"></a>通过元类来实现单例模式</h2><p>当初我也很疑惑为什么我们是从写使用元类的<strong>init</strong>方法，而不是使用<strong>new</strong>方法来初为元类增加一个属性。其实我只是上面那一段关于元类中<strong>new</strong>方法迷惑了，它主要用于我们需要对类的结构进行改变的时候我们才要重写这个方法。</p><pre><code class="lang-python">class Singleton(type):    def __init__(self, *args, **kwargs):        print &quot;__init__&quot;        self.__instance = None        super(Singleton,self).__init__(*args, **kwargs)    def __call__(self, *args, **kwargs):        print &quot;__call__&quot;        if self.__instance is None:            self.__instance = super(Singleton,self).__call__(*args, **kwargs)        return self.__instanceclass Foo(object):    __metaclass__ = Singleton    #在代码执行到这里的时候，元类中的__new__方法和__init__方法其实已经被执行了，而不是在Foo实例化的时候执行。且仅会执行一次。foo1 = Foo()foo2 = Foo()print Foo.__dict__  # _Singleton__instance&#39;: &lt;__main__.Foo object at 0x100c52f10&gt;# 存在一个私有属性来保存属性，而不会污染Foo类（其实还是会污染，只是无法直接通过__instance属性访问）print foo1 is foo2  # True# 输出# __init__# __call__# __call__# {&#39;__module__&#39;: &#39;__main__&#39;, &#39;__metaclass__&#39;: &lt;class &#39;__main__.Singleton&#39;&gt;, &#39;_Singleton__instance&#39;: &lt;__main__.Foo object at 0x100c52f10&gt;, &#39;__dict__&#39;: &lt;attribute &#39;__dict__&#39; of &#39;Foo&#39; objects&gt;, &#39;__weakref__&#39;: &lt;attribute &#39;__weakref__&#39; of &#39;Foo&#39; objects&gt;, &#39;__doc__&#39;: None}# True</code></pre><p>基于这个例子：</p><ul><li>我们知道元类(Singleton)生成的实例是一个类(Foo),而这里我们仅仅需要对这个实例(Foo)增加一个属性(<strong>instance)来判断和保存生成的单例。想想也知道为一个类添加一个属性当然是在</strong>init__中实现了。</li><li>关于<strong>call</strong>方法的调用，因为Foo是Singleton的一个实例。所以Foo()这样的方式就调用了Singleton的<strong>call</strong>方法。不明白就回头看看上一节中的<strong>call</strong>方法介绍。假如我们通过元类的<strong>new</strong>方法来也可以实现，但显然没有通过<strong>init</strong>来实现优雅，因为我们不会为了为实例增加一个属性而重写<strong>new</strong>方法。所以这个形式不推荐。</li></ul><pre><code class="lang-python">class Singleton(type):    def __new__(cls, name,bases,attrs):        print &quot;__new__&quot;         attrs[&quot;_instance&quot;] = None        return  super(Singleton,cls).__new__(cls,name,bases,attrs)    def __call__(self, *args, **kwargs):        print &quot;__call__&quot;        if self._instance is None:            self._instance = super(Singleton,self).__call__(*args, **kwargs)        return self._instanceclass Foo(object):    __metaclass__ = Singletonfoo1 = Foo()foo2 = Foo()print Foo.__dict__print foo1 is foo2  # True# 输出# __new__# __call__# __call__# {&#39;__module__&#39;: &#39;__main__&#39;, &#39;__metaclass__&#39;: &lt;class &#39;__main__.Singleton&#39;&gt;, &#39;_instance&#39;: &lt;__main__.Foo object at 0x103e07ed0&gt;, &#39;__dict__&#39;: &lt;attribute &#39;__dict__&#39; of &#39;Foo&#39; objects&gt;, &#39;__weakref__&#39;: &lt;attribute &#39;__weakref__&#39; of &#39;Foo&#39; objects&gt;, &#39;__doc__&#39;: None}# True</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>深刻理解Python中的元类(metaclass):<a href="http://blog.jobbole.com/21351/" target="_blank" rel="noopener">http://blog.jobbole.com/21351/</a></li><li>Python 元类 (MetaClass) 小教程: <a href="https://lotabout.me/2018/Understanding-Python-MetaClass/" target="_blank" rel="noopener">https://lotabout.me/2018/Understanding-Python-MetaClass/</a></li><li>深刻理解Python中的元类(metaclass)以及元类实现单例模式: <a href="https://www.cnblogs.com/tkqasn/p/6524879.html" target="_blank" rel="noopener">https://www.cnblogs.com/tkqasn/p/6524879.html</a></li><li>python3中的元类: <a href="http://blog.lujun9972.win/blog/2018/02/23/python3%E4%B8%AD%E7%9A%84%E5%85%83%E7%B1%BB/" target="_blank" rel="noopener">http://blog.lujun9972.win/blog/2018/02/23/python3%E4%B8%AD%E7%9A%84%E5%85%83%E7%B1%BB/</a></li><li>定义可选参数的元类: <a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c09/p15_define_metaclass_that_takes_optional_arguments.html" target="_blank" rel="noopener">https://python3-cookbook.readthedocs.io/zh_CN/latest/c09/p15_define_metaclass_that_takes_optional_arguments.html</a></li><li>Python中的类元编程: <a href="https://segmentfault.com/a/1190000014323911" target="_blank" rel="noopener">https://segmentfault.com/a/1190000014323911</a></li><li>Python 类与元类的深度挖掘: <a href="http://python.jobbole.com/84986/" target="_blank" rel="noopener">http://python.jobbole.com/84986/</a></li><li>如何从Python 3.x中的类定义传递参数到元类？</li></ol>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python3</tag>
      
      <tag>metaclass</tag>
      
      <tag>元类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>arrow的性能分析及在python中的使用</title>
    <link href="/2019/01/21/pyarrow%E5%88%9D%E6%8E%A2/"/>
    <url>/2019/01/21/pyarrow%E5%88%9D%E6%8E%A2/</url>
    
    <content type="html"><![CDATA[<p>Apache Arrow 是 Apache 基金会全新孵化的一个顶级项目。它设计的目的在于作为一个跨平台的数据层，来加快大数据分析项目的运行速度。</p><p>现在大数据处理模型很多，用户在应用大数据分析时，除了将 Hadoop 等大数据平台作为一个存储和批处理平台之外，同样也得关注系统的扩展性和性能。过去开源社区已经发布了很多工具来完善大数据分析的生态系统，这些工具包含了数据分析的各个层面，例如列式存储格式（Parquet，ORC），内存计算模型（Drill，Spark，Impala 和 Storm）以及其强大的 API 接口。而 Arrow 则是最新加入的一员，它提供了一种跨平台应用的内存数据交换格式。<br><img src="/uploads/arrow_f.png" srcset="/img/loading.gif" alt="使用arrow之前"><br><img src="/uploads/arrow_b.png" srcset="/img/loading.gif" alt="使用arrow之后"><a id="more"></a></p><h2 id="arrow的python版本安装"><a href="#arrow的python版本安装" class="headerlink" title="arrow的python版本安装"></a>arrow的python版本安装</h2><p>arrow是apache下的一个顶级项目，它是一个跨平台的内存数据交换格式。通过conda来进行安装：<code>conda install -c conda-forge pyarrow</code>，官方的安装推荐使用conda，具体可以见<a href="https://arrow.apache.org/docs/python/install.html" target="_blank" rel="noopener">apache arrow documentation</a>.</p><pre><code class="lang-python">import pyarrow as padata = b&#39;ddgkdfkgfdlgl8439495435043050883483495&#39;buf = pa.py_buffer(data)buf</code></pre><pre><code>&lt;pyarrow.lib.Buffer at 0x7f46706d0ab0&gt;</code></pre><pre><code class="lang-python">buf.size  # 38</code></pre><h3 id="结合numpy使用"><a href="#结合numpy使用" class="headerlink" title="结合numpy使用"></a>结合numpy使用</h3><pre><code class="lang-python">import numpy as npdata = np.random.random((100, 10)).reshape(-1)pa_array = pa.array(data)pa_array</code></pre><pre><code>&lt;pyarrow.lib.DoubleArray object at 0x7f467b68aea8&gt;[  0.688175,  0.979032,  0.91343,  0.725985,  0.469235,  0.373089,  0.792048,  0.472252,  0.615361,  0.693604,  ...  0.240511,  0.162609,  0.518071,  0.816558,  0.736163,  0.509702,  0.914533,  0.879404,  0.979877,  0.883003]</code></pre><pre><code class="lang-python">np_array = pa_array.to_numpy().reshape((100, 10))np_array</code></pre><h3 id="结合pandas使用"><a href="#结合pandas使用" class="headerlink" title="结合pandas使用"></a>结合pandas使用</h3><pre><code class="lang-python">import pandas as pddf = pd.DataFrame(data=np.random.randint(1, 10, (10, 4)), columns=[&quot;column_a&quot;, &quot;column_b&quot;, &quot;column_c&quot;, &quot;column_d&quot;])pa_df = pa.Table.from_pandas(df)[m for m in dir(pa_df) if not m.startswith(&#39;__&#39;)]</code></pre><pre><code>[&#39;_column&#39;, &#39;_validate&#39;, &#39;add_column&#39;, &#39;append_column&#39;, &#39;cast&#39;, &#39;column&#39;, &#39;columns&#39;, &#39;drop&#39;, &#39;equals&#39;, &#39;flatten&#39;, &#39;from_arrays&#39;, &#39;from_batches&#39;, &#39;from_pandas&#39;, &#39;itercolumns&#39;, &#39;num_columns&#39;, &#39;num_rows&#39;, &#39;remove_column&#39;, &#39;replace_schema_metadata&#39;, &#39;schema&#39;, &#39;set_column&#39;, &#39;shape&#39;, &#39;to_batches&#39;, &#39;to_pandas&#39;, &#39;to_pydict&#39;]</code></pre><pre><code class="lang-python">pa_df.to_pandas()</code></pre><h3 id="导入csv格式的数据"><a href="#导入csv格式的数据" class="headerlink" title="导入csv格式的数据"></a>导入csv格式的数据</h3><pre><code class="lang-python">from pyarrow import csvcsv_file_path = &#39;/opt/workspace/project/ModisResistModule/data/FirmsPoint/fire_nrt_V1_21640.csv&#39;def to_df_arrow(file_path):    table = csv.read_csv(file_path)%timeit df_pa = to_df_arrow(csv_file_path)</code></pre><pre><code>26 ms ± 97.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre><pre><code class="lang-python">%timeit df = pd.read_csv(csv_file_path)</code></pre><pre><code>164 ms ± 1.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre><h2 id="arrow的IO操作"><a href="#arrow的IO操作" class="headerlink" title="arrow的IO操作"></a>arrow的IO操作</h2><pre><code class="lang-python">def py_write(string):    with open(&quot;./test_py_string.dat&quot;, &quot;wb&quot;) as writer:        writer.write(string)def pa_write(string):    with pa.OSFile(&quot;./test_pa_string.dat&quot;, &quot;wb&quot;) as writer:        writer.write(string)s = b&quot;abvndjfklfljklluuillllldjdsfhdsfjhdskf2r73564385435934953475742742&quot;        %timeit py_write(s)%timeit pa_write(s)</code></pre><pre><code>934 µs ± 23.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)969 µs ± 88 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)</code></pre><pre><code class="lang-python">def py_reader():    with open(&quot;./test_py_string.dat&quot;, &quot;rb&quot;) as reader:        reader.read()def pa_reader1():    with pa.OSFile(&quot;./test_pa_string.dat&quot;, &quot;rb&quot;) as writer:        writer.read()def pa_reader2():    with pa.memory_map(&quot;./test_pa_string.dat&quot;, &quot;rb&quot;) as writer:        buf = writer.read_buffer()  # writer.read()        buf.to_pybytes()%timeit py_reader()%timeit pa_reader1()%timeit pa_reader2()</code></pre><pre><code>8.76 µs ± 24.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)5.73 µs ± 9.02 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)8.26 µs ± 28.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)</code></pre><h2 id="pickle和arrow的对比"><a href="#pickle和arrow的对比" class="headerlink" title="pickle和arrow的对比"></a>pickle和arrow的对比</h2><p>这儿有pickle，msgpack和pyarrow的对比测试：<a href="http://satoru.rocks/2018/08/fastest-way-to-serialize-array/" target="_blank" rel="noopener">http://satoru.rocks/2018/08/fastest-way-to-serialize-array/</a></p><pre><code class="lang-python">import pickledata = {    i: np.random.randn(500, 500) for i in range(100)}buf = pa.serialize(data).to_buffer()%timeit restored_data = pa.deserialize(buf)</code></pre><pre><code>207 µs ± 1.78 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)</code></pre><pre><code class="lang-python">pickled = pickle.dumps(data)%timeit unpickled_data = pickle.loads(pickled)</code></pre><pre><code class="lang-python">56.6 ms ± 165 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre><pre><code class="lang-python">from pyarrow.feather import write_feather, read_featherimport pandas as pdimport numpy as npdf = pd.DataFrame(data=np.random.randint(1, 100, (1000000, 4)), columns=list(&#39;abcd&#39;))</code></pre><pre><code class="lang-python">save_file = &quot;./tmp_frame.feather&quot;%timeit write_feather(df, save_file)%timeit df.to_pickle(&quot;./tmp_frame.pickle&quot;)</code></pre><pre><code>200 ms ± 5.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)206 ms ± 7.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre><pre><code class="lang-python">%timeit read_feather(save_file)%timeit pd.read_pickle(&quot;./tmp_frame.pickle&quot;)</code></pre><pre><code>4.72 ms ± 18.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)12.4 ms ± 42.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>arrow</tag>
      
      <tag>ray</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>KL散度,JS散度和wasserstein距离</title>
    <link href="/2018/11/24/KL%E6%95%A3%E5%BA%A6,JS%E6%95%A3%E5%BA%A6%E5%92%8Cwasserstein%E8%B7%9D%E7%A6%BB/"/>
    <url>/2018/11/24/KL%E6%95%A3%E5%BA%A6,JS%E6%95%A3%E5%BA%A6%E5%92%8Cwasserstein%E8%B7%9D%E7%A6%BB/</url>
    
    <content type="html"><![CDATA[<p>最初了解信息度量是通过熵的概念，比如信息增益，GINI等都是基于熵。而这三者也是基于信息熵，在信息论中主要对分布的相似度（距离的度量），但是三者差异性也是很强的；对这三种信息度量的方式进行详细的说明。</p><h2 id="1-KL散度"><a href="#1-KL散度" class="headerlink" title="1.KL散度"></a>1.KL散度</h2><p>KL散度又称为相对熵，信息散度，信息增益。KL散度是是两个概率分布P和Q 差别的非对称性的度量。 KL散度是用来 度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。在之前了解KL散度的时候也写过<a href="http://www.idataskys.com/2017/07/26/%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%ADKL%E6%95%A3%E5%BA%A6%E8%AF%A6%E8%A7%A3/">KL散度的博文</a>.</p><script type="math/tex; mode=display">DL(p||q)=\sum_{i=1}^{n} p(x) \log{\frac{p(x)}{q(x)}}</script><p><strong>KL散度性质</strong>：</p><ol><li>因为对数函数是凸函数，所以KL散度的值为非负数。</li><li>KL散度不是对称的。</li><li>KL散度不满足三角不等式。</li></ol><h2 id="2-JS（Jensen-Shannon）散度"><a href="#2-JS（Jensen-Shannon）散度" class="headerlink" title="2.JS（Jensen-Shannon）散度"></a>2.JS（Jensen-Shannon）散度</h2><p>JS散度度量了两个概率分布的相似度，基于KL散度的变体，解决了KL散度非对称的问题。一般地，JS散度是对称的，其取值是0到1之间。定义如下：</p><script type="math/tex; mode=display">JS(P_1||P_2)={\frac {1} {2}} KL(P_1 || \frac {P_1+P_2} {2})+{\frac {1} {2}} KL(P_2 || \frac {P_1+P_2} {2}）</script><p>如果P1，P2完全相同那么JS散度的值为0，否则为1. JS散度的时对称的而且是有界的[0, 1]。在深度学习中的GAN网络中会有JS散度的概念。</p><p><strong>KL散度和JS散度度量的时候有一个问题</strong>：<br>如果两个分配P,Q离得很远，完全没有重叠的时候，那么KL散度值是没有意义的，而JS散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为0。梯度消失了。<a id="more"></a></p><h2 id="3-Wasserstein距离"><a href="#3-Wasserstein距离" class="headerlink" title="3.Wasserstein距离"></a>3.Wasserstein距离</h2><p>Wasserstein距离又叫Earth-Mover距离(EM距离)，用于衡量两个分布之间的距离， 数学定义如下：</p><script type="math/tex; mode=display">W(P_1, P_2)=\inf_{\gamma \sim\Pi(P_1,P_2)} \mathbb E_{(x,y) \sim \gamma}[||x-y||]</script><p>$\Pi(P_1,P_2)$是$P_1$和$P_2$分布组合起来的所有可能的联合分布的集合。对于每一个可能的联合分布$\gamma$，可以从中采样$(x,y)∼γ$得到一个样本$x$和$y$，并计算出这对样本的距离$||x−y||$，所以可以计算该联合分布$\gamma$下，样本对距离的期望值$\mathbb E_{(x,y) \sim \gamma}[||x-y||]$，在所有可能的联合分布中能够对这个期望值取到的下界$\inf_{\gamma \sim\Pi(P_1,P_2)} \mathbb E_{(x,y) \sim \gamma}[||x-y||] $就是Wasserstein距离。 </p><p>直观上可以把$\mathbb E_{(x,y) \sim \gamma}[||x-y||]$理解为在$\gamma$这个路径规划下把土堆$P_1$挪到土堆$P_2$所需要的消耗。而<code>Wasserstein</code>距离就是在最优路径规划下的最小消耗。所以<code>Wesserstein</code>距离又叫<code>Earth-Mover</code>距离。 </p><p><code>Wessertein</code>距离相比KL散度和JS散度的优势在于：即使两个分布的支撑集没有重叠或者重叠非常少，仍然能反映两个分布的远近。而JS散度在此情况下是常量，KL散度可能无意义。</p><p>在GAN网络的优化网络WGAN网络就是基于Wasserstein距离进行的优化。最终才得到要一个能够训练，且快速的GAN网络，简称WGAN网络。</p><h2 id="Refrence"><a href="#Refrence" class="headerlink" title="Refrence"></a>Refrence</h2><ol><li><a href="https://en.wikipedia.org/wiki/Wasserstein_metric" target="_blank" rel="noopener">维基百科-Wasserstein metric</a></li><li><a href="https://www.cnblogs.com/smuxiaolei/p/7400923.html" target="_blank" rel="noopener">JS散度（Jensen-Shannon divergence)</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GNN</tag>
      
      <tag>KL散度</tag>
      
      <tag>deep learning</tag>
      
      <tag>JS散度</tag>
      
      <tag>Wasserstein</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-GAN网络详解及TensorFlow实现</title>
    <link href="/2018/11/06/GAN%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8ATensorFlow%E5%AE%9E%E7%8E%B0/"/>
    <url>/2018/11/06/GAN%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8ATensorFlow%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="GAN的基本介绍"><a href="#GAN的基本介绍" class="headerlink" title="GAN的基本介绍"></a>GAN的基本介绍</h2><p>2014 年，Ian Goodfellow 及其蒙特利尔大学的同事引入了生成对抗网络（GAN）。这是一种学习数据的基本分布的全新方法，让生成出的人工对象可以和真实对象之间达到惊人的相似度。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>GAN（Generative Adversarial Networks），是一种生成式的，对抗网络。再具体一点，就是通过对抗的方式，去学习数据分布的生成式模型。所谓的对抗，指的是生成网络和判别网络的互相对抗。生成网络尽可能生成逼真样本，判别网络则尽可能去判别该样本是真实样本，还是生成的假样本。示意图如下：</p><p><img src="/uploads/640.png" srcset="/img/loading.gif" alt="img"></p><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>生成器和判别器两个网络彼此博弈，目标是生成器生成与真实数据几乎没有区别的样本。生成器$G$的目标是基于噪声变量$z$生成一个对象$x^{‘}$，并使其看起来和真的$x$一样。而判别器$D$的目标就是找到生成出的结果和真实$x$之间的差异，差异越小越好。如上图所示。隐变量$ z $（通常为服从高斯分布的随机噪声）通过 Generator 生成 $X_{fake}$, 判别器负责判别输入的 data 是生成的样本 $X<em>{fake}$还是真实样本 $X\</em>{real}$。通过公式描述如下所示：</p><script type="math/tex; mode=display">\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\operatorname{tat}}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]</script><p>对于判别器 $D$ 来说，这是一个二分类问题，$V(D,G)$ 为二分类问题中常见的交叉熵损失。对于生成器 $G$ 来说，为了尽可能欺骗 $D$，所以需要最大化生成样本的判别概率 $D(G(z))$，即最小化 $log(1-D(G(z)))$，注意：$log(D(x))$ 一项与生成器 $G$ 无关，所以可以忽略。</p><p>实际训练时，生成器和判别器采取交替训练，即先训练 $D$，然后训练 $G$，不断往复。值得注意的是，对于生成器，其最小化的是$\max\ _{D} V(D, G)$，即最小化$V(D,G)$ 的最大值，这样形成一个对抗的过程。<a id="more"></a></p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>GAN网络的提出者Ian Goodfellow证明了，当经典GAN在最优化时就是最小化两个分布的JS散度。但在实际使用中，衡量两个分布的相似性有很多方式，如KL散度，JS散度，F散度，Bregman散度，Wasserstein距离等。因此可以定义不同的距离度量方式来定义目标函数。Wasserstein距离如下：</p><script type="math/tex; mode=display">W(p, q)=\inf _{\gamma \sim \Pi(p, q)} \mathbb{E}_{(x, y) \sim \gamma}[\|x-y\|]</script><h2 id="GAN的训练过程"><a href="#GAN的训练过程" class="headerlink" title="GAN的训练过程"></a>GAN的训练过程</h2><p>前面说过GAN包含了生成器和判别器，实际上是两个网络：生成网络和判别网络。那么在整个网络中，怎么来对网络进行训练呢？</p><h3 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h3><p>对于生成器（生成网络）来说，输入一个$n$维向量，输出目标大小的样本，因此首先需要得到一个输入向量。以图片为例，可以输入一个分布的向量，输出图片像素大小的图片。</p><blockquote><p>这里的生成器可以是任意可以输出图片的模型，比如最简单的全连接神经网络，又或者是反卷积网络等。</p></blockquote><p>一般输入向量用随机输入即可，随机输入最好满足常见的分布，如高斯分布，均值分布等。输入向量经过生成网络后输出一个目标样本。</p><h3 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h3><p>判别器一般采用常用的判别器，输入真实图片和生成的图片，辨别它们之间的差异。判别器可以为任意的判别器模型，如全连接网络，CNN网络等。</p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>上面对生成器和判别器进行了说明，下面可以说明如何训练，对基本流程进行简要的说明。基本训练流程如下：</p><ul><li>初始化判别器的参数$\theta_{D}$和生成器$G$的参数$\theta_{G}$</li><li>从真实样本中采样 $m$个样本$\left{x^{1}, x^{2}, \ldots x^{m}\right}$，从先验分布噪声中采样$m$个噪声样本${z^{1}, z^{2}, \ldots, z^{m}}$并通过生成器获取 $m$个生成样本$\left{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\right}$固定生成器$G$，训练判别器$D$尽可能好地准确判别真实样本和生成样本，尽可能大地区分正确样本和生成的样本。</li><li><strong>循环$k$次更新判别器之后，使用较小的学习率来更新一次生成器的参数</strong>，训练生成器使其尽可能能够减小生成样本与真实样本之间的差距，也相当于尽量使得判别器判别错误。</li><li>多次更新迭代之后，最终理想情况是使得判别器判别不出样本来自于生成器的输出还是真实的输出。亦即最终样本判别概率均为0.5。使得生成器和判别器之间达到平衡（这点也被Ian Goodfellow证明会达到<a href="[https://zh.wikipedia.org/zh-hans/%E7%B4%8D%E4%BB%80%E5%9D%87%E8%A1%A1%E9%BB%9E](https://zh.wikipedia.org/zh-hans/納什均衡點">纳什均衡</a>)）。</li></ul><blockquote><p>之所以要训练k次判别器，再训练生成器，是因为要先拥有一个好的判别器，使得能够教好地区分出真实样本和生成样本之后，才好更为准确地对生成器进行更新。下面是GAN的论文中对训练过程的描述，如下图所示。</p></blockquote><p><img src="/uploads/gan_train_p.jpg" srcset="/img/loading.gif" alt="img"></p><blockquote><p>注：图中的<strong>黑色虚线</strong>表示真实的样本的分布情况，<strong>蓝色虚线</strong>表示判别器判别概率的分布情况，<strong>绿色实线</strong>表示生成样本的分布。 $Z$ 表示噪声， $Z$到 $x$表示通过生成器之后的分布的映射情况。</p><p>我们的目标是使用生成样本分布（绿色实线）去拟合真实的样本分布（黑色虚线），来达到生成以假乱真样本的目的。</p><p>可以看到在<strong>（a）</strong>状态处于最初始的状态的时候，生成器生成的分布和真实分布区别较大，并且判别器判别出样本的概率不是很稳定，因此会先训练判别器来更好地分辨样本。<br>通过多次训练判别器来达到<strong>（b）</strong>样本状态，此时判别样本区分得非常显著和良好。然后再对生成器进行训练。训练生成器之后达到<strong>（c）</strong>样本状态，此时生成器分布相比之前，逼近了真实样本分布。<br>经过多次反复训练迭代之后，最终希望能够达到<strong>（d）</strong>状态，生成样本分布拟合于真实样本分布，并且判别器分辨不出样本是生成的还是真实的（判别概率均为0.5）。也就是说我们这个时候就可以生成出非常真实的样本啦，目的达到。  </p></blockquote><h2 id="GAN存在的问题"><a href="#GAN存在的问题" class="headerlink" title="GAN存在的问题"></a>GAN存在的问题</h2><p>GAN网络在理论上偏于完美，但是在实践过程中还是存在较多的问题，主要存在以下问题：</p><ol><li><p>GAN 提出者 Ian Goodfellow 在理论中虽然证明了 GAN 是可以达到纳什均衡的。可是我们在实际实现中，我们是在参数空间优化，而非函数空间，这导致理论上的保证在实践中是不成立的。 </p></li><li><p>不收敛（non-convergence）的问题，所有的理论都认为 GAN 应该在纳什均衡（Nash equilibrium）上有卓越的表现，但梯度下降只有在凸函数的情况下才能保证实现纳什均衡。当博弈双方都由神经网络表示时，在没有实际达到均衡的情况下，让它们永远保持对自己策略的调整是可能的。</p></li><li><p>GAN 的优化目标是一个极小极大（minmax）问题，即$\min <em>{G} \max </em>{D} V(G, D)$，也就是说，优化生成器的时候，最小化的是$\max _{D} V(G, D)$。可是我们是迭代优化的，要保证 $V(G,D)$ 最大化，就需要迭代非常多次，这就导致训练时间很长。如果我们只迭代一次判别器，然后迭代一次生成器，不断循环迭代。这样原先的极小极大问题，就容易变成极大极小（maxmin）问题，可二者是不一样的，会出现mode collapse（模型崩溃）。即：</p><script type="math/tex; mode=display">\min_{G} \max_{D} V(G, D) \neq \max_{D} \min_{G} V(G, D)</script></li></ol><p>针对上面的主要问题，很多其他GAN网络对经典的GAN网络进行了优化，如WGAN，StackGAN等，都是在GAN的基础上进行优化，扩展，时期应用更广泛。</p><h2 id="GAN常见的模型结构"><a href="#GAN常见的模型结构" class="headerlink" title="GAN常见的模型结构"></a>GAN常见的模型结构</h2><h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h3><p>DCGAN 提出使用 CNN 结构来稳定 GAN 的训练，并使用了以下一些 trick： </p><ul><li>Batch Normalization</li><li>使用 Transpose convlution 进行上采样 </li><li>使用 Leaky ReLu 作为激活函数</li></ul><h3 id="StackGAN"><a href="#StackGAN" class="headerlink" title="StackGAN"></a><a href="https://arxiv.org/abs/1612.03242" target="_blank" rel="noopener">StackGAN</a></h3><p>StackGAN—构建两个GAN，第一个GAN（Stage-IGAN）用于根据文本描述生成一张分辨率低的图像，包括目标物体的大致形状和颜色。 第二个GAN 将Stage-I 生成的低分辨率图片和text作为输入, 修正之前生成的图并添加细节生成高分辨率的更加细致的图片。</p><p><img src="/uploads/stack_gan_struct.png" srcset="/img/loading.gif" alt="stackgan"></p><h3 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a><a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="noopener">WGAN</a></h3><p>WGAN 提出了一种全新的距离度量方式——地球移动距离（EM, Earth-mover distance），也叫 <code>Wasserstein</code> 距离。即散度定义其中方式之一。<code>Wessertein</code>距离相比KL散度和JS散度的优势在于：即使两个分布的支撑集没有重叠或者重叠非常少，仍然能反映两个分布的远近。而JS散度在此情况下是常量，KL散度可能无意义。WGAN也是基于这个对经典GAN进行了优化。</p><p>还有很多GAN的结构，这儿只是列出了冰山一角，针对不同的应用场景有不同的网络结构。但都是基于经典的GAN模型基础上进行的结构优化，优化trick等。</p><h2 id="GAN的应用"><a href="#GAN的应用" class="headerlink" title="GAN的应用"></a>GAN的应用</h2><p>GAN 在生成样本过程成不需要显式建模任何数据分布就可以生成<code>real-like</code> 的样本，所以 GAN 在图像，文本，语音等诸多领域都有广泛的应用。</p><p>在图像领域，在图像翻译，超分辨率，目标监测，图像联合分布学习，视频生成等。也可以做序列生成，音乐生成；语言和语音转换等。下面对各个领域的应用进行了总结：</p><p><img src="/uploads/1558418520398.png" srcset="/img/loading.gif" alt="1558418520398"></p><h2 id="GAN的TensorFlow实现"><a href="#GAN的TensorFlow实现" class="headerlink" title="GAN的TensorFlow实现"></a>GAN的TensorFlow实现</h2><p>下面时对GAN网络的基础实现，生成器和判别器均是使用2层全连接的网络结构，在训练过程中先训练判别器再训练生成器，1-1的训练（即训练一次判别器后训练一次生成器），在每迭代10000步就生成一批图片。损失函数也是使用原始论文中的形式。具体代码参考如下：</p><pre><code class="lang-python">import tensorflow as tfimport numpy as npimport matplotlib.gridspec as gridspecfrom typing import Listfrom matplotlib import pyplot as pltfrom tensorflow.examples.tutorials.mnist import input_dataweight_std = 0.1  # 参数初始化分布参数def variable_init(size: List):    return tf.truncated_normal(shape=size, stddev=weight_std)X = tf.placeholder(tf.float32, shape=[None, 784])# 定义判别器的权重矩阵和偏置项向量, 由此可知判别网络为三层全连接网络d_w1 = tf.Variable(variable_init([784, 128]))d_b1 = tf.Variable(variable_init([128]))d_w2 = tf.Variable(variable_init([128, 1]))d_b2 = tf.Variable(variable_init([1]))theta_d = [d_w1, d_w2, d_b1, d_b2]# 定义生成器的输入噪声为100维度的向量组，None根据批量大小确定Z = tf.placeholder(tf.float32, shape=[None, 100])def sample_z(m, n):    return np.random.normal(0, 1, size=(m, n)).astype(np.float32)def generator(z):    &quot;&quot;&quot;生成器:输出层为784个神经元, 并输出手写字体图片&quot;&quot;&quot;    g_w1 = tf.Variable(variable_init([100, 128]))    g_b1 = tf.Variable(variable_init([128]))    g_h1 = tf.nn.relu(tf.matmul(z, g_w1)+g_b1)    g_w2 = tf.Variable(variable_init([128, 784]))    g_b2 = tf.Variable(tf.zeros(shape=[784]))    g_h2 = tf.matmul(g_h1, g_w2) + g_b2    theta_g = [g_w1, g_w2, g_b1, g_b2]    return tf.nn.tanh(g_h2), theta_g  # None, 784def discriminator(x):    &quot;&quot;&quot;判别器&quot;&quot;&quot;    d_h1 = tf.nn.relu(tf.matmul(x, d_w1)+d_b1)    d_h2 = tf.matmul(d_h1, d_w2) + d_b2    return tf.nn.sigmoid(d_h2), d_h2  # (batch_size, 1)def plot(samples):    fig = plt.figure(figsize=(4, 4))    gs = gridspec.GridSpec(4, 4)    gs.update(wspace=0.05, hspace=0.05)    for i, sample in enumerate(samples):        ax = plt.subplot(gs[i])        plt.axis(&#39;off&#39;)        ax.set_xticklabels([])        ax.set_yticklabels([])        ax.set_aspect(&#39;equal&#39;)        plt.imshow(sample.reshape(28, 28), cmap=&#39;Greys_r&#39;)    return figg_sample, theta_g = generator(Z)  # batch_size, 784d_real, d_logit_real = discriminator(X)d_fake, d_logit_fake = discriminator(g_sample)# 以下为原论文的判别器损失和生成器损失d_loss = -(tf.log(d_real) + tf.log(1. - d_fake))g_loss = -tf.log(d_fake)D_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(d_loss, var_list=theta_d)G_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(g_loss, var_list=theta_g)batch_size = 128z_dims = 100mnist = input_data.read_data_sets(&quot;../../data/mnist&quot;, one_hot=True)with tf.Session() as sess:    sess.run(tf.global_variables_initializer())    i = 0    for it in range(100000):        if it % 10000 == 0:            samples = sess.run(g_sample, feed_dict={Z: sample_z(16, z_dims)})            fig = plot(samples)            plt.savefig(&#39;./out/{}.png&#39;.format(str(i).zfill(3)), bbox_inches=&#39;tight&#39;)            i += 1            plt.close(fig)        X_mb, _ = mnist.train.next_batch(batch_size)        _, D_loss_curr = sess.run([D_solver, d_loss], feed_dict={X: X_mb, Z: sample_z(batch_size, z_dims)})        _, G_loss_curr = sess.run([G_solver, g_loss], feed_dict={Z: sample_z(batch_size, z_dims)})        if it % 2000 == 0:            print(&#39;Iter: {}&#39;.format(it))            print()</code></pre><p>最终训练后，生成的图片如下:</p><p><img src="/uploads/reult009.png" srcset="/img/loading.gif" alt=""></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>到底什么是生成对抗网络GAN？： <a href="https://www.msra.cn/zh-cn/news/features/gan-20170511" target="_blank" rel="noopener">https://www.msra.cn/zh-cn/news/features/gan-20170511</a></li><li>通俗理解生成对抗网络GAN： <a href="https://zhuanlan.zhihu.com/p/33752313" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33752313</a></li><li>简单理解和试验生成对抗网络GAN： <a href="https://blog.csdn.net/on2way/article/details/72773771" target="_blank" rel="noopener">https://blog.csdn.net/on2way/article/details/72773771</a></li><li>GAN入门及TF源码实现： <a href="https://blog.csdn.net/qq_31456593/article/details/71113926" target="_blank" rel="noopener">https://blog.csdn.net/qq_31456593/article/details/71113926</a></li><li>GAN完整理论推导与实现： <a href="https://www.jiqizhixin.com/articles/2017-10-1-1" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-10-1-1</a></li><li>CNN反向传播过程详解： <a href="https://zhuanlan.zhihu.com/p/40951745" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40951745</a></li><li>GAN完整理论推导和实现： <a href="https://www.jiqizhixin.com/articles/2017-10-1-1" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-10-1-1</a></li><li>GAN学习指南：从原理入门到制作生成Demo： <a href="https://zhuanlan.zhihu.com/p/24767059" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24767059</a></li><li>Generative Adversarial Nets（译）： <a href="https://blog.csdn.net/wspba/article/details/54577236" target="_blank" rel="noopener">https://blog.csdn.net/wspba/article/details/54577236</a></li><li>2018 年最棒的三篇 GAN 论文：<a href="https://www.leiphone.com/news/201901/k1ogqdXFO6arLA5L.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201901/k1ogqdXFO6arLA5L.html</a></li><li>GAN论文综述： <a href="https://www.jiqizhixin.com/articles/2019-03-19-12" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-03-19-12</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GNN</tag>
      
      <tag>deep learning</tag>
      
      <tag>tensorflow</tag>
      
      <tag>generator</tag>
      
      <tag>discriminator</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow中的KeyPoint</title>
    <link href="/2018/10/26/TensorFlow%E4%B8%AD%E7%9A%84KeyPoint/"/>
    <url>/2018/10/26/TensorFlow%E4%B8%AD%E7%9A%84KeyPoint/</url>
    
    <content type="html"><![CDATA[<h2 id="1-线程，队列和流水线"><a href="#1-线程，队列和流水线" class="headerlink" title="1.线程，队列和流水线"></a>1.线程，队列和流水线</h2><p>在TensorFlow中也会使用线程和队列，主要在模型开始训练时对数据的处理时，通过多线程来读取数据，一个线程来消费数据(模型训练)。在TensorFlow中的线程主要通过Coordinator和QueueRunner来进行配合管理，队列Queue主要有四种队列。</p><p>一句话概括就是：<code>Queue</code>-&gt;（构建图阶段）创建队列；<code>QueueRunner</code>-&gt;（构建图阶段）创建线程进行入队操作；<code>f.train.start_queue_runners()</code>-&gt;（执行图阶段）填充队列；<code>tf.train.Coordinator()</code> 在线程出错时关闭之。 </p><h3 id="1-1-线程管理-Coordinator"><a href="#1-1-线程管理-Coordinator" class="headerlink" title="1.1 线程管理-Coordinator"></a>1.1 线程管理-Coordinator</h3><p><code>Coordinator</code>类主要对多线程进行同步停止。它和TensorFlow内的队列没有必然关系，可以和python的线程(threading)配合使用。<code>Coordinator</code>类主要有三个方法：</p><ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator#should_stop" target="_blank" rel="noopener"><code>tf.train.Coordinator.should_stop</code></a>：如果线程停止，则返回True。</li><li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator#request_stop" target="_blank" rel="noopener"><code>tf.train.Coordinator.request_stop</code></a>：请求线程停止。</li><li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator#join" target="_blank" rel="noopener"><code>tf.train.Coordinator.join</code></a>：等待直到指定的线程停止。</li></ul><p>在使用Coordinator时，首先创建一个Coordinator对象，再创建一些Coordinator使用的线程。线程通常一直循环运行，直到should_stop()返回True时停止。任何线程也可以被请求停止，只需要Coordinator对象调用<code>request_stop</code>方法，调用该方法时，其他线程下的<code>should_stop</code>都被返回为True，其他线程停止。如下，通过Coordinator管理python的线程：</p><pre><code class="lang-python"># -*- coding: utf-8 -*-import tensorflow as tfimport threadingdef thread_func(coord, id_num):    &quot;&quot;&quot;    :param coord: tensorflow Coordinator object.    :param id_num: int, the constant number    :return:        id_num, int    &quot;&quot;&quot;    while not coord.should_stop():        print(&quot;This thread id is %d&quot; % id_num)        if id_num &gt;= 9:            print(&quot;Stop thread id is %d&quot; % id_num)            coord.request_stop()# init coord objectcoord_object = tf.train.Coordinator()# create 10 threads and run thread functionthreads = [threading.Thread(target=thread_func, args=(coord_object, num)) for num in range(10)]for t in threads:    t.start()coord_object.join()</code></pre><p>Coordinator还支持捕获和报告异常，如<code>try:.......except Exception as error: coord.request_stop(errr) finally: coord.request_stop()</code>。<br><a id="more"></a></p><h3 id="1-2-队列-Queue"><a href="#1-2-队列-Queue" class="headerlink" title="1.2 队列-Queue"></a>1.2 队列-Queue</h3><p>像TensorFlow中的所有组件一样，队列是TensorFlow图中的一个<strong>节点</strong>。它是一个<strong>有状态</strong>的节点，像变量一样：<strong>其他节点可以修改其内容</strong>。特别地，节点可以将新元素插入队列，或者从现有队列中取出队列。在TensorFlow中，队列主要有四种类型：</p><ul><li><code>tf.FIFOQueue</code> ：按入列顺序出列的队列，先进先出队列</li><li><code>tf.RandomShuffleQueue</code>： 随机顺序出列的队列</li><li><code>tf.PaddingFIFOQueue</code> ：以固定长度批量出列的队列</li><li><code>tf.PriorityQueue</code> ：带优先级出列的队列</li></ul><p>这些队列的使用方法都是一样，主要有：<code>dequeue</code>，<code>enqueue</code>，<code>enqueue_many</code>，<code>dequeue_many</code>等方法。enqueue操作返回计算图中的一个Operation节点，dequeue操作返回一个Tensor值，队列操作同样只是声明或定义，要通过session来运行，才能获取这个值。下面时操作queue：</p><pre><code class="lang-python">queue = tf.FIFOQueue(10, tf.int32)# 如果队列中的值超过了queue size, 在enqueue会卡主,直到queue中的值被消费queue_init = queue.enqueue_many(([0],), name=&quot;init_queue&quot;)  x = queue.dequeue()with tf.Session() as sess:    sess.run(queue_init)    for i in range(9):        print(&quot;--- the %d times enqueue ---&quot; % (i+1))        sess.run(queue.enqueue([i+1]))    for j in range(10):        # 对一个已经取空的队列使用dequeue操作也会卡住,直到有新的数据写入        print(&quot;Get value from queue:&quot;, sess.run(x), &quot;The queue size:&quot;, sess.run(queue.size()))</code></pre><p>还可以将上面的代码进行修改：</p><pre><code class="lang-python">index = tf.placeholder(tf.int32)queue = tf.FIFOQueue(10, tf.int32)queue_init = queue.enqueue_many(([0],), name=&quot;init_queue&quot;)x = queue.dequeue()enqueue_op = queue.enqueue([index+1], name=&quot;enqueue&quot;)with tf.Session() as sess:    sess.run(queue_init)    for i in range(9):        print(&quot;--- the %d times enqueue ---&quot; % (i+1))        sess.run(enqueue_op, feed_dict={index: i})    for j in range(10):        print(&quot;Get value from queue:&quot;, sess.run(x), &quot;The queue size:&quot;, sess.run(queue.size()))</code></pre><h3 id="1-3-QueueRunner"><a href="#1-3-QueueRunner" class="headerlink" title="1.3 QueueRunner"></a>1.3 QueueRunner</h3><p>Tensorflow的计算主要在使用CPU/GPU和内存，而数据读取涉及磁盘操作，速度远低于前者操作。因此通常会使用多个线程读取数据，然后使用一个线程消费数据。QueueRunner就是来管理这些读写队列的线程的（创建线程，对队列进行enqueue或dequeue操作），QueueRunner是一个不存在于代码中的东西，而是后台运作的一个概念；可以通过两种方式来使用QueueRunner；一种是显示的使用QueueRunner，另外一种是隐式使用：<code>tf.train.start_queue_runners</code>。下面是一个多线程读取文件的例子：</p><p><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/AnimatedFileQueues.gif" srcset="/img/loading.gif" alt="img"></p><p>QueueRunner的API和关键方法create_threads的api说明：</p><pre><code class="lang-python">def __init__(self, queue=None, enqueue_ops=None, close_op=None, cancel_op=None,              queue_closed_exception_types=None, queue_runner_def=None, import_scope=None):    &quot;&quot;&quot;    queue：为tensorflow的队列对象,如FIFOQueue, RandomShuffleQueue等    enqueue_ops: 为入队操作(enqueue ops)的列表，列表的长度为定义的线程数    &quot;&quot;&quot;def create_threads(self, sess, coord=None, daemon=False, start=False):    &quot;&quot;&quot;    parameters    ----------    sess: A session    coord: Optional `Coordinator` object     start: Boolean.  If `True` starts the threads.  If `False` the caller must call the `start()` method of the returned threads.    return    ------    A list of threads.    &quot;&quot;&quot;</code></pre><p>下面使用队列和QueueRunner：</p><pre><code class="lang-python">q = tf.FIFOQueue(10, &quot;float&quot;)counter = tf.Variable(0.0)  # 定义计数器increment_op = tf.assign_add(counter, 1.0)  # 给计数器加一enqueue_op = q.enqueue(counter)  # 将计数器加入队列# 创建QueueRunner, 用多个线程向队列添加数据, 这里实际上定义了4个线程, 两个增加计数, 两个执行入队, 但是还没有执行qr = tf.train.QueueRunner(q, enqueue_ops=[increment_op, enqueue_op] * 2)with tf.Session() as sess:    tf.global_variables_initializer().run()    qr.create_threads(sess, start=True)  # 启动入队线程,开始执行    for i in range(20):        print(sess.run(q.dequeue()))</code></pre><p>上面的运行结束会产生异常，但是整个运行过程是正确的。因此，QueueRunner和Coordinator会配合使用，避免这种情况。下面一起合使用。</p><h3 id="1-4-输入流水线（线程队列配合使用）"><a href="#1-4-输入流水线（线程队列配合使用）" class="headerlink" title="1.4 输入流水线（线程队列配合使用）"></a>1.4 输入流水线（线程队列配合使用）</h3><p>下面将1.3中的示例稍加修改就可以避免后面异常的情况。修改后的代码如下所示。通过Coordinator来对所有线程进行同步和停止。</p><h4 id="1-4-1-Coordinator和QueueRunner配合使用"><a href="#1-4-1-Coordinator和QueueRunner配合使用" class="headerlink" title="1.4.1 Coordinator和QueueRunner配合使用"></a>1.4.1 Coordinator和QueueRunner配合使用</h4><pre><code class="lang-python">q = tf.FIFOQueue(10, &quot;float&quot;)counter = tf.Variable(0.0)  # 定义计数器increment_op = tf.assign_add(counter, 1.0)  # 给计数器加一enqueue_op = q.enqueue(counter)  # 将计数器加入队列qr = tf.train.QueueRunner(q, enqueue_ops=[increment_op, enqueue_op] * 2)  # 创建多个线程with tf.Session() as sess:    tf.global_variables_initializer().run()    coord = tf.train.Coordinator()    enqueue_threads = qr.create_threads(sess, coord=coord, start=True)  # 启动入队线程    for i in range(20):        print(sess.run(q.dequeue()))    coord.request_stop()    coord.join(enqueue_threads)</code></pre><p>使用QueueRunner有两种方式，一种是显示的使用QueueRunner，如上面的示例所示，另外一种就是隐式的使用，后面的例子中都是通过隐式（<code>tf.train.start_queue_runners</code>）的使用，在隐式的使用中也是调用的<code>QueueRunner.create_threads</code>的方法。下面将mnist的数据进行流水线操作：</p><pre><code class="lang-python">from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfimport numpy as npmnist = input_data.read_data_sets(&quot;/opt/workspace/project/deep-st-nn/data/MNIST_DATA&quot;, one_hot=True)all_data = mnist.train.imagesall_target = mnist.train.labelsqueue = tf.FIFOQueue(capacity=50, dtypes=[tf.float32, tf.float32], shapes=[[784], [10]])enqueue_op = queue.enqueue_many([all_data, all_target])data_sample, label_sample = queue.dequeue()qr = tf.train.QueueRunner(queue, [enqueue_op] * 4)with tf.Session() as sess:    # create a coordinator, launch the queue runner threads.    coord = tf.train.Coordinator()    enqueue_threads = qr.create_threads(sess, coord=coord, start=True)    for step in range(100):        # do to 100 iterations        if coord.should_stop():            break        one_data, one_label = sess.run([data_sample, label_sample])        print(one_data.shape, one_label.shape)    coord.request_stop()    coord.join(enqueue_threads)</code></pre><h4 id="1-4-2-流水线"><a href="#1-4-2-流水线" class="headerlink" title="1.4.2 流水线"></a>1.4.2 流水线</h4><p>tensorflow的输入流水线：<strong>准备文件名 -&gt; 创建一个<code>Reader</code>从文件中读取数据 -&gt; 定义文件中数据的解码规则 -&gt; 解析数据</strong>。下面是官方给的一个例子，下面的代码中对关键部分进行了注释</p><pre><code class="lang-python">import tensorflow as tf# 一个Queue,用来保存文件名字.对此Queue,只读取,不dequeuefilename_queue = tf.train.string_input_producer([&quot;/opt/workspace/project/deep-st-nn/data/file0.csv&quot;,                                                 &quot;/opt/workspace/project/deep-st-nn/data/file1.csv&quot;])reader = tf.TextLineReader()  # 用来从文件中读取数据, LineReader, 每次读一行key, value = reader.read(filename_queue)# Default values, in case of empty columns. Also specifies the type of therecord_defaults = [[1.0], [1.0], [1.0], [1.0], [1.0]]col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)  # 如果有控制就会用record_defaults填充缺失值features = tf.stack([col1, col2, col3, col4])with tf.Session() as sess:    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(coord=coord)  # 启动线程, 返回所有的线程    for i in range(10):        # Retrieve a single instance:        example, label = sess.run([features, col5])        print(&quot;The times: %d\n&quot; % i, example, label)    coord.request_stop()    coord.join(threads)</code></pre><p>再对上面的代码进行解析：</p><p>（1）<code>tf.train.string_input_producer([&#39;file0.csv&#39;, &#39;file1.csv&#39;])</code>，下面是该函数的关键代码：</p><pre><code class="lang-python">q = data_flow_ops.FIFOQueue(capacity=capacity,                                dtypes=[input_tensor.dtype.base_dtype],                                shapes=[element_shape],                                shared_name=shared_name, name=name)    enq = q.enqueue_many([input_tensor])    queue_runner.add_queue_runner(        queue_runner.QueueRunner(            q, [enq], cancel_op=cancel_op))    if summary_name is not None:      summary.scalar(summary_name,                     math_ops.to_float(q.size()) * (1. / capacity))    return q</code></pre><p>上面，首先创建了一个queue，进行了入队(enqueue)操作；通过QueueRunner创建了一个线程来执行enqueue_op，并将QueueRunner放入了一个collections中。并返回queue。</p><p>（2）定义了数据解析的OP，主要通过TextLineReader来按行进行解析。解析完成后返回了一个Tensor的label和data。<code>TextLineReader.read()</code>方法直接接收一个Queue对象。</p><p>（3）通过session来获取到真实的数据，并进行下一步操作。在session中通过<code>start_queue_runners</code>方法启动所有的线程，并返回所有线程。获取自己需要的值。</p><h4 id="1-4-3-流水线的过程中准备minibatch数据"><a href="#1-4-3-流水线的过程中准备minibatch数据" class="headerlink" title="1.4.3 流水线的过程中准备minibatch数据"></a>1.4.3 流水线的过程中准备minibatch数据</h4><p>在数据输入流水线的过程中，利用多线程来准备好batch数据，并通过dequeue的方式来获取一个minibatch的数据集，在TensorFlow中也实现了，如下所示将上面的代码进行修改：</p><pre><code class="lang-python"># -*- coding: utf-8 -*-import tensorflow as tfdef read_my_file_format(filename_queue):    &quot;&quot;&quot;定义数据的读取与解析规则&quot;&quot;&quot;    reader = tf.TextLineReader()    key, record_string = reader.read(filename_queue)    col1, col2, col3, col4, label = tf.decode_csv(record_string,                                                   record_defaults=[[1.0], [1.0], [1.0], [1.0], [1.0]])    processed_example = tf.stack([col1, col2, col3, col4])    return processed_example, labeldef input_pipeline(filenames, batch_size, num_epochs=None):    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=True)    example, label = read_my_file_format(filename_queue)    min_after_dequeue = 10000    capacity = min_after_dequeue + 3 * batch_size  # queue size    example_batch, label_batch = tf.train.shuffle_batch([example, label], batch_size=batch_size,                                                        capacity=capacity,                                                        min_after_dequeue=min_after_dequeue)    return example_batch, label_batchfiles = [&quot;/opt/workspace/project/deep-st-nn/data/file0.csv&quot;,          &quot;/opt/workspace/project/deep-st-nn/data/file1.csv&quot;]x, y = input_pipeline(files, 5)with tf.Session() as sess:    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord, start=True)    for num_step in range(10):        data, label = sess.run([x, y])        print(data, label)    coord.request_stop()    coord.join(threads)</code></pre><p>上面代码中最关键的部分为<code>shuffle_batch</code>方法，shuffle_batch接受的参数如下所示，该方法所做的事情主要有：</p><ol><li>创建一个<code>RandomShuffleQueue</code>用来保存样本。</li><li>使用<code>QueueRunner</code>创建多个<code>enqueue</code>线程向<code>Queue</code>中放数据。</li><li>创建一个<code>dequeue_many</code> OP。</li><li>返回<code>dequeue_many</code> OP。</li></ol><p>然后我们就可以使用<code>dequeue</code>出来的<code>mini-batch</code>来训练网络了。</p><pre><code class="lang-python">def shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None,                   enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None,                   name=None):    &quot;&quot;&quot;    parameters    ----------        tensors: The list of tensors, this tensor will to enqueue.        batch_size: The new batch size pulled from the queue.        capacity: An integer. The maximum number of elements in the queue.         min_after_dequeue: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.        num_threads: The number of threads enqueuing `tensor_list`.    return    ------    Returns:    A list or dictionary of tensors with the types as `tensors`.    &quot;&quot;&quot;</code></pre><p>通过<code>shuff_batch</code>来实现mnist数据的队列就更加方便了：</p><pre><code class="lang-python">mnist = input_data.read_data_sets(&quot;/opt/workspace/project/deep-st-nn/data/MNIST_DATA&quot;, one_hot=True)all_data = mnist.train.imagesall_target = mnist.train.labelswith tf.Session() as sess:    batch_x, batch_y = tf.train.shuffle_batch([all_data, all_target], batch_size=100, capacity=300+1000,                                              min_after_dequeue=1000)    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord, start=True)    for i in range(21):        print(batch_x.shape, batch_y.shape)    coord.request_stop()    coord.join(threads)</code></pre><p>另外再介绍一个常用的方法，再后续可能会经常用到。<code>tf.train.slice_input_producer</code>，该方法的API为：·<code>slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code>，接收一个tensor的list，再看他的源码：</p><pre><code class="lang-python">with ops.name_scope(name, &quot;input_producer&quot;, tensor_list):    tensor_list = ops.convert_n_to_tensor_or_indexed_slices(tensor_list)    if not tensor_list:      raise ValueError(          &quot;Expected at least one tensor in slice_input_producer().&quot;)    range_size = array_ops.shape(tensor_list[0])[0]    # TODO(josh11b): Add an assertion that the first dimension of    # everything in TensorList matches. Maybe just check the inferred shapes?    queue = range_input_producer(range_size, num_epochs=num_epochs,                                 shuffle=shuffle, seed=seed, capacity=capacity,                                 shared_name=shared_name)    index = queue.dequeue()    output = [array_ops.gather(t, index) for t in tensor_list]    return output</code></pre><p>该方法再内部调用<code>range_input_producer</code>方法，生成一个队列，并将出队的结果进行组合，形成一个list，返回（<code>A list of tensors, one for each element of tensor_list.  If the tensor n tensor_list has shape [N, a, b, .., z], then the corresponding outputtensor will have shape [a, b, ..., z].</code>）。返回的是一个出队后的元素。</p><h3 id="1-5-Refrence"><a href="#1-5-Refrence" class="headerlink" title="1. 5 Refrence"></a>1. 5 Refrence</h3><p>[1].<a href="https://blog.csdn.net/rockingdingo/article/details/55652662" target="_blank" rel="noopener">Tensorflow并行计算：多核(multicore)，多线程(multi-thread)，图分割(Graph Partition)</a></p><p>[2].<a href="https://www.jianshu.com/p/c29965e6c40c" target="_blank" rel="noopener">TensorFlow中的多线程使用</a></p><p>[3].<a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029441" target="_blank" rel="noopener">TensorFlow的线程和队列</a></p><p>[4].<a href="https://blog.csdn.net/u012436149/article/details/72353313" target="_blank" rel="noopener">输入流水线</a></p><p>[5].<a href="https://ischlag.github.io/2016/06/19/tensorflow-input-pipeline-example/" target="_blank" rel="noopener">TENSORFLOW INPUT PIPELINE EXAMPLE</a></p><p>[6].<a href="http://www.enpeizhao.com/?p=514" target="_blank" rel="noopener">tensorflow 管道队列模式（pipeline）读取文件</a></p><p>[7].<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/reading_data.html" target="_blank" rel="noopener">数据读取</a></p><p>[8].<a href="https://www.jianshu.com/p/f1aeb7ef75ca" target="_blank" rel="noopener">TensorFlow-input pipeline</a></p><h2 id="2-共享变量"><a href="#2-共享变量" class="headerlink" title="2.共享变量"></a>2.共享变量</h2><p>在深度学习网络中，随模型复杂程度增加，模型的参数会不断的增加，因此训练模型过程中会涉及到<strong>变量共享</strong>。这样减小模型中参数的量，甚至在部分模型中需要共享变量，才能达到好的效果。</p><h3 id="2-1-问题的提出"><a href="#2-1-问题的提出" class="headerlink" title="2.1 问题的提出"></a>2.1 问题的提出</h3><p>假设，我们创建一个图像过滤模型，设置模型中为2个卷积，现在要将同一个模型(相同参数的模型)应用到不同的图片下，实现大量图片的过滤。如果通过<code>tf.Variable()</code>实现如下：</p><pre><code class="lang-python">def my_image_filter(input_images):    conv1_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]), name=&quot;conv1_weights&quot;)    conv1_biases = tf.Variable(tf.zeros([32]), name=&quot;conv1_biases&quot;)    conv1 = tf.nn.conv2d(input_images, conv1_weights, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)    relu1 = tf.nn.relu(conv1 + conv1_biases)    conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]), name=&quot;conv2_weights&quot;)    conv2_biases = tf.Variable(tf.zeros([32]), name=&quot;conv2_biases&quot;)    conv2 = tf.nn.conv2d(relu1, conv2_weights, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)    return tf.nn.relu(conv2 + conv2_biases)</code></pre><p>在这个模型中，我们有四个变量：<code>conv1_weight,conv1_biases,conv2_weights,conv2_biases</code>，当我们应用在每个图片上时，如下，我们应用在2个图片上：</p><pre><code class="lang-python">result1 = my_image_filter(image1)  # First call creates one set of 4 variables.result2 = my_image_filter(image2)  # Another set of 4 variables is created in the second call.</code></pre><p>这样调用2次，那么会创建2组变量，每组4个变量，生成8个变量。这样就出现了问题，每张图片应用的模型是不一样的。因此需要通过共享变量的模式。那么怎么来实现呢？</p><h3 id="2-2-python实现共享变量"><a href="#2-2-python实现共享变量" class="headerlink" title="2.2 python实现共享变量"></a>2.2 python实现共享变量</h3><p>在python中也是可以实现的，通过字典提前将变量进行定义，如下：</p><pre><code class="lang-python">parameters_dict = {&quot;conv1_weights&quot;: tf.Variable(tf.random_normal([5, 5, 32, 32]), name=&quot;conv1_weights&quot;),                  &quot;conv1_biases&quot;: tf.Variable(tf.zeros([32]), name=&quot;conv1_biases&quot;),                  &quot;conv2_weights&quot;: tf.Variable(tf.random_normal([5, 5, 32, 32]), name=&quot;conv2_weights&quot;),                  &quot;conv2_biases&quot;: tf.Variable(tf.zeros([32]), name=&quot;conv2_biases&quot;)}def my_image_filter(input_images, variables_dict):    conv1 = tf.nn.conv2d(input_images, variables_dict[&quot;conv1_weights&quot;], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)    relu1 = tf.nn.relu(conv1 + variables_dict[&quot;conv1_biases&quot;])    conv2 = tf.nn.conv2d(relu1, variables_dict[&quot;conv2_weights&quot;], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)    return tf.nn.relu(conv2 + variables_dict[&quot;conv2_biases&quot;])# Both calls to my_image_filter() now use the same variablesresult1 = my_image_filter(image1, variables_dict)result2 = my_image_filter(image2, variables_dict)</code></pre><p>这样是可以实现共享变量，但是对代码的<strong>封装不好</strong>，在构建模型时，<strong>需要提前创建好变量</strong>的名称，大小，类型等，当网络非常大时，需要维护一个非常长的参数列表，这样代码显得非常冗余；同时，这样代码显得比较死板，不灵活，扩展性很弱。因此TensorFlow提供了更加轻便的方式来实现共享变量。</p><h3 id="2-3-在TensorFlow中实现共享变量"><a href="#2-3-在TensorFlow中实现共享变量" class="headerlink" title="2.3 在TensorFlow中实现共享变量"></a>2.3 在TensorFlow中实现共享变量</h3><p>在TensorFlow中通过<code>tf.get_variable()</code>和<code>`tf.variable_scope()</code>配合使用，实现变量共享。这两个方法的作用主要如下所示，下面通过这两个方法来实现变量共享。</p><pre><code class="lang-python">tf.get_variable(&lt;name&gt;, &lt;shape&gt;, &lt;initializer&gt;)  # 创建或返回具有给定名称的变量tf.variable_scope(&lt;scope_name&gt;)  # 管理传递给的名称的名称空间tf.get_variable()</code></pre><p>对上面的过滤模型进行修改，如下：</p><pre><code class="lang-python">def conv_relu(input_tensor, kernel_shape, bias_shape):    # Create variable named &quot;weights&quot;.    weights = tf.get_variable(&quot;weights&quot;, kernel_shape, initializer=tf.random_normal_initializer())    # Create variable named &quot;biases&quot;.    biases = tf.get_variable(&quot;biases&quot;, bias_shape, initializer=tf.constant_initializer(0.0))    conv = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)    return tf.nn.relu(conv + biases)def my_image_filter(input_images):    with tf.variable_scope(&quot;conv1&quot;):        # Variables created here will be named &quot;conv1/weights&quot;, &quot;conv1/biases&quot;.        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])    with tf.variable_scope(&quot;conv2&quot;):        # Variables created here will be named &quot;conv2/weights&quot;, &quot;conv2/biases&quot;.        return conv_relu(relu1, [5, 5, 32, 32], [32])result1 = my_image_filter(image1)result2 = my_image_filter(image2)</code></pre><p>上面对模型应用到两个图片上面，会提示一个错误：<code>Raises ValueError(... conv1/weights already exists ...)</code>；在conv2中会引发错误，主要因为<code>tf.get_variable()</code>默认变量是不共享的，只是检查变量名，防止重复，因此在conv2中调用的时候，发现已经存在了变量。需要共享变量，必须指定某个变量域内进行共享变量：</p><pre><code class="lang-python">with tf.variable_scope(&quot;image_filters&quot;) as scope:    result1 = my_image_filter(image1)    scope.reuse_variables()    result2 = my_image_filter(image2)</code></pre><p>经过上面的修改，变量共享就已经完成了。不需要在函数外定义变量，只需要添加变量域，tensorflow就会自动帮我管理变量。代码也非常直观。</p><h3 id="2-4-共享变量的方式"><a href="#2-4-共享变量的方式" class="headerlink" title="2.4 共享变量的方式"></a>2.4 共享变量的方式</h3><p>通过<code>tf.get_variable()</code>和<code>`tf.variable_scope()</code>有两种方式来进行共享变量。第一种就是上述所示的，通过设置域下面的共享：<code>scope.reuse_variables()</code>；还有一种方式，如下：</p><pre><code class="lang-python">with tf.variable_scope(&quot;image_filters&quot;, reuse=tf.AUTO_REUSE) as scope:    result1 = my_image_filter(image1)    result2 = my_image_filter(image2)</code></pre><p>这种方式会自动去共享变量，当系统检测到当期变量域下之前定义了一个重名的变量，那么该变量就共享，否则就创建新的变量。这是非常智能的写法。这种方式也解决了一个问题，比如部分模型，前半部分需要共享变量，后半部分不需要共享变量，可以通过这种方式来实现。</p><h3 id="2-5-变量域（variable-scope）的工作机制"><a href="#2-5-变量域（variable-scope）的工作机制" class="headerlink" title="2.5 变量域（variable_scope）的工作机制"></a>2.5 变量域（variable_scope）的工作机制</h3><h4 id="2-5-1-get-variable的理解"><a href="#2-5-1-get-variable的理解" class="headerlink" title="2.5.1 get_variable的理解"></a>2.5.1 get_variable的理解</h4><p>首先，对<code>tf.get_variable()</code>进行理解，该方法的使用主要取决于调用的域的设置：<code>tf.get_variable_scope().reuse == False or tf.get_variable_scope().reuse == True</code>。当结果值 为False时，这是<code>tf.get_variable()</code>就会初始化一个变量，并且会判断这个变量在这个域下是否存在，如果存在就会引发<code>ValueError</code>，否则就会初始化一个变量出来：</p><pre><code class="lang-python">with tf.variable_scope(&quot;foo&quot;):    v = tf.get_variable(&quot;v&quot;, [1])assert v.name == &quot;foo/v:0&quot;</code></pre><p>而如果 <code>tf.get_variable_scope().reuse == True</code>，那么 TensorFlow 会执行相反的动作，就是到程序里面寻找变量名为 <code>scope name + name</code> 的变量，如果变量不存在，会抛出 <code>ValueError</code> 异常，否则，就返回找到的变量：</p><pre><code class="lang-python">with tf.variable_scope(&quot;foo&quot;):    v = tf.get_variable(&quot;v&quot;, [1])with tf.variable_scope(&quot;foo&quot;, reuse=True):    v1 = tf.get_variable(&quot;v&quot;, [1])assert v1 is v</code></pre><h4 id="2-5-2-variable-scope的基本使用"><a href="#2-5-2-variable-scope的基本使用" class="headerlink" title="2.5.2 variable_scope的基本使用"></a>2.5.2 variable_scope的基本使用</h4><p>变量域时可以嵌套使用的，嵌套后，变量名会以此加上变量域作为路径。如下代码：</p><pre><code class="lang-python">with tf.variable_scope(&quot;foo&quot;):    with tf.variable_scope(&quot;bar&quot;):        v = tf.get_variable(&quot;v&quot;, [1])assert v.name == &quot;foo/bar/v:0&quot;</code></pre><p>我们也可以通过 <code>tf.get_variable_scope()</code> 来获得当前的变量域对象，并通过 <code>reuse_variables()</code> 方法来设置是否共享变量。不过，TensorFlow 并不支持将 <code>reuse</code> 值设为 <code>False</code>，如果你要停止共享变量，可以选择离开当前所在的变量域，或者再进入一个新的变量域（比如，再进入一个 <code>with</code> 语句，然后指定新的域名）。</p><p>还需注意的一点是，一旦在一个变量域内将 <code>reuse</code> 设为 <code>True</code>，那么这个变量域的子变量域也会继承这个 <code>reuse</code> 值，自动开启共享变量：</p><pre><code class="lang-python">with tf.variable_scope(&quot;root&quot;):    # At start, the scope is not reusing.    assert tf.get_variable_scope().reuse == False    with tf.variable_scope(&quot;foo&quot;):        assert tf.get_variable_scope().reuse == False    with tf.variable_scope(&quot;foo&quot;, reuse=True):        assert tf.get_variable_scope().reuse == True        with tf.variable_scope(&quot;bar&quot;):            assert tf.get_variable_scope().reuse == True    assert tf.get_variable_scope().reuse == False</code></pre><p>变量域也可以 作为一个对象，这样方便使用变量域，跳出当前变量域等。如下面的代码所示：</p><pre><code class="lang-python">with tf.variable_scope(&quot;foo&quot;) as foo_scope:    assert foo_scope.name == &quot;foo&quot;with tf.variable_scope(&quot;bar&quot;)    with tf.variable_scope(&quot;baz&quot;) as other_scope:        assert other_scope.name == &quot;bar/baz&quot;        with tf.variable_scope(foo_scope) as foo_scope2:            assert foo_scope2.name == &quot;foo&quot;  # Not changed.</code></pre><h4 id="2-5-3-在变量域内初始化变量"><a href="#2-5-3-在变量域内初始化变量" class="headerlink" title="2.5.3 在变量域内初始化变量"></a>2.5.3 在变量域内初始化变量</h4><p>每次初始化变量时都要传入一个 <code>initializer</code>，这实在是麻烦，而如果使用变量域的话，就可以批量初始化参数了，如下代码所示：</p><pre><code class="lang-python">with tf.variable_scope(&quot;foo&quot;, initializer=tf.constant_initializer(0.4)):    v = tf.get_variable(&quot;v&quot;, [1])    assert v.eval() == 0.4  # Default initializer as set above.    w = tf.get_variable(&quot;w&quot;, [1], initializer=tf.constant_initializer(0.3)):    assert w.eval() == 0.3  # Specific initializer overrides the default.    with tf.variable_scope(&quot;bar&quot;):        v = tf.get_variable(&quot;v&quot;, [1])        assert v.eval() == 0.4  # Inherited default initializer.    with tf.variable_scope(&quot;baz&quot;, initializer=tf.constant_initializer(0.2)):        v = tf.get_variable(&quot;v&quot;, [1])        assert v.eval() == 0.2  # Changed default initializer.</code></pre><h2 id="3-最新的数据处理类—data-Dataset"><a href="#3-最新的数据处理类—data-Dataset" class="headerlink" title="3. 最新的数据处理类—data.Dataset"></a>3. 最新的数据处理类—data.Dataset</h2><p>这部分和1中对线程和队列的功能有些类似，但是这部分更多在数据输入部分，第一部分中的还有在其他方面使用。<code>Dataset</code>API是TensorFlow在版本1.3中引入的模块，1.4版本中已经作为一个核心的模块。主要服务于数据读取，构建数据的pipeline。前面说了可以通过队列和线程构建，但是整个过程还是比较繁琐，TensorFlow便可以通过这种方式来构建。主要支持从内存和硬盘读取数。</p><h3 id="3-1-使用Dataset的步骤"><a href="#3-1-使用Dataset的步骤" class="headerlink" title="3.1 使用Dataset的步骤"></a>3.1 使用<code>Dataset</code>的步骤</h3><p>在数据输入中用<code>Dataset</code>模块，需要三个步骤：</p><ol><li>导入数据，从一些数据来构建dataset，创建dataset对象， 可以通过<code>from_tensors</code>，<code>from_tensors_slice</code></li><li>实例化，将dataset实例化为<strong>Iterator</strong>，下图中为dataset下几个实例的关系：</li></ol><p><img src="http://img.blog.csdn.net/20171114112942700" srcset="/img/loading.gif" alt="Dataset API 下的类图"></p><ol><li>消费数据，在Iterator的基础上对数据进行消费，进行下一步的计算或训练</li></ol><h3 id="3-2-基本使用"><a href="#3-2-基本使用" class="headerlink" title="3.2 基本使用"></a>3.2 基本使用</h3><p>在最开始使用时可以只关注Dataset和Iterator这两个类，再进行逐步的扩展到其他类的使用。Dataset可以看作是相同类型“元素”的有序列表。在实际使用时，单个“元素”可以是向量，也可以是字符串、图片，甚至是tuple或者dict。</p><p>在消费数据的时候，是通过get_next方法获取数据。不论通过什么方式创建数据集，在返回数据时都是返回一行或多行数据。下面的几个dataset就可以看出返回数据的规律。</p><h4 id="3-2-1-from-numpy的数据"><a href="#3-2-1-from-numpy的数据" class="headerlink" title="3.2.1 from numpy的数据"></a>3.2.1 from numpy的数据</h4><p>从numpy下的array读取数据到dataset：</p><pre><code class="lang-python">dataset = tf.data.Dataset.from_tensor_slices(np.random.uniform(size=(5, 3)))  # 1.import dataiterator = dataset.make_one_shot_iterator()  # 2.从dataset实例化一个iteratorone_element = iterator.get_next()  # 3.消费数据with tf.Session() as sess:    try:        for i in range(6):            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end&quot;)# ---------------------- output ----------------------# 每次返回一行数据, 共返回5次, 相当于5个样本，3个特征</code></pre><h4 id="3-2-2-从字典中创建dataset"><a href="#3-2-2-从字典中创建dataset" class="headerlink" title="3.2.2 从字典中创建dataset"></a>3.2.2 从字典中创建dataset</h4><pre><code class="lang-python">dataset = tf.data.Dataset.from_tensor_slices(    {        &quot;a&quot;: np.array([1.0, 2.0, 3.0, 4.0, 5.0]),  # &quot;可以将key-a当做label的序列&quot;        &quot;b&quot;: np.random.uniform(size=(5, 2))    })dataset = dataset.map(lambda x: {&quot;a&quot;: x[&quot;a&quot;], &quot;b&quot;: 10 * x[&quot;b&quot;]})iterator = dataset.make_one_shot_iterator()  # 从dataset实例化一个iteratorone_element = iterator.get_next()with tf.Session() as sess:    try:        for i in range(6):            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end&quot;)# ------------------ output -----------------------{&#39;a&#39;: 1.0, &#39;b&#39;: array([4.18069729, 3.36752717])}  # 每次会一个lable所对应的sample，共返回5次{&#39;a&#39;: 2.0, &#39;b&#39;: array([7.37556694, 1.79710602])}{&#39;a&#39;: 3.0, &#39;b&#39;: array([1.76684338, 0.48396737])}{&#39;a&#39;: 4.0, &#39;b&#39;: array([6.21267904, 5.28298128])}{&#39;a&#39;: 5.0, &#39;b&#39;: array([8.36019678, 2.08220728])}</code></pre><h4 id="3-2-3-从tuple中创建dataset"><a href="#3-2-3-从tuple中创建dataset" class="headerlink" title="3.2.3 从tuple中创建dataset"></a>3.2.3 从tuple中创建dataset</h4><pre><code class="lang-python">dataset = tf.data.Dataset.from_tensor_slices(  (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.random.uniform(size=(5, 2))))dataset = dataset.map(lambda x, y: (x, y*10))iterator = dataset.make_one_shot_iterator()  # 从dataset实例化一个iteratorone_element = iterator.get_next()with tf.Session() as sess:    try:        for i in range(6):            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end&quot;)# ---------------------- output ------------------------(1.0, array([2.45236335, 9.04392201]))  # 这个返回方式和字典类似(2.0, array([5.16675082, 4.89549424]))(3.0, array([3.66104816, 3.01531896]))(4.0, array([8.56580726, 3.77034437]))(5.0, array([8.18391386, 6.2216879 ]))</code></pre><h4 id="3-2-4-从生成器创建dataset"><a href="#3-2-4-从生成器创建dataset" class="headerlink" title="3.2.4 从生成器创建dataset"></a>3.2.4 从生成器创建dataset</h4><pre><code class="lang-python">sequence = np.array([[1, 2, 2.3464],[2, 3, 45.253],[3, 4, 455.22]])def generator():    for el in sequence:        yield eldataset = tf.data.Dataset().from_generator(generator,                                           output_types=tf.float32)iterator = dataset.make_one_shot_iterator()  # 从dataset实例化一个iteratorone_element = iterator.get_next()with tf.Session() as sess:    try:        for i in range(6):            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end&quot;)# ---------------------- output ---------------------[1.     2.     2.3464]  # 这个返回方式和第一种方式类似[ 2.     3.    45.253][  3.     4.   455.22]</code></pre><h3 id="3-3-在dataset中进行数据处理"><a href="#3-3-在dataset中进行数据处理" class="headerlink" title="3.3 在dataset中进行数据处理"></a>3.3 在dataset中进行数据处理</h3><p>dataset中也可以对数据进行处理，变换等。主要有的方法：map，cache，shuffle，repeat，batch，prefetch，fileter，flat_map等。这些方法在处理数据时会经常用大。</p><h4 id="3-3-1-map的使用"><a href="#3-3-1-map的使用" class="headerlink" title="3.3.1 map的使用"></a>3.3.1 map的使用</h4><p>dataset中的map和python的map用法一致，接受一个处理函数。再返回处理后的数据。map主要接收两个参数，它的的api是：<code>map(map_func, num_parallel_calls=None)</code>，第一参数为map函数，用来变换数据；第二个参数为并发数，一般为cpu的线程数。如：</p><pre><code class="lang-python">dataset = tf.data.Dataset.from_tensor_slices(  (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.random.uniform(size=(5, 2))))dataset = dataset.map(lambda x, y: (x, y*10), num_parallel_calls=4)  # map的使用，线程数为4iterator = dataset.make_one_shot_iterator() one_element = iterator.get_next()with tf.Session() as sess:    try:        for i in range(6):            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end&quot;)</code></pre><p>需要注意的是，dataset有<strong>map和apply</strong>两个方法，它们是有一定<strong>区别</strong>的，map是将map_function应用到每一个函数。而apply是将function应用到整个dataset。map的参数是一个element ，而apply的函数参数是dataset，apply可用的方法<a href="https://www.tensorflow.org/versions/master/api_guides/python/input_dataset#Transformations_on_existing_datasets" target="_blank" rel="noopener">在这儿</a>。如下：</p><pre><code class="lang-python">dataset = tf.data.Dataset.from_tensor_slices(  (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.random.uniform(size=(5, 2))))dataset = dataset.apply(tf.contrib.data.map_and_batch(lambda x, y: (x, y * 100), 2))iterator = dataset.make_one_shot_iterator() one_element = iterator.get_next()with tf.Session() as sess:    try:        for i in range(5):            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end&quot;)</code></pre><h4 id="3-3-2-cache的使用"><a href="#3-3-2-cache的使用" class="headerlink" title="3.3.2 cache的使用"></a>3.3.2 cache的使用</h4><p>cache主要是将数据进行缓存，可以缓存到内存，也可以缓存到磁盘。默认为缓存到内存中。比较好理解，具体就不介绍了。</p><h4 id="3-3-3-shuffle的使用"><a href="#3-3-3-shuffle的使用" class="headerlink" title="3.3.3 shuffle的使用"></a>3.3.3 shuffle的使用</h4><p>shuffle的功能为打乱dataset中的元素，它有一个参数buffersize，表示打乱时使用的buffer的大小，建议舍的不要太小，一般建议是dataset的size+1，即样本数+1。如下代码，输出的顺序被打乱了。</p><pre><code class="lang-python">dataset = tf.data.Dataset.from_tensor_slices(  (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.random.uniform(size=(5, 2))))dataset = dataset.map(lambda x, y: (x, y*10), num_parallel_calls=4).shuffle(buffer_size=6)iterator = dataset.make_one_shot_iterator()  # 从dataset实例化一个iteratorone_element = iterator.get_next()with tf.Session() as sess:    try:        for i in range(5):            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end&quot;)# --------------------- output ------------------------(4.0, array([2.57439552, 3.38879843]))(2.0, array([7.25460172, 3.86126726]))(1.0, array([9.08901095, 1.75219504]))(5.0, array([5.04888193, 3.89428254]))(3.0, array([9.78304135, 3.44271984]))</code></pre><h4 id="3-3-4-repeat的使用"><a href="#3-3-4-repeat的使用" class="headerlink" title="3.3.4 repeat的使用"></a>3.3.4 repeat的使用</h4><p>repeat的功能就是将整个序列重复多次，主要用来处理机器学习中的epoch，假设原先的数据是一个epoch，使用repeat(2)就可以将之变成2个epoch：</p><pre><code class="lang-python">dataset = tf.data.Dataset.from_tensor_slices({&quot;a&quot;: np.array([1.0, 2.0, 3.0, 4.0, 5.0]),                                               &quot;b&quot;: np.random.uniform(size=(5, 2))})dataset = dataset.repeat(2) # repeatiterator = dataset.make_one_shot_iterator()one_element = iterator.get_next()with tf.Session() as sess:    try:        while True:            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end!&quot;)# --------------------- output --------------------{&#39;a&#39;: 1.0, &#39;b&#39;: array([0.13072499, 0.81223459])}{&#39;a&#39;: 2.0, &#39;b&#39;: array([0.67836451, 0.02996121])}{&#39;a&#39;: 3.0, &#39;b&#39;: array([0.17338524, 0.73540362])}{&#39;a&#39;: 4.0, &#39;b&#39;: array([0.57598212, 0.11428893])}{&#39;a&#39;: 5.0, &#39;b&#39;: array([0.55184749, 0.8721738 ])}{&#39;a&#39;: 1.0, &#39;b&#39;: array([0.13072499, 0.81223459])}{&#39;a&#39;: 2.0, &#39;b&#39;: array([0.67836451, 0.02996121])}{&#39;a&#39;: 3.0, &#39;b&#39;: array([0.17338524, 0.73540362])}{&#39;a&#39;: 4.0, &#39;b&#39;: array([0.57598212, 0.11428893])}{&#39;a&#39;: 5.0, &#39;b&#39;: array([0.55184749, 0.8721738 ])}</code></pre><h4 id="3-3-5-batch的使用"><a href="#3-3-5-batch的使用" class="headerlink" title="3.3.5 batch的使用"></a>3.3.5 batch的使用</h4><p>batch就是将多个样本组合成batch，如API所说，按照输入元素第一个维度进行组合成一个batch：</p><pre><code class="lang-python"># batch的使用dataset = tf.data.Dataset.from_tensor_slices(  (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.random.uniform(size=(5, 2))))dataset = dataset.map(lambda x, y: (x, y*10), num_parallel_calls=4).shuffle(buffer_size=6).batch(3)iterator = dataset.make_one_shot_iterator()  # 从dataset实例化一个iteratorone_element = iterator.get_next()with tf.Session() as sess:    try:        for i in range(5):            print(sess.run(one_element))    except tf.errors.OutOfRangeError:        print(&quot;end&quot;)# ------------------------- output ------------------------(array([4., 3., 5.]),  array([[6.32798625, 3.34494733],       [8.6747479 , 6.6041502 ],       [2.96699653, 9.73854298]]))(array([1., 2.]),  array([[4.75347977, 5.51553647],       [4.12808918, 6.81227941]]))</code></pre><p>这些操作都可以组合起来使用。如上面的例子，将map，shuffle，batch进行组合使用。</p><h3 id="3-4-模拟读取文件并通过dataset进行处理"><a href="#3-4-模拟读取文件并通过dataset进行处理" class="headerlink" title="3.4 模拟读取文件并通过dataset进行处理"></a>3.4 模拟读取文件并通过dataset进行处理</h3><p>可以将第一部分中读取文件数据的例子近修改，通过dataset的方式进行处理。下面再dataset中存入的是每张图片的路径，后面通过map读取数据，并后续继续使用。具体代码如下：</p><pre><code class="lang-python">def _parse_function(filename, label):    image_string = tf.read_file(filename)    image_decoded = tf.image.decode_image(image_string)    image_resized = tf.image.resize_images(image_decoded, [28, 28])    return image_resized, labelfilenames = tf.constant([&quot;/var/data/image1.jpg&quot;, &quot;/var/data/image2.jpg&quot;, ...])labels = tf.constant([0, 37, ...])dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))dataset = dataset.map(_parse_function)dataset = dataset.shuffle(buffersize=1000).batch(32).repeat(10)</code></pre><h3 id="3-5-创建dataset的其他方法"><a href="#3-5-创建dataset的其他方法" class="headerlink" title="3.5 创建dataset的其他方法"></a>3.5 创建dataset的其他方法</h3><p>除了<code>tf.data.Dataset.from_tensor_slices</code>外，目前Dataset API还提供了另外三种创建Dataset的方式：</p><ul><li>tf.data.TextLineDataset()：这个函数的输入是一个文件的列表，输出是一个dataset。dataset中的每一个元素就对应了文件中的一行。可以使用这个函数来读入CSV文件。</li><li>tf.data.FixedLengthRecordDataset()：这个函数的输入是一个文件的列表和一个record_bytes，之后dataset的每一个元素就是文件中固定字节数record_bytes的内容。通常用来读取以二进制形式保存的文件，如CIFAR10数据集就是这种形式。</li><li>tf.data.TFRecordDataset()：顾名思义，这个函数是用来读TFRecord文件的，dataset中的每一个元素就是一个TFExample。</li></ul><h3 id="3-6-将dataset实例化为iterator的其他方法"><a href="#3-6-将dataset实例化为iterator的其他方法" class="headerlink" title="3.6 将dataset实例化为iterator的其他方法"></a>3.6 将dataset实例化为iterator的其他方法</h3><p>除了这种one shot iterator外，还有三个更复杂的Iterator，即：<code>initializable iterator, reinitializable iterator, feedable iterator</code>.</p><h4 id="3-6-1-initializable-iterator实例化"><a href="#3-6-1-initializable-iterator实例化" class="headerlink" title="3.6.1 initializable iterator实例化"></a>3.6.1 initializable iterator实例化</h4><p><code>initializable iterator</code>方法要在使用前通过<code>sess.run()</code>来初始化，使用initializable iterator，可以将placeholder代入Iterator中，实现更为灵活的数据载入，实际上占位符引入了dataset对象创建中，我们可以通过feed来控制数据集合的实际情况。示例代码如下：</p><pre><code class="lang-python">limit = tf.placeholder(dtype=tf.int32, shape=[])dataset = tf.data.Dataset.from_tensor_slices(tf.range(start=0, limit=limit))iterator = dataset.make_initializable_iterator()next_element = iterator.get_next()with tf.Session() as sess:    sess.run(iterator.initializer, feed_dict={limit: 10})    for i in range(10):      value = sess.run(next_element)      print(value)      assert i == value# output:0 1 2 3 4 5 6 7 8 9</code></pre><p><code>initializable iterator</code>还有一个功能：读入较大的数组。在使用<code>tf.data.Dataset.from_tensor_slices(array)</code>时，实际上发生的事情是将<code>array</code>作为一个<code>tf.constants</code>保存到了计算图中。当<code>array</code>很大时，会导致计算图变得很大，给传输、保存带来不便。这时，我们可以用一个<code>placeholder</code>取代这里的<code>array</code>，并使用<code>initializable iterator</code>，只在需要时将<code>array</code>传进去，这样就可以避免把大数组保存在图里，示例代码为（来自官方例程）：</p><pre><code class="lang-python">with np.load(&quot;/var/data/training_data.npy&quot;) as data:    features = data[&quot;features&quot;]    labels = data[&quot;labels&quot;]features_placeholder = tf.placeholder(features.dtype, features.shape)labels_placeholder = tf.placeholder(labels.dtype, labels.shape)dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))iterator = dataset.make_initializable_iterator()sess.run(iterator.initializer, feed_dict={features_placeholder: features, labels_placeholder: labels})</code></pre><p>可见，在上面程序中，feed也遵循着类似字典一样的规则，创建两个占位符(keys)，给<code>data_holder</code>去feed数据文件，给<code>label_holder</code>去feed标签文件。</p><h4 id="3-6-2-reinitializable-iterator实例化"><a href="#3-6-2-reinitializable-iterator实例化" class="headerlink" title="3.6.2 reinitializable iterator实例化"></a>3.6.2 reinitializable iterator实例化</h4><p><code>reinitializable iterator</code>可以从多个不同的Dataset对象处初始化。例如，你可能有一个training input pipeline（它对输入图片做随机扰动来提高泛化能力）；以及一个validation input pipeline（它会在未修改过的数据上进行预测的评估）。这些pipeline通常使用不同的Dataset对象，但它们具有相同的结构（例如：对每个component相同的types和shapes）。</p><pre><code class="lang-python"># Define training and validation datasets with the same structure.training_dataset = tf.data.Dataset.range(100).map(lambda x: x + tf.random_uniform([], -10, 10, tf.int64))validation_dataset = tf.data.Dataset.range(50)# A reinitializable iterator is defined by its structure. We could use the# `output_types` and `output_shapes` properties of either `training_dataset`# or `validation_dataset` here, because they are compatible.iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)next_element = iterator.get_next()training_init_op = iterator.make_initializer(training_dataset)validation_init_op = iterator.make_initializer(validation_dataset)# Run 20 epochs in which the training dataset is traversed, followed by the# validation dataset.for _ in range(20):    # Initialize an iterator over the training dataset.    sess.run(training_init_op)    for _ in range(100):        sess.run(next_element)        # Initialize an iterator over the validation dataset.        sess.run(validation_init_op)        for _ in range(50):            sess.run(next_element)</code></pre><h3 id="3-7-Refrence"><a href="#3-7-Refrence" class="headerlink" title="3.7 Refrence"></a>3.7 Refrence</h3><p>[1].<a href="http://d0evi1.com/tensorflow/datasets/" target="_blank" rel="noopener">tensorflow中的dataset</a></p><p>[2].<a href="https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428" target="_blank" rel="noopener">How to use Dataset in TensorFlow</a></p><p>[3].<a href="https://www.cnblogs.com/hellcat/p/8569651.html" target="_blank" rel="noopener">TensorFlow』数据读取类_data.Dataset</a></p><h2 id="4-collection"><a href="#4-collection" class="headerlink" title="4.collection"></a>4.collection</h2><p>TensorFlow中有一个集合叫collection，collection主要是提供tensorflow全局存储的机制，不会受到变量名空间的影响，在一个地方保存，任何地方可用。</p><p>TensorFlow自身会维护一些自己的collection，如<code>tf.GraphKeys.SUMMARIES</code>，<code>ops.GraphKeys.QUEUE_RUNNERS</code>(<code>from tensorflow.python.framework import ops</code>)，<code>GLOBAL_VARIABLES</code>，<code>ops.GraphKyes.LOCAL_VARIABLES</code>等等。在使用TensorFlow的过程中也可以获取这些collection。</p><h3 id="4-1-向collection中存入数据"><a href="#4-1-向collection中存入数据" class="headerlink" title="4.1 向collection中存入数据"></a>4.1 向collection中存入数据</h3><p>在编程过程中可以向collection存入数据，主要通过<code>add_to_collection</code>方法将数据加入collection中。该方法是<code>tf.add_to_collection(name, value)</code>。该方法<code>`tf.add_to_collection</code> 的作用是将value以name的名称存储在收集器(<code>self._collections</code>)中。另外还可以同时将一个value增加到多个collection中，通过方法<code>tf.add_to_collections(names, value)</code>。</p><h3 id="4-2-从collection中获取数据"><a href="#4-2-从collection中获取数据" class="headerlink" title="4.2 从collection中获取数据"></a>4.2 从collection中获取数据</h3><p>如果要从collection中获取数据，需要通过<code>tf.get_collection(name, scope=None)</code>来获取集合中的变量值。将返回一个list，list中的值都是之前存入的（如果name不存在，则返回一个空的list）。必须给定一个name参数，来指定获取collection下某个name下的值。scope参数用来过滤某个scope下的值。下面是一个完整的示例：</p><pre><code class="lang-python">import numpy as npimport tensorflow as tfx = tf.constant(np.random.random(10) * 10, dtype=tf.float32, name=&quot;const&quot;)with tf.name_scope(&quot;scope_y&quot;):    z = tf.constant(np.random.random(5) * 100, dtype=tf.float32, name=&quot;const&quot;)x_ = x * 10tf.add_to_collection(&quot;x&quot;, x_)tf.add_to_collection(&quot;x&quot;, x)  # 向一个name增加多个值, 如果增加[x, x_], 那么输出的是[[x, x_]]with tf.Session() as sess:    print(sess.run(x_))    # tf.graph.add_to_collection(&quot;x&quot;, x_)    print(sess.run(tf.get_collection(&quot;x&quot;)))    print(sess.run(tf.get_collection(&quot;y&quot;)))    print(sess.run(tf.get_collection(&quot;x&quot;, scope=&quot;scope_y&quot;)))  # 只返回这个scope下的值</code></pre><p>输出如下：</p><pre><code class="lang-python"># --------------------------- x_ --------------------------------[22.296236  89.52586   94.95532   82.79374    4.5324626 99.39854  54.841103  50.33739   95.65411  95.163025 ]# ------------------------ collection ---------------------------[    array([22.296236 , 89.52586  , 94.95532  , 82.79374  ,  4.5324626,       99.39854  , 54.841103 , 50.33739  , 95.65411  , 95.163025 ], dtype=float32),     array([2.2296236 , 8.952586  , 9.495532  , 8.279374  , 0.45324627,       9.939854  , 5.4841104 , 5.033739  , 9.565412  , 9.516302  ], dtype=float32),    array([19.762728, 97.20074 , 35.839146, 82.53057 , 63.590633], dtype=float32)]# ----------------------- name is not exists ---------------------[]# ----------------------- scope is used --------------------------[array([19.762728, 97.20074 , 35.839146, 82.53057 , 63.590633], dtype=float32)]</code></pre>]]></content>
    
    
    <categories>
      
      <category>tensorflow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tensorflow</tag>
      
      <tag>Queue</tag>
      
      <tag>Coordinator</tag>
      
      <tag>Dataset</tag>
      
      <tag>collection</tag>
      
      <tag>共享变量</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-详解CNN网络和TensorFlow实现</title>
    <link href="/2018/07/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3CNN%E7%BD%91%E7%BB%9C%E5%8F%8ATensorFlow%E5%AE%9E%E7%8E%B0/"/>
    <url>/2018/07/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3CNN%E7%BD%91%E7%BB%9C%E5%8F%8ATensorFlow%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-卷积神经网络概述"><a href="#1-卷积神经网络概述" class="headerlink" title="1. 卷积神经网络概述"></a>1. 卷积神经网络概述</h2><p>卷积神经网络（Convolutional Neural Network, CNN） 也是神经网络中的一种结构，主要针对图像，音频，视频等方面应用。卷积神经网络较早的架构是<code>LeNet</code>，被认为是CNN的开端，当时主要将<code>LeNet</code>应用于字符识别任务，如手写数字的识别等。下面是一个<code>LeNet</code>的网络结构：</p><p><img src="\uploads\LeNet-2.jpg" srcset="/img/loading.gif" alt="LeNet网络结构"></p><h3 id="1-1-CNN的一般结构"><a href="#1-1-CNN的一般结构" class="headerlink" title="1.1 CNN的一般结构"></a>1.1 CNN的一般结构</h3><p>从上面<code>LeNet</code>的结构中基本可以看出，CNN的大体结构：<strong>卷积层</strong>(一个或多个，<strong>包括<code>ReLU</code></strong>操作等)，<strong>池化或者亚采样</strong>，<strong>分类</strong>(全连接层)几个部分组成。每个部分当然有很多细节的地方。比如卷积是如何计算，卷积核的定义等；池化操作的方式，计算方式等。但CNN模型一般都会存在这几个组件。下面对一些名词进行解释，便于后面的理解：</p><blockquote><p><strong>通道：</strong>常用于表示图像的某种组成。一个标准数字相机拍摄的图像会有三通道 - 红、绿和蓝；你可以把它们看作是互相堆叠在一起的二维矩阵（每一个通道代表一个颜色），每个通道的像素值在 0 到 255 的范围内。</p><p><a href="https://en.wikipedia.org/wiki/Grayscale" target="_blank" rel="noopener">灰度</a>图像，仅仅只有一个通道。在本篇文章中，我们仅考虑灰度图像，这样我们就只有一个二维的矩阵来表示图像。矩阵中各个像素的值在 0 到 255 的范围内——零表示黑色，255 表示白色。 </p></blockquote><h2 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2.卷积层"></a>2.卷积层</h2><h3 id="2-1-卷积的数学定义"><a href="#2-1-卷积的数学定义" class="headerlink" title="2.1 卷积的数学定义"></a>2.1 卷积的数学定义</h3><p>卷积神经网络中很重要的就是卷积的概念，<a href="https://zh.wikipedia.org/zh/%E5%8D%B7%E7%A7%AF" target="_blank" rel="noopener">卷积</a>本身是在泛函分析中的定义：<strong>卷积、旋积或摺积(Convolution)是通过两个函数f 和g 生成第三个函数的一种数学算子，表征函数f 与g经过翻转和平移的重叠部分的面积</strong>。 </p><p>从定义可以看出，卷积是一种运算，因为从定义上得出的面积，基本可以看出可以通过积分来定义，这个积分就定义 了一个新的函数,如下，就不用数学公式来详细展开了，具体可以参考<a href="https://zh.wikipedia.org/zh/%E5%8D%B7%E7%A7%AF" target="_blank" rel="noopener">维基百科</a>上的定义（如：连续定义(积分)和离散定义(求和)）。</p><script type="math/tex; mode=display">h(x)=f(x)*g(x)=(f*g)(x)</script><h3 id="2-2-卷积的物理意义"><a href="#2-2-卷积的物理意义" class="headerlink" title="2.2 卷积的物理意义"></a>2.2 卷积的物理意义</h3><p>从卷积的定义上可以看出，卷积主要是进行反转，平移，加权求和(积分)；从定义上比较抽象，不易理解，从物理意义上进行解释便于理解，那么卷积计算的意义是什么呢？</p><ol><li>平滑：卷积运算是一种线性运算，比如在图像中，将某个像素点用周围所有点进行加权求和值代替。</li><li>消除”噪声”：在1中的平滑和噪声消除相似，因为卷积本身就是进行加权求和，因此卷积后的值会过滤部分”噪声”。</li><li>空间显著性：卷积是对某一部分值进行操作，体现了空间的特性，能对空间中的特征进行增强。</li><li>空间变换：卷积在空间上有显著性，但是卷积本身是反转平移等操作，相当于进行了空间变换；比如某个物体，无论在空间的某个位置，卷积操作都可以将其特征抽取出来。</li></ol><p>卷积在其他领域也有很多应用，比如频谱分析，信号处理等；也有很多其他物理意义，在这儿只是列举了部分具有代表性的物理意义。<a id="more"></a></p><h3 id="2-3-卷积的实现"><a href="#2-3-卷积的实现" class="headerlink" title="2.3 卷积的实现"></a>2.3 卷积的实现</h3><p>首先看下卷积的计算过程，根据上面卷积的定义，来展示下卷积的计算过程是怎么样的，假设现在有一个$3 <em>4$的矩阵$f$和$3</em>3$的矩$g$，卷积核一般为奇数：</p><script type="math/tex; mode=display">f=\left[\begin{array}{ccc}0 & 1 & 2 & 3 \\2 & 3 & 4 & 5\\4 & 3 & 2 & 1\end{array}\right], g=\left[\begin{array}{ccc}-1 & 0 & 1 \\0 & 1 & 0 \\-1 & 0 & 1\end{array}\right]</script><p>第一步是将$g$进行翻转180度，得到：</p><script type="math/tex; mode=display">g=\left[\begin{array}{ccc}1 & 0  & -1 \\0 & 1 & 0 \\1 & 0 & -1\end{array}\right]</script><p>再使用$g$的中心和f的每个元素对齐，并对应元素相乘，在边缘外的元素用0来填充（在实际使用中不一定用0填充，也可以用边缘拷贝的方式）：</p><p><img src="\uploads\conv_dym.png" srcset="/img/loading.gif" alt="卷积计算过程"></p><p>最后得到了一个卷积后的矩阵：</p><script type="math/tex; mode=display">h=\left[\begin{array}{ccc}-3 &- 1 & 0 & 7 \\-1 & 3 & 4 & 9\\1 & 5 & 1 & 5\end{array}\right]</script><p>整个计算过程如上所示。上面的计算过程为一般的计算过程，对于细节还有其他方式，比如存在边际效应，完全重叠计算等等，在<code>python</code>的<code>numpy</code>和<code>sicpy</code>中就实现了卷积运算，可以直接进行使用：</p><pre><code class="lang-python">import numpy as npfrom scipy.signal import convolve2dx = np.random.randint(0, 10, (4, 4))g = np.array([[1, 0, -1], [0, 1, 0], [1, 0, -1]])h1 = np.convolve(x.ravel(), g.ravel(), mode=&#39;full&#39;)print(h1)h_same = convolve2d(x, g, mode=&#39;same&#39;)print(h_same)</code></pre><p><code>numpy</code>中的<code>convolve</code>只是支持<code>1-dim</code>的数据。但<code>scipy</code>中实现了<code>2-dim</code>的卷积运算。在这两个模块中卷积运算都有三种模式，即<code>mode</code> 选项：<code>{&#39;full&#39;, &#39;valid&#39;, &#39;same&#39;}</code>，不同的模式得到卷积预算的结果不同，主要是对边缘的处理方式不同。</p><h3 id="2-4-CNN中的卷积"><a href="#2-4-CNN中的卷积" class="headerlink" title="2.4 CNN中的卷积"></a>2.4 CNN中的卷积</h3><p>在上面我们使用的卷积运算中使用的卷积核($g$)，经过了翻转再进行计算。在CNN网络中的卷积和上面的卷积基本相似，但是使用的算子(<strong>相关核</strong>-<code>Kernel</code>，也称为<strong>相关算子</strong>)不会进行翻转（当然也可以翻转，翻转的算子叫<strong>卷积核</strong>或<strong>卷积算子</strong>），当然也可以使用卷积核。下面是其他文献中的说法：</p><blockquote><p>对图像大矩阵和滤波小矩阵对应位置元素相乘再求和的操作叫<strong>卷积</strong>(<code>Convolution</code>)或<strong>协相关</strong>(<code>Correlation</code>).协相关(<code>Correlation</code>)和卷积(<code>Convolution</code>)很类似, 两者唯一的差别就是卷积在计算前需要翻转<strong>卷积核</strong>, 而<strong>协相关</strong>则不需要翻转.</p></blockquote><p>相关核或卷积核一般都为一个方正，如3*3的矩阵，5*5的矩阵，方正中每个元素都可以看成一个权重系数；使用卷积进行计算时，需要将卷积核的中心放置在要计算的像素上，一次计算核中每个元素和其覆盖的图像像素值的乘积并求和，得到的结构就是该位置的新像素值。下面主要演示了相关核（相关算子）的卷积计算过程：</p><p><img src="\uploads\conv_dym.gif" srcset="/img/loading.gif" alt=""></p><p>步骤：</p><p>(1).滑动核，使得核中心位置位于要计算的像素点上</p><p>(2).重叠的位置进行对于元素乘积并求和，得到该像素点新的像素值</p><p>(3).重复上面的过程，输出整个图的所有新的像素值</p><p><img src="\uploads\center.png" srcset="/img/loading.gif" alt=""></p><p>上面的步骤也可类似看作：隐藏层中的<strong>神经元</strong> 具有一个<strong>固定大小</strong>的<strong>感受视野</strong>去感受上一层的<strong>部分特征</strong>。在全连接神经网络中，隐藏层中的神经元的感受视野足够大乃至可以看到上一层的所有特征。 而在CNN（卷积神经网络）中，隐藏层中的神经元的感受视野比较小，只能看到<strong>上一次的部分特征</strong>，上一层的其他特征可以通过平移<strong>感受视野</strong>来得到同一层的其他神经元，由同一层其他神经元来看。</p><h3 id="2-5-常用的卷积核"><a href="#2-5-常用的卷积核" class="headerlink" title="2.5 常用的卷积核"></a>2.5 常用的卷积核</h3><ol><li>低通滤波器（常用于计算模糊后的效果）</li></ol><script type="math/tex; mode=display">\begin{bmatrix}     1/9 & 1/9  & 1/9  \\     1/9  & 1/9  & 1/9   \\     1/9  & 1/9  & 1/9 \end{bmatrix},\begin{bmatrix}     1/10 & 1/10  & 1/10 \\     1/10  & 2/10  & 1/10   \\     1/10  & 1/10  & 1/10 \end{bmatrix},\begin{bmatrix}     1/16 & 2/16  & 1/16 \\     2/16  & 4/16  & 2/16   \\     1/16  & 2/16  & 1/16 \end{bmatrix}</script><ol><li><p>高斯滤波器（常用于计算高斯模糊后的效果）        高斯模糊的卷积核也是一个正方形的滤波核，其中每个元素通过以下公式计算得出：  </p><script type="math/tex; mode=display">G(x,y)=\frac{1}{2πσ^{2}}·e^{\frac{x^{2}+y^{2}}{2σ^{2}}}</script><p>该公式中$\sigma$是标准方差（一般取值为1），$x$和$y$分别对应了当前位置到卷积核中心的整数距离。通过这个公式，就可以计算出高斯核中每个位置对应的值。为了保证滤波后的图像不会变暗，需要对高斯核中的权重进行归一化。 </p></li><li><p>边缘检测（常用于计算图像边缘或者说梯度值） ：</p></li></ol><script type="math/tex; mode=display">\begin{bmatrix}     -1 & 0  & -1 \\     0  & 4  & 0   \\     -1  & 0  & -1\end{bmatrix}</script><h3 id="2-6-TensorFlow中卷积的实现"><a href="#2-6-TensorFlow中卷积的实现" class="headerlink" title="2.6 TensorFlow中卷积的实现"></a>2.6 <code>TensorFlow</code>中卷积的实现</h3><p>在<code>TensorFlow</code>中实现了卷积运算，本文中主要针对2维卷积运算说明，<code>conv2d</code>和<code>depthwise_conv2d</code>这两个实现，首先对conv2d进行说明。</p><h4 id="2-6-1-conv2d"><a href="#2-6-1-conv2d" class="headerlink" title="2.6.1 conv2d"></a>2.6.1 conv2d</h4><p><code>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</code>，下面对<strong>参数</strong>进行说明：</p><p><code>input</code>：需要卷积运算的<code>tensor</code>，输入的<code>shape</code>为[<code>batch_size, height, width, n_channels</code>]，即矩阵意义[批次的样本数，行，列，通道数]，一般都是用于图片。这是一个4维的<code>Tensor</code>，要求的类型为<code>float32</code>或<code>float64</code>。</p><p><code>filter</code>：相当于CNN中的卷积核，它要求是一个<code>Tensor</code>，具有<code>[filter_height, filter_width, in_channels, out_channels</code>这样的<code>shape</code>，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数<code>input</code>相同，有一个地方需要注意，第三维<code>in_channel</code>s，就是参数<code>input</code>的第四维 </p><p><code>strides</code>：卷积时在图像每一维的步长，这是一个一维的向量，长度4 </p><p><code>padding</code>：<code>string</code>类型的量，只能是<code>SAME</code>, <code>VALID</code>其中之一，这个值决定了不同的卷积方式，这个和<code>scipy</code>和<code>numpy</code>中的<code>mode</code>类似。</p><p><code>use_cudnn_on_gpu</code>：是否使用cudnn加速，默认为<code>true</code>。</p><p>卷积运算后会输出一个Tensor，也就CNN中的<code>feature map</code>。</p><p>下面是在<code>tensorflow</code>中卷积运算的示例：</p><pre><code class="lang-python">import tensorflow as tfimport numpy as npx = tf.Variable(initial_value=tf.random_normal(shape=[100, 28, 28, 3]), dtype=tf.float32)filter_kernel = tf.constant(np.random.randint(0, 2, [3, 3, 3, 5]), dtype=tf.float32)conv_tensor_same = tf.nn.conv2d(x, filter_kernel, [1, 1, 1, 1], padding=&#39;SAME&#39;)conv_tensor_valid = tf.nn.conv2d(x, filter_kernel, [1, 1, 1, 1], padding=&#39;VALID&#39;)print(conv_tensor_same)  # 输出和输入的矩阵大小一致,通道数为卷积核的输出通道数,batch_size不变print(conv_tensor_valid)  # 输出和输入的矩阵大小不一致,通道数为卷积核的输出通道数,batch_size不变# ----------------- output -------------------Tensor(&quot;Conv2D:0&quot;, shape=(100, 28, 28, 5), dtype=float32)Tensor(&quot;Conv2D_1:0&quot;, shape=(100, 26, 26, 5), dtype=float32)</code></pre><p>上面的例子中输入为3通道的28<em>28的100张图片，经过卷积核（3\</em>3的卷积核，输出5通道）后，得到了每张图（100张）都有5张28*28或26*26的<code>feature map</code>。当<code>pading</code>参数不同时返回<code>feature map</code>的大小不同，下面动态展示：</p><ul><li><p><code>padding=’SAME’</code>时，<code>TensorFlow</code>会自动对原图像进行补零,从而使输入输出的图像的<a href="https://www.tensorflow.org/api_guides/python/nn#convolution" target="_blank" rel="noopener">高度和宽度计算</a>如下：</p><p><img src="\uploads\conv_same.gif" srcset="/img/loading.gif" alt="SAME"></p></li></ul><pre><code class="lang-python">out_height = ceil(float(in_height) / float(strides[1]))out_width  = ceil(float(in_width) / float(strides[2]))</code></pre><ul><li><code>padding=&#39;VAILD&#39;</code>时，则会缩小原图像的大小 ，缩小后的图像即为<code>feature map</code>，其<code>size</code>为<code>(W – F + 1) / S</code> ，<strong>结果向上取整</strong>，其中<code>W</code>为输入矩阵的<code>width</code>，<code>F</code>为卷积核的大小，<code>S</code>为步长，按输出图像的长度和宽度分阶后：</li></ul><pre><code class="lang-python">out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))</code></pre><p>  <img src="\uploads\conv_vaild.gif" srcset="/img/loading.gif" alt="VAILD"></p><p>上面的两个公式也可以通过一个公式代替：</p><script type="math/tex; mode=display">W_1 = (W - F + 2P)/S + 1</script><p>其中$W$为输入图像的宽度，$F$为卷积核的宽度，$P$为<code>padding size</code>，<code>padding size</code>的计算方式$P=\frac {F-1} 2$，当padding为<code>vaild</code>时$P=0$，$S$为在当前维度下移动的步长。</p><h4 id="2-6-2-depthwise-conv2d"><a href="#2-6-2-depthwise-conv2d" class="headerlink" title="2.6.2 depthwise_conv2d"></a>2.6.2 depthwise_conv2d</h4><p><code>depthwise_conv2d(input,  filter, strides, padding, rate=None, name=None, data_format=None)</code>，参数说明如下：</p><ol><li>input 的数据维度 <code>[batch ,in_height,in_wight,in_channels]</code></li><li>卷积核的维度是<code>[filter_height,filter_heught,in_channel,channel_multiplierl]</code></li><li>卷积核<code>filter</code>和独立的应用在<code>in_channels</code>的每一个通道上(从通道 1 到通道<code>channel_multiplier</code>)</li><li>然后将所有结果进行汇总,输出通道的总数是，<code>in_channel * channel_multiplier</code></li></ol><pre><code class="lang-python">input_data = tf.Variable(np.random.rand(10, 9, 9, 3), dtype=np.float32)filter_data = tf.Variable(np.random.rand(2, 2, 3, 2), dtype=np.float32)y = tf.nn.depthwise_conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)print(&#39;tf.nn.depthwise_conv2d : &#39;, y)# -------------- output --------------# tf.nn.depthwise_conv2d :  Tensor(&quot;depthwise:0&quot;, shape=(10, 9, 9, 6), dtype=float32)</code></pre><p>其效果类似于多个卷积核运算都是张量的一个维度增加，不同之处在于通道数的增加是卷积核在不同通道上运算的结果，而多个卷积核运算（<code>conv2d</code>运算操作）相当于是<code>batch</code>的数量增加 。</p><h3 id="2-7-CNN中的Feature-Map"><a href="#2-7-CNN中的Feature-Map" class="headerlink" title="2.7 CNN中的Feature Map"></a>2.7 CNN中的<code>Feature Map</code></h3><p>卷积的主要作用就是生成<code>feature map</code>，从图像中提取特征图，特征图主要是根据卷积核来生成，不同的卷积核生成不同的特征图。</p><p>前面说了卷积的物理意义，卷积在空间上对特征进行了提取，同时也是对局<strong>部视野的感受</strong>，卷积的大小相当于感受野的大小，这个是其他网络无法达到的。因此<code>feature map</code>是CNN中非常重要的一环。</p><h4 id="2-7-1-feature-map的大小"><a href="#2-7-1-feature-map的大小" class="headerlink" title="2.7.1 feature map的大小"></a>2.7.1 <code>feature map</code>的大小</h4><p>因为<code>feature map</code>是由卷积生成的，一个卷积核生成一个<code>feature map</code>,因此卷积层输出的大小就是<code>feature map</code>的大小，具体大小的计算方式主要是基于上层输入的大小，<code>feature map</code>大小和步长有关系。具体大小的计算公式在2.6中已经进行了说明。</p><h4 id="2-7-2-feature-map的数量"><a href="#2-7-2-feature-map的数量" class="headerlink" title="2.7.2 feature map的数量"></a>2.7.2 <code>feature map</code>的数量</h4><p><code>feature map</code>的数量和卷积核的数量相关，在<code>tensorflow</code>中的卷积运算时，比如输入的input为1*32*32*3（3为通道数，也可以理解为feature map数），卷积核为5*5*3*5的大小，其中最后一个5就是卷积核数量，即输出的通道数，生成的feature map数量。下面通过一个动态图来展示整个过程：</p><hr><p><a href="http://cs231n.github.io/\uploads\conv-demo/index.html" target="_blank" rel="noopener">conv feature map demo</a></p><hr><h4 id="2-7-3-feature-map的计算"><a href="#2-7-3-feature-map的计算" class="headerlink" title="2.7.3 feature map的计算"></a>2.7.3 <code>feature map</code>的计算</h4><p>其实<code>Feature Map</code>的计算过程就是卷积计算的过程，下图中是对其进行了可视化，输入的为1*32*32*3的图像，卷积核为3*3*3*5的大小，对输入的<strong>局部（一个局部）</strong>进行计算，如下的右图所示。其中<strong>$w_i$</strong>就是其中一个卷积核的的某个值，这个值会通过模型训练进行<strong>训练</strong>。</p><p><img src="\uploads\neuron_model0.jpeg" srcset="/img/loading.gif" style="zoom:60%" align = center />              |               <img src="\uploads\neuron_model.jpeg" srcset="/img/loading.gif" style="zoom:60%" align=center /> </p><h3 id="2-8-卷积的训练参数"><a href="#2-8-卷积的训练参数" class="headerlink" title="2.8 卷积的训练参数"></a>2.8 卷积的训练参数</h3><p> 在卷积层的训练主要是对卷积核的训练，上面我们已经明确了在卷积运算时是怎样计算的，下面就看在训练过程中的参数问题，首先就必须了解参数的大小。</p><p>如果是使用全连接网络，再输入一个100<em>100的图像，假设有1000个隐藏神经元，那么就会有100\</em>100*1000个连接，$10^7$个参数需要进行训练，如果我们使用卷积的方式，采用10*10的卷积，那么参数就是100*100*10*10，数量级减少了一个，但是这个数量级的参数还是很多。有什么其他办法么？那就是权值共享。</p><h4 id="2-8-1-权值共享"><a href="#2-8-1-权值共享" class="headerlink" title="2.8.1 权值共享"></a>2.8.1 权值共享</h4><p>我们从卷积计算过程就知道，每次会从图像的区域取和卷积核同样大小的区域来进行运算。也就是每个<strong>神经元（feature map中的元素）</strong>会连接一个10*10的区域，这样就会有100个参数，如果每个神经元都使用同一组参数，那么参数还是100个。因此这样就是权值共享。</p><p>在进行通俗的理解：在一张图片上，通过一个卷积核进行扫描所有点时，使用的相同的卷积核，卷积核的值就是权重。这样权重就是共享的。扫描整个图片的参数也就只是卷积核的大小。</p><h4 id="2-8-2-多通道多卷积核"><a href="#2-8-2-多通道多卷积核" class="headerlink" title="2.8.2 多通道多卷积核"></a>2.8.2 多通道多卷积核</h4><p>当为多通道进行多个卷积计算时，每个通道上面对应一个卷积核。假设有一个四通道的图片，利用两个卷积核进行运算，最后生成两通道，即得到两个<code>feature map</code>。如下所示：</p><p><img src="\uploads\mul-input-mul-k.png" srcset="/img/loading.gif" alt=""></p><p>四个通道上每个通道对应一个卷积核，先将$w_2$忽略，只看$w_1$，那么在$w_1$的某位置（<code>i,j</code>）处的值，是由四个通道上（<code>i,j</code>）处的卷积结果相加然后再取激活函数值得到的。  所以最后得到两个<code>feature map</code>， 即输出层的卷积核核个数为 <code>feature map</code> 的个数。其计算公式：</p><script type="math/tex; mode=display">h_{ij}^k = f((w^k * x)_{ij} + b^k)</script><p>上面的参数个数就变为：$2 <em> 2 </em> 4 * 2 $，前面的两个2指卷积核大小，4为输入的通道数，最后的2为输出的通道数。因此多通道多核的运算核单通道多核运算不同，参数个数量级差别也很大。下面这个可视化也展示了多通道多核的计算过程：</p><p><img src="\uploads\mul-input-mul-conv.gif" srcset="/img/loading.gif" alt=""></p><h2 id="3-非线性操作（ReLU）"><a href="#3-非线性操作（ReLU）" class="headerlink" title="3.非线性操作（ReLU）"></a>3.非线性操作（<code>ReLU</code>）</h2><p>一般再卷积后都会有一个非线性的操作，我们前面也提到过卷积其实是线性的操作，而对于一般的分类来说，线程分类不能满足，因此需要再卷积中加入非线性。这样才会进行更好的分类。因此这一步也是非常关键的。</p><p>在使用非线性操作函数时，<code>ReLU</code>，<code>sigmoid</code>，<code>tanh</code>等都是可以的，但是根据历史经验，大部分情况下使用<code>ReLU</code>的效果会更好。</p><h2 id="4-池化层（Pooling-Layer）"><a href="#4-池化层（Pooling-Layer）" class="headerlink" title="4.池化层（Pooling Layer）"></a>4.池化层（Pooling Layer）</h2><p>Pooling也称为下采样或亚采样，主要对上面过程输出的特征图进行采样，降低特征图的维度，但是可以保持部分重要的信息；池化<strong>规模</strong>一般为<strong>2×2</strong> （也可以使用其他的大小），在feature map上<strong>不重合的平移*2×2</strong>大小区域，并进行采用；空间池化的主要方式有以下几种。</p><p><img src="\uploads\pooling.jpg" srcset="/img/loading.gif" style="zoom:40%" \></p><h4 id="4-1-1-池化方法"><a href="#4-1-1-池化方法" class="headerlink" title="4.1.1 池化方法"></a>4.1.1 池化方法</h4><ol><li>最大化（<code>max pooling</code>）：取4个点的最大值 </li><li>平均化（<code>average pooling</code>）：取4个点的平均值</li><li>高斯池化（<code>gauss pooling</code>）：类似高斯模糊，这个不太常用</li><li>训练池化（<code>train pooling</code>）：也可以根据四个值训练一个函数$f$得到一个值，不常用</li></ol><p>由于特征图不一定是2的倍数，因此池化操作在边缘处理上也有两种方式：</p><ul><li>忽略边缘：多出来的边缘直接去掉</li><li>保留边缘：即将特征图的变长用0填充为2的倍数，然后再池化。 </li></ul><h4 id="4-1-1-池化的作用"><a href="#4-1-1-池化的作用" class="headerlink" title="4.1.1 池化的作用"></a>4.1.1 池化的作用</h4><p>池化在CNN中也是一个不可缺少的部分，那池化操作的意义是什么呢？下面对池化的主要作用或意义进行一一说明，加深对池化的理解：</p><ol><li>对特征的概要统计，对特征图<code>(feature map)</code>进行简要的统计，对特征的进一步抽象。</li><li><a href="https://www.zhihu.com/question/36686900" target="_blank" rel="noopener">不变性</a>：这个是非常重要的作用，这种不变性包括了平移，旋转，尺度等；因为池化过程丢掉了局部信息（允许局部有一些形变），并统计了局部的信息（不论局部在什么位置，只要存在这个统计信息），因此对于平移或位移都能识别。</li><li>输出控制，比如控制输出的长度，大小；文本分类中的长度是不同的，可以通过池化来获得特定的长度。</li><li>降维，通过池化后会进一步降低维度（相比不进行池化操作）</li><li>防止过拟合，因为池化过程中抛弃了部分的特征，而且对特征进行了概要统计，因此对模型结果会进行改善，防止了过拟合的情况。</li></ol><h2 id="5-FC层（Full-Connect-Layer）"><a href="#5-FC层（Full-Connect-Layer）" class="headerlink" title="5.FC层（Full Connect Layer）"></a>5.FC层（Full Connect Layer）</h2><p>全连接层是CNN中最后一个步骤（当然这个步骤也不是必须的），在这层主要是解决分类任务，将输出的<code>feature map</code>进行向量化，形成一个列向量，并乘以每个值得权重。比如输出了一个3通道得<code>10*10</code>的<code>feature map</code>那么就输出一个300长度的列向量。对这个列向量进行线性组合，输出到分类器其中进行分类，如<code>softmax</code>分类。</p><p>这一层的主要作用还是将<code>feature map</code>的特征进行线性组合，对目标进行分类；因为只是用部分的特征，可能对分类效果不太好，因此需要将所有的特征组合在一起来进行目标分类。</p><h2 id="6-CNN中的其他功能层"><a href="#6-CNN中的其他功能层" class="headerlink" title="6.CNN中的其他功能层"></a>6.CNN中的其他功能层</h2><p>一般的CNN网络基本都会存在上面的卷积层，激励层(非线性操作)，池化层，全连接层；但是也可以在其他加入一些其他层，如归一化层等。下面对一些其他层进行简要说明。</p><h3 id="6-1-归一化层"><a href="#6-1-归一化层" class="headerlink" title="6.1 归一化层"></a>6.1 归一化层</h3><h4 id="6-1-1-批量归一化（Batch-Normalization）"><a href="#6-1-1-批量归一化（Batch-Normalization）" class="headerlink" title="6.1.1 批量归一化（Batch Normalization）"></a>6.1.1 批量归一化（<code>Batch Normalization</code>）</h4><p>在卷积层输出后，每个<code>feature map</code>的特征<code>scale</code>可能不一致。经过归一化（<code>Batch Normalization</code>，<code>BN</code>）后可以加速训练，提高精度。BN算法的过程：</p><p><strong>输入</strong>：$x$的mini-batch：$B=(x_1, x_2, x_3, ……, x_m)$</p><p><strong>计算</strong>：</p><ol><li>计算mini-batch的均值：$\mu_b = \frac {1} {m} \sum_{i=1}^m x_i $</li><li>计算mini-batch的方差：$\sigma ^ 2_b = \frac {1} {m} \sum_{i=1}^m (x_i - \mu_b)^2$</li><li>归一化：$\bar{x_i} = \frac {x_i - \mu_b}{\sqrt{\sigma ^ 2_b + \epsilon }}$，获得0-1分布，其中$\epsilon$是为了避免除数为0时所使用的微小正数。 </li><li>尺度变化及偏移：$y_i = \gamma \bar{x_i} + \beta$，这两个参数需要通过学习得到，这儿个$\gamma$为尺度因子，$\beta$为平移因子</li></ol><p><strong>输出</strong>：规范化后的$y_i$</p><h4 id="6-1-2-CNN中的批归一化（BN）"><a href="#6-1-2-CNN中的批归一化（BN）" class="headerlink" title="6.1.2 CNN中的批归一化（BN）"></a>6.1.2 CNN中的批归一化（BN）</h4><p>根据前面对CNN的了解，CNN中卷积输出的是很多特征图，比如卷积层输出的是<code>100*28*28*5</code>的大小，其中100为<code>batch size</code>，5为特征图的个数，那么如果将特征图中的每个神经元进行BN，那么就会有<code>2*28*28*5</code>个$\gamma,\beta$参数，这样计算参数就太多了。因此在卷积后进行BN使用类似权值共享的策略，即把一整张<code>feature map</code>当作一个神经元进行处理，相当于将<code>100*28*28</code>作为BN的输入，即一个<code>feature map</code>中的神经元都使用同一组$\gamma,\beta$参数，这样计算，$\gamma,\beta$参数就减小到：<code>5*2</code>，比之前小了很多的数量级。</p><ul><li><strong>BN的训练和测试（预测）</strong></li></ul><p>在训练阶段，BN的输入为<code>batch_size</code>的大小，因此可以算出所有计算均值和方差时；但是在测试或预测阶段可能输入的一个样本，那么均值和方差均为0，因此需要在训练<code>batch_num</code>次的过程中将均值和方差给记录下来，在预测或测试时使用训练过程中的均值平均值和方差平均值来进行BN。</p><ul><li><strong>BN在CNN中的位置</strong></li></ul><p>一般BN层放入到卷积层后，未进行非线性操作（激励函数）之前。输出后作为非线性操作层的输入。</p><h4 id="6-1-3-BN解决的问题"><a href="#6-1-3-BN解决的问题" class="headerlink" title="6.1.3 BN解决的问题"></a>6.1.3 BN解决的问题</h4><p><img src="\uploads\normazier.png" srcset="/img/loading.gif" alt=""></p><p><img src="\uploads\batch_normal.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>a中左图是没有经过任何处理的输入数据，曲线是sigmoid函数，如果数据在梯度很小的区域，那么学习率就会很慢甚至陷入长时间的停滞。减均值除方差后，数据就被移到中心区域如右图所示，对于大多数激活函数而言，这个区域的梯度都是最大的或者是有梯度的（比如ReLU），这可以看做是一种对抗梯度消失的有效手段。对于一层如此，如果对于每一层数据都那么做的话，数据的分布总是在随着变化敏感的区域，相当于不用考虑数据分布变化了，这样训练起来更有效率。</p><p>那么为什么要有第4步，不是仅使用减均值除方差操作就能获得目的效果吗？我们思考一个问题，减均值除方差得到的分布是正态分布，我们能否认为正态分布就是最好或最能体现我们训练样本的特征分布呢？不能，比如数据本身就很不对称，或者激活函数未必是对方差为1的数据最好的效果，比如Sigmoid激活函数，在-1~1之间的梯度变化不大，那么非线性变换的作用就不能很好的体现，换言之就是，减均值除方差操作后可能会削弱网络的性能！针对该情况，在前面三步之后加入第4步完成真正的batch normalization。</p><p>BN的本质就是利用优化变一下方差大小和均值位置，使得新的分布更切合数据的真实分布，保证模型的非线性表达能力。BN的极端的情况就是这两个参数等于mini-batch的均值和方差，那么经过batch normalization之后的数据和输入完全一样，当然一般的情况是不同的。</p></blockquote><h3 id="6-2-切分层"><a href="#6-2-切分层" class="headerlink" title="6.2 切分层"></a>6.2 切分层</h3><p>在一些应用中,需要对图片进行切割，独立地对某一部分区域进行单独学习。这样可以对特定部分进行通过调整 <strong>感受视野</strong>进行力度更大的学习。 </p><h3 id="6-3-融合层"><a href="#6-3-融合层" class="headerlink" title="6.3 融合层"></a>6.3 融合层</h3><p>对独立进行特征学习的分支进行融合，来构建高效而精简的特征组合。 融合层可以对切分层进行融合，也可以对不同大小的卷积核学习到的特征进行融合。 </p><p><img src="\uploads\3.jpg" srcset="/img/loading.gif" alt=""></p><p>可以用 <strong>级连(concatenation)</strong> 的方法，其实也就是不同输入网络特征的简单叠加，比如说首尾相接。 也可以是合并，或者说运算的融合，对形状一致的特征，通过 <code>+, -, x, max, conv</code> 等运算，形成形状相同的输出 。</p><h2 id="7-在TensorFlow中实现CNN（MNIST-DataSets）"><a href="#7-在TensorFlow中实现CNN（MNIST-DataSets）" class="headerlink" title="7.在TensorFlow中实现CNN（MNIST DataSets）"></a>7.在TensorFlow中实现CNN（MNIST DataSets）</h2><pre><code class="lang-python">import tensorflow as tfimport tensorflow.examples.tutorials.mnist.input_data as input_datamnist = input_data.read_data_sets(&quot;/opt/workspace/project/deep-st-nn/data/MNIST_DATA&quot;, one_hot=True)  # 下载并加载mnist数据x = tf.placeholder(tf.float32, [None, 784])  # 输入的数据占位符y_actual = tf.placeholder(tf.float32, shape=[None, 10])  # 输入的标签占位符def weight_variable(shape):    &quot;&quot;&quot;    parameter    ---------        shape: list, create variable tensor for shape.    return    ------        tensor, the variable tensor of weight.    &quot;&quot;&quot;    initial = tf.truncated_normal(shape, stddev=0.1)    return tf.Variable(initial)def bias_variable(shape):    initial = tf.constant(0.1, shape=shape)    return tf.Variable(initial)def conv2d(x, W):    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)def max_pool(x):    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)# 构建网络x_image = tf.reshape(x, [-1, 28, 28, 1])  # 转换输入数据shape,以便于用于网络中W_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32])h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  # 第一个卷积层h_pool1 = max_pool(h_conv1)  # 第一个池化层W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  # 第二个卷积层h_pool2 = max_pool(h_conv2)  # 第二个池化层W_fc1 = weight_variable([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])  # reshape成向量h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  # 第一个全连接层keep_prob = tf.placeholder(&quot;float&quot;)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  # dropout层W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_predict = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  # softmax层cross_entropy = -tf.reduce_sum(y_actual * tf.log(y_predict))  # 交叉熵train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(cross_entropy)  # 梯度下降法correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_actual, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))  # 精确度计算sess = tf.InteractiveSession()sess.run(tf.initialize_all_variables())for i in range(20000):    batch = mnist.train.next_batch(50)    if i % 100 == 0:  # 训练100次，验证一次        train_acc = accuracy.eval(feed_dict={x: batch[0], y_actual: batch[1], keep_prob: 1.0})        print(&#39;step&#39;, i, &#39;training accuracy&#39;, train_acc)    train_step.run(feed_dict={x: batch[0], y_actual: batch[1], keep_prob: 0.5})test_acc = accuracy.eval(feed_dict={x: mnist.test.images, y_actual: mnist.test.labels, keep_prob: 1.0})print(&quot;test accuracy&quot;, test_acc)</code></pre><h2 id="8-Reference"><a href="#8-Reference" class="headerlink" title="8.Reference"></a>8.Reference</h2>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>tensorflow</tag>
      
      <tag>CNN</tag>
      
      <tag>ReLU</tag>
      
      <tag>convolution</tag>
      
      <tag>pooling</tag>
      
      <tag>BN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-详解LSTM网络和TensorFlow实现</title>
    <link href="/2018/06/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3LSTM%E7%BD%91%E7%BB%9C%E5%92%8CTensorFlow%E5%AE%9E%E7%8E%B0/"/>
    <url>/2018/06/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3LSTM%E7%BD%91%E7%BB%9C%E5%92%8CTensorFlow%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<p>LSTM网络是RNN网络中的特殊网络，再RNN文章中已经提到，RNN在时间步过长时，学习不到依赖关系。主要是RNN会引起梯度消失和梯度爆炸这两个问题，因此为了解决问题，研究者们提出了很多方式，其中GRU和LSTM网络就是这样诞生的。LSTM网络在应用中也取得了非凡的成就，特别是在语音识别，语言建模，翻译等等方面。</p><h2 id="1-长期依赖问题"><a href="#1-长期依赖问题" class="headerlink" title="1.长期依赖问题"></a>1.长期依赖问题</h2><p>RNN的核心就是能将历史的信息连接到当前的场景下，即RNN对历史是有记忆功能的，能对一定时间步的信息进行记忆。但是<code>time step</code>过长的时候，就会出现问题，对过久(时间跨度太大的信息)信息没有记忆。从理论的角度RNN是有这样的功能，在应用中却不尽人意。为了解决这种长期依赖的问题，研究者提出了新的RNN模型，如GRU，LSTM等网络，来解决这种长期依赖的问题。</p><h2 id="2-什么是LSTM网络"><a href="#2-什么是LSTM网络" class="headerlink" title="2.什么是LSTM网络"></a>2.什么是LSTM网络</h2><p>LSTM，全称为长短期记忆网络（Long Short Term Memory networks）,它也是一种特殊的RNN网络，但是可以学习到长期依赖的关系。那么LSTM是如何解决长期依赖的问题呢？</p><p>在RNN中我们也提到了，可以通过gate的方式来解决梯度消失和梯度爆炸的问题，而LSTM就是通过gate的方式来实现的。下面是LSTM的cell单元可视化结构。</p><p>下面是在整个时间序列上LSTM的整体结构，$X_t$表示不同时间点的输入序列，$h_t$为每个时间点的输出，从下面的结构图中可以看出LSTM网络中比RNN网络多了一个循环结构，从上面的结构中可以看出，<code>LSTM Cell</code>中多出了一个$C_t$的的变量，在LSTM中被称为记忆单元，记忆单元贯穿整个时间步，不会被输出，只会在循环过程中进行更新，并输出到下一时间步作为输入。$C_t$在每个cell中会进行简单的线性交互，上面承载了一些历史的输入信息。</p><p><img src="\uploads\LSTM.png" srcset="/img/loading.gif" style="zoom:30" /><br><a id="more"></a></p><h2 id="3-LSTM结构详解"><a href="#3-LSTM结构详解" class="headerlink" title="3.LSTM结构详解"></a>3.LSTM结构详解</h2><p>从上面中也提出了<code>LSTM Cell</code>中主要通过gate的方式在RNN基础上进行变换的。具体<code>LSTM Cell</code>的结构如下所示，主要通过三道<code>gate</code>（门）来控制输入，输出等，这个门来选择性的控制信息的是否通过。主要是通过sigmod神经网络层和一个元素的乘积实现门的控制。</p><p><img src="\uploads\lstm_cell.png" srcset="/img/loading.gif" style="zoom:50%" /></p><h3 id="3-1-LSTM分步详解"><a href="#3-1-LSTM分步详解" class="headerlink" title="3.1 LSTM分步详解"></a>3.1 LSTM分步详解</h3><p><code>LSTM Cell</code>的整体输入有：$X_t$，$h_{t-1}$，$c_{t-1}$，而整体输出和RNN的输出一致，主要是$h_t$，$z_t$；而$c_t$主要是在循环过程中使用。而<code>LSTM Cell</code>中最关键的就是<code>gate</code>实现，而<code>LSTM</code>的三道门主要作用是不一样的，分别为”遗忘门”，”输入门”，”输出门”；下面分别对这三道门进行分别详细说明。</p><h4 id="3-1-1-Forget-Gate"><a href="#3-1-1-Forget-Gate" class="headerlink" title="3.1.1 Forget Gate"></a>3.1.1 Forget Gate</h4><p>遗忘门主要对$h_{t-1}$和$x_t$进行观察，对记忆单元$C_{t-1}$的元素选择性的遗忘，遗忘门输出的是0-1的数，1表示完全保留该消息，0表示遗忘该消息。遗忘门输出一个$f_t$，公式为：</p><script type="math/tex; mode=display">f_t=\sigma(W_f \cdot [x_t, h_{t-1}] + b_f)</script><p>输出后的$f_t$再和$c_{t-1}$相乘，来更新记忆单元。在应用过程中的意义时，选择性的忘记一些历史信息(不是所有的历史信息都是有意义的)。</p><h4 id="3-1-2-Input-Gate"><a href="#3-1-2-Input-Gate" class="headerlink" title="3.1.2 Input Gate"></a>3.1.2 Input Gate</h4><p>这一步的主要作用是将旧的记忆单元$c_{t-1}$更新到新的记忆单元$c_t$上。这一步只需要将$c_{t-1}$乘以$f_t$在加上$i_t$*$c_{t}^-$即可。$i_t$则为输入门，可以理解为对本次的输入更新程度。，具体公式如下：</p><script type="math/tex; mode=display">i_t = \sigma(W_i \cdot [x_t, h_{t-1}] + b_i)\\c_t^-=tanh(W_c \cdot [x_t, h_{t-1})] + b_c)</script><p>$i_t$输出的为0-1的值，$c_{t}^-$的计算方式和RNN中的state更新方式一致，再乘以$i_t$后，表示对最新$c_t$的更新程度。这儿一步的意义是，添加了当前时间步的信息，随后添加到记忆单元中。这一步输出们计算完成后随后需要更新$C_t$：</p><script type="math/tex; mode=display">c_t = c_{t-1} * f_t + i_t*c_t^-</script><h4 id="3-1-3-Output-Gate"><a href="#3-1-3-Output-Gate" class="headerlink" title="3.1.3 Output Gate"></a>3.1.3 Output Gate</h4><p>最后需要决定最终的输出，输出会基于当前的信息，并且可能会进行一些过滤，并用于下一步和$C_t$的结合中，确定最终的输出值：</p><script type="math/tex; mode=display">o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\h_t = o_t * tanh(C_t)</script><p>最终的输出，结合当前的信息和记忆单元信息进行输出。</p><h3 id="3-2-LSTM网络结构可视化"><a href="#3-2-LSTM网络结构可视化" class="headerlink" title="3.2 LSTM网络结构可视化"></a>3.2 LSTM网络结构可视化</h3><p>根据LSTM Cell的结构，即内部的计算公式进行内部结构可视化，从输入，到计算，再到输出的过程进行详细的结构可视化，如下图所示：<img src="\uploads\LSTM_X.png" srcset="/img/loading.gif" alt="LSTM内部结构可视化"></p><p>上面将LSTM Cell进行了划分，主要划分为三个部分，第一部分主要是三道门和输入的转换；第二部分可以看作是记忆单元$C_t$的更新；第三部分就是输出部分。</p><p>在上面的结构中也给出了输入，输出的数据<code>shape</code>，明白数据在整个计算过程中是怎么流通和计算的。并可以根据输入的情况，知道参数的<code>shape</code>等。在计算过程中，三道门中的参数$W$的<code>shape</code>分为两种情况：</p><ul><li>当$X_t$和$H_{t-1}$进行<code>concat</code>后，参数<code>W</code>的<code>shape</code>为<code>(depth+n_hidden_units， n_hidden_units)</code>，参数$b$的<code>shape</code>为<code>(n_hidden_units)</code>。</li><li>当$X_t$和$H_{t-1}$分别和参数$W$进行运算，那么参数$W$就会存在两个，$W_x$的<code>shape</code>为<code>(depth, n_hidden_units)</code>，$W_h$的$shape$为<code>(n_hidden_units, n_hidden_units)</code>；参数$b$的<code>shape</code>为<code>(n_hidden_units)</code>。</li></ul><p>三个gate的输出分别为$f_t$，$i_t$，$o_t$；这三个值的输出<code>shape</code>均为<code>(batch_size, n_hidden_units)</code>。</p><h2 id="4-LSTM网络在TensorFlow中的实现"><a href="#4-LSTM网络在TensorFlow中的实现" class="headerlink" title="4.LSTM网络在TensorFlow中的实现"></a>4.LSTM网络在TensorFlow中的实现</h2><p>LSTM也是RNN网络中的一种，因此在TensorFlow实现时，和RNN模型实现的方式一致，唯一不同的地方在RNN中定义<code>cell</code>的地方。在TensorFlow中实现了<code>LSTM Cell</code>的基本结构，实现了两种<code>LSTM Cell</code>的结构<code>BasicLSTMCell</code>和<code>LSTMCell</code>，下面依次对两种结构进行说明。</p><h3 id="4-1-BasicLSTMCell实现"><a href="#4-1-BasicLSTMCell实现" class="headerlink" title="4.1 BasicLSTMCell实现"></a>4.1 <code>BasicLSTMCell</code>实现</h3><p>这个Cell的实现是以最基本的LSTM结构为基础，在TensorFlow中实现了该结构，也是基于<code>LayerRNNCell</code>实现的，主要实现了<code>build</code>和<code>call</code>方法，下面为<code>call</code>方法的源码：</p><pre><code class="lang-python">  def call(self, inputs, state):    &quot;&quot;&quot;Long short-term memory cell (LSTM).    Args:      inputs: `2-D` tensor with shape `[batch_size, input_size]`.      state: An `LSTMStateTuple` of state tensors, each shaped        `[batch_size, self.state_size]`, if `state_is_tuple` has been set to        `True`.  Otherwise, a `Tensor` shaped        `[batch_size, 2 * self.state_size]`.    Returns:      A pair containing the new hidden state, and the new state (either a        `LSTMStateTuple` or a concatenated state, depending on        `state_is_tuple`).    &quot;&quot;&quot;    sigmoid = math_ops.sigmoid    one = constant_op.constant(1, dtype=dtypes.int32)    # Parameters of gates are concatenated into one multiply for efficiency.    if self._state_is_tuple:      c, h = state    else:      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one)    gate_inputs = math_ops.matmul(        array_ops.concat([inputs, h], 1), self._kernel)    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)    # i = input_gate, j = new_input, f = forget_gate, o = output_gate    i, j, f, o = array_ops.split(        value=gate_inputs, num_or_size_splits=4, axis=one)    forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)    # Note that using `add` and `multiply` instead of `+` and `*` gives a    # performance improvement. So using those at the cost of readability.    add = math_ops.add    multiply = math_ops.multiply    new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),                multiply(sigmoid(i), self._activation(j)))    new_h = multiply(self._activation(new_c), sigmoid(o))    if self._state_is_tuple:      new_state = LSTMStateTuple(new_c, new_h)    else:      new_state = array_ops.concat([new_c, new_h], 1)    return new_h, new_state</code></pre><p>源码中主要对四个<code>gate</code>进行计算，并更新<code>C</code>和<code>H</code>，并返回输出和<code>state</code>，<code>LSTM</code>中的<code>state</code>包括了<code>C</code>和<code>H</code>。上面代码中对四个<code>gate</code>的计算，是一次性生成权重变量，再和输入的$X<em>t$和$H</em>{t-1}$进行运算，再拆分为四部分，权重变量的定义如下：</p><pre><code class="lang-python">def build(self, inputs_shape):    if inputs_shape[1].value is None:        raise ValueError(&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot;                         % inputs_shape)        input_depth = inputs_shape[1].value        h_depth = self._num_units        self._kernel = self.add_variable(            _WEIGHTS_VARIABLE_NAME,            shape=[input_depth + h_depth, 4 * self._num_units])        self._bias = self.add_variable(            _BIAS_VARIABLE_NAME,            shape=[4 * self._num_units],            initializer=init_ops.zeros_initializer(dtype=self.dtype))    self.built = True</code></pre><p>在定义<code>LSTM</code>中四道<code>gate</code>的权重变量<code>W</code>和<code>B</code>时，是一次性定义了4个，并在一个变量中。后续在计算时，做一次计算，再进行拆分成四道<code>gate</code>的输出：<code>i,j,f,o</code>。</p><h3 id="4-2-LSTMCell的实现"><a href="#4-2-LSTMCell的实现" class="headerlink" title="4.2 LSTMCell的实现"></a>4.2 <code>LSTMCell</code>的实现</h3><p><code>LSTMCell</code>是<code>BasicLSTMCell</code>扩展实现，增加了窥视的窥视孔的功能，<code>LSTMCell</code>的初始化参数如下所示，先对初始化的参数进行详细说明。</p><p><img src="\uploads\LSTM-diag.png" srcset="/img/loading.gif" alt=""></p><pre><code class="lang-python">def __init__(self, num_units, use_peepholes=False, cell_clip=None, initializer=None, num_proj=None,              proj_clip=None, num_unit_shards=None, num_proj_shards=None, forget_bias=1.0,             state_is_tuple=True, activation=None, reuse=None, name=None):    &quot;&quot;&quot;Initialize the parameters for an LSTM cell.    Args:      num_units: int, The number of units in the LSTM cell.      use_peepholes: bool, 是否使用窥视孔, 当设置为True时则使用窥视孔.      cell_clip: (optional) A float value, 单元(四个gate)输出的值被限制在`±cell_clip`内.      initializer: (optional) 用于权重和投影矩阵(projection matrices)的初始值设定.      num_proj: (optional) int, 投影矩阵的输出维数. 如果设置为None则不执行`投影`操作.      proj_clip: (optional) A float value. 如果设置了 `num_proj &gt; 0` 和 `proj_clip`, 则投影值将被限制在`[-proj_clip, proj_clip]`范围内.      num_unit_shards: Deprecated, 已经弃用.      num_proj_shards: Deprecated, 已经弃用.      forget_bias: Biases of the forget gate are initialized by default to 1 in order to reduce the scale of forgetting at the beginning of the training. Must set it manually to `0.0` when restoring from CudnnLSTM trained checkpoints.      state_is_tuple: If True, accepted and returned states are 2-tuples of the `c_state` and `m_state`.  If False, they are concatenated along the column axis.  This latter behavior will soon be deprecated.      activation: Activation function of the inner states.  Default: `tanh`.      reuse: (optional) Python boolean describing whether to reuse variables in an existing scope.  If not `True`, and the existing scope already has the given variables, an error is raised.      name: String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases.      When restoring from CudnnLSTM-trained checkpoints, use `CudnnCompatibleLSTMCell` instead.    &quot;&quot;&quot;</code></pre><p>从上面参数说明，可以看出<code>LSTMCell</code>中主要多处了一个<code>窥视孔</code>的功能，当参数<code>use_peepholes</code>设置为<code>True</code>时，就使用了窥视孔的功能。后续的几个参数也是对窥视功能的设置参数。下面对实现的<code>call</code>方法进行简要说明：</p><pre><code class="lang-python">def call(self, inputs, state):    &quot;&quot;&quot;Run one step of LSTM.    Args:      inputs: input Tensor, 2D, `[batch, num_units].      state: if `state_is_tuple` is False, this must be a state Tensor, `2-D, [batch, state_size]`.  If `state_is_tuple` is True, this must be a tuple of state Tensors, both `2-D`, with column sizes `c_state` and `m_state`.    Returns:      A tuple containing:      - A `2-D, [batch, output_dim]`, Tensor representing the output of the LSTM after reading `inputs` when previous state was `state`.        Here output_dim is:           num_proj if num_proj was set, num_units otherwise.      - Tensor(s) representing the new state of LSTM after reading `inputs` when the previous state was `state`.  Same type and shape(s) as `state`.    Raises:      ValueError: If input size cannot be inferred from inputs via static shape inference.    &quot;&quot;&quot;    num_proj = self._num_units if self._num_proj is None else self._num_proj    sigmoid = math_ops.sigmoid    if self._state_is_tuple:      (c_prev, m_prev) = state    else:      c_prev = array_ops.slice(state, [0, 0], [-1, self._num_units])      m_prev = array_ops.slice(state, [0, self._num_units], [-1, num_proj])    input_size = inputs.get_shape().with_rank(2)[1]    if input_size.value is None:      raise ValueError(&quot;Could not infer input size from inputs.get_shape()[-1]&quot;)    # i = input_gate, j = new_input, f = forget_gate, o = output_gate    lstm_matrix = math_ops.matmul(        array_ops.concat([inputs, m_prev], 1), self._kernel)    lstm_matrix = nn_ops.bias_add(lstm_matrix, self._bias)    i, j, f, o = array_ops.split(        value=lstm_matrix, num_or_size_splits=4, axis=1)    # Diagonal connections    if self._use_peepholes:      c = (sigmoid(f + self._forget_bias + self._w_f_diag * c_prev) * c_prev +           sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))      # sigmoid(f + self._forget_bias + self._w_f_diag * c_prev)为新的forget输出,    # sigmoid(i + self._w_i_diag * c_prev) 为新的input_gate输出    else:      c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *           self._activation(j))    if self._cell_clip is not None:      # pylint: disable=invalid-unary-operand-type      c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)      # pylint: enable=invalid-unary-operand-type    if self._use_peepholes:      m = sigmoid(o + self._w_o_diag * c) * self._activation(c)    else:      m = sigmoid(o) * self._activation(c)    if self._num_proj is not None:      m = math_ops.matmul(m, self._proj_kernel)      if self._proj_clip is not None:        # pylint: disable=invalid-unary-operand-type        m = clip_ops.clip_by_value(m, -self._proj_clip, self._proj_clip)        # pylint: enable=invalid-unary-operand-type    new_state = (LSTMStateTuple(c, m) if self._state_is_tuple else                 array_ops.concat([c, m], 1))    return m, new_state</code></pre><p>上面<code>call</code>方法实现的过程中，输出的维度为<code>num_proj</code>(如果设置了<code>num_proj</code>的值)或<code>num_units</code>。当计算<code>c</code>的时候取决于是否使用窥视孔，使用的时候计算方式会不同<code>sigmoid(f + self._forget_bias + self._w_f_diag * c_prev)</code>为新的<code>forget gate</code>输出(加入了窥视孔<code>self._w_f_diag * c_prev</code>), <code>sigmoid(i + self._w_i_diag * c_prev)</code>为新的<code>input gate</code>输出，然后会限制<code>c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)</code>输出。再计算<code>m</code>的时候也会取决于是否使用窥视孔，再对<code>m</code>的值进行限定。最后输出<code>m</code>和<code>state</code>。</p><p>当定义好了Cell后，后续就是动态或者静态计算时间步，或者多层的LSTM模型，和RNN网络中的使用就是一样的。这里就不再详细说明。</p><h2 id="5-Refrence"><a href="#5-Refrence" class="headerlink" title="5.Refrence"></a>5.Refrence</h2><p>(1).<a href="https://www.jiqizhixin.com/articles/2017-07-24-2" target="_blank" rel="noopener">LSTM入门必读：从入门基础到工作方式详解</a></p><p>(2).<a href="http://blog.gdf.name/lstm-with-tensorflow/" target="_blank" rel="noopener">从Tensorflow代码中理解LSTM网络</a></p><p>(3).<a href="https://yugnaynehc.github.io/2017/01/03/understanding-lstm-networks/" target="_blank" rel="noopener">[译]理解LSTM网络</a></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>tensorflow</tag>
      
      <tag>RNN</tag>
      
      <tag>LSTM</tag>
      
      <tag>gate</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-详解RNN网络及TensorFlow实现</title>
    <link href="/2018/05/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3RNN%E7%BD%91%E7%BB%9C%E5%8F%8ATensorFlow%E5%AE%9E%E7%8E%B0/"/>
    <url>/2018/05/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3RNN%E7%BD%91%E7%BB%9C%E5%8F%8ATensorFlow%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-RNN是什么"><a href="#1-RNN是什么" class="headerlink" title="1.RNN是什么"></a>1.RNN是什么</h2><p>RNN在<a href="https://zh.wikipedia.org/zh-hans/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">维基</a>上面有两种定义，但是一般默认的为时间递归神经网络，全称为(<code>Recurrent Neural Network</code>，简称为RNN)。RNN主要解决序列数据的处理，如文本，语音，视频等。典型应用在语言模型中，比如下面的示例：</p><pre><code class="lang-tex">我昨天上学迟到了，老师批评了____。</code></pre><p>让机器在空的地方填词，这儿填写的词最有可能是”我”，但对于这样的模型通过什么来实现呢？RNN就是很好的选择，它很擅长处理序列数据。</p><p>传统的神经网络是层与层之间进行连接，但是每层之间的神经元是没有连接的（假设各个数据之间是相互独立的）。而RNN的结构就是当前层的数据和之前的输出也有关系，即每层之间的神经元不再是无连接，而是有连接的。基本结构可以通过下面表示：</p><p><img src="\uploads\rnn_struct.png" srcset="/img/loading.gif" alt="nn"> </p><p>上面的结构中非常清晰的表示了layer的结构，主要针对序列型的数据，各个神经元之间存在关联，每个时刻的状态会输入到后续的时刻中。</p><h2 id="2-RNN结构"><a href="#2-RNN结构" class="headerlink" title="2.RNN结构"></a>2.RNN结构</h2><p>RNN的大体结构如上图所示，也可以更加详细的表示，如下图中，输入，输出，状态项等。如下图中，存在一个循环结构，每个时间点的状态进行了下一时间点输入。</p><p><img src="\uploads\rnn_01.jpg" srcset="/img/loading.gif" alt="RNN01"> </p><p>在上图中，输入单元（<code>input units</code>）为：$x_{t-1}, x_t, x_{t+1}$，输出单元（<code>output units</code>）为：$o_{t-1}, o_t, o_{t+1}$，隐藏单元（<code>hidden units</code>）为：$s_{t-1}, s_t, s_{t+1}$。</p><p>在某个时刻的隐层单元的输出：$s_t=f(Ws_{t-1}+Ux_t)$ ，其中$f$函数为激活函数，一般为<code>sigmoid,tanh,relu</code>等函数。在计算$s_0$时需要前面的状态，但是并不存在，因此一般设置为0向量。</p><p>某个时刻$t$的输出为：$o_t=softmax(Vs_t)=softmax(V(f(Ws_{t-1}+Ux_t)))$，其中$s_t$为时刻$t$的记忆单元。$s_t$包含了前面所有步的记忆，但是在实际使用过程中，$s_t$只会包含前面若干步的记忆，而不是所有步。</p><p>RNN中，每输入一步，每一层都共享参数$U,V,W$，RNN中主要在于隐藏层，在隐藏层能够捕捉到序列的关键信息。<br><a id="more"></a></p><h2 id="3-RNN的泛化结构"><a href="#3-RNN的泛化结构" class="headerlink" title="3.RNN的泛化结构"></a>3.RNN的泛化结构</h2><p>RNN的变体有很多，如双向RNN，多层双向RNN，多对多的RNN，一对多的RNN等等。下面对各种结构进行展示，并对这些结构进行简要说明。</p><h3 id="3-1-一对一"><a href="#3-1-一对一" class="headerlink" title="3.1 一对一"></a>3.1 一对一</h3><p>即一个输入对应一个输出的结构，如上图中的结构。</p><h3 id="3-2-多对一的结构"><a href="#3-2-多对一的结构" class="headerlink" title="3.2 多对一的结构"></a>3.2 多对一的结构</h3><p><img src="\uploads\RNN_02.png" srcset="/img/loading.gif" alt="Many to one"> </p><p>多个输入对应一个输出，比如情感分析。如一段话，判断这段话的情感。其中，$x_1, x_2, …, x_{t-1}, x_t$表示句子中的$t$个词语，$o$表示最终的情感输出标签。</p><h3 id="3-3-一对多的结构"><a href="#3-3-一对多的结构" class="headerlink" title="3.3 一对多的结构"></a>3.3 一对多的结构</h3><p><img src="\uploads\RNN_03.png" srcset="/img/loading.gif" alt="One to Many"> </p><p>这个结构和3.2中的多对一的结构类似。</p><h3 id="3-4-多对多结构"><a href="#3-4-多对多结构" class="headerlink" title="3.4 多对多结构"></a>3.4 多对多结构</h3><p><img src="\uploads\RNN_04.png" srcset="/img/loading.gif" alt="Many to Many"> </p><h3 id="3-5-双向RNN"><a href="#3-5-双向RNN" class="headerlink" title="3.5 双向RNN"></a>3.5 双向RNN</h3><p>前面的结构均为单向的RNN，$s_t$都只是记录了之前的信息，未考虑后面的信息。基于这种情况，于是出现了双向RNN，这种结构可以用到机器翻译，需要根据上下文的情况，给出翻译结果。</p><p><img src="\uploads\RNN_05.png" srcset="/img/loading.gif" alt="RNN"> </p><p>双向RNN的结构要复杂一些，如前向计算：</p><script type="math/tex; mode=display">o_t=w^{os}_ts_t+w^{oh}_th_t=w^{os}_t(w_{t-1}^{sh}s_{t-1}+w_{t-1}^{sx}x_{t})+w^{oh}_t(w_{t}^{hx}s_{t+1}+w_{t}^{hh}x_{t})</script><h3 id="3-6-多层RNN"><a href="#3-6-多层RNN" class="headerlink" title="3.6 多层RNN"></a>3.6 多层RNN</h3><p>前面的结构中多只是单层的state形式RNN，深度网络肯定是深层次结构会有更好的效果。因此可以是多层的RNN，多层次的RNN结构如下：</p><p><img src="\uploads\RNN_06.png" srcset="/img/loading.gif" alt="æ·±å±çRNN"> </p><h2 id="4-Back-Propagation-Through-Time-BPTT-训练"><a href="#4-Back-Propagation-Through-Time-BPTT-训练" class="headerlink" title="4.Back Propagation Through Time(BPTT)训练"></a>4.Back Propagation Through Time(BPTT)训练</h2><h3 id="4-1-符号说明"><a href="#4-1-符号说明" class="headerlink" title="4.1 符号说明"></a>4.1 符号说明</h3><p>现在根据上面的1对1结构，如下图所示，说明反向传播过程。在整个模型过程中有部分激励函数来对节点进行计算：</p><p><img src="http://lawlite.me/assets/blog_images/RNN/RNN_09.png" srcset="/img/loading.gif" alt="RNNåºæ¬ç»æ"> </p><p>$\phi$ ：隐藏层的激活函数</p><p>$\varphi$ ：输出层的变换函数</p><p>$L_t=L_t(o_t, y_t)$：模型的损失函数</p><p>其中$y_t$为一个one-hot向量。</p><h3 id="4-2-反向传播过程"><a href="#4-2-反向传播过程" class="headerlink" title="4.2 反向传播过程"></a>4.2 反向传播过程</h3><p>上面的损失函数只是计算了某个时刻的损失，当接受完序列后，再统一计算损失，此时模型的总损失为（假设输入序列的长度为$n$时）：</p><script type="math/tex; mode=display">L=\sum_{t=1}^n{L_t}</script><p>下面对整个结构细化后：</p><p><img src="http://lawlite.me/assets/blog_images/RNN/RNN_10.png" srcset="/img/loading.gif" alt="RNN"> </p><p>$o_t=\varphi(Vs_t)=\varphi(V(Ux_t+ws_{t-1}))$</p><p>其中$s_0=(0, 0, 0, ……, 0)^T$</p><p>令：$o_t^=Vs_t$，$s_t^=Us_t+ws_{t-1}$，即就是没有经过激励函数和变换函数前 ，则有：$o_t=\varphi(o_t^), s_t=\phi(s_t^)$</p><h4 id="4-2-1-矩阵V的更新"><a href="#4-2-1-矩阵V的更新" class="headerlink" title="4.2.1 矩阵V的更新"></a>4.2.1 矩阵V的更新</h4><p>对于V的更新和传统的神经网络更新方式一致，主要通过链式法则来进行求导：</p><script type="math/tex; mode=display">\frac{\partial{L_t}}{\partial{s_t^,}}=\frac{\partial{L_t}}{\partial{Vs_t}}\times\frac{\partial{V^Ts^T}}{\partial{s_t}}\times\frac{\partial{s_t}}{\partial{s_t^,}}=V^T(\frac{\partial{L_t}}{\partial{o_t}}\varphi'(o_t^,))\\\frac{\partial{L_t}}{\partial{s_{k-1}^,}}=\frac{\partial{L_t}}{\partial{s_{k}^,}}\times\frac{\partial{s_k^,}}{\partial{s_{k-1}^,}}=W^T\times(\frac{\partial{L_t}}{\partial{s_{k}^,}}\varphi'(s_{k-1}^,)), k=1,2,3,4,......,t</script><p>因为$L=\sum_{t=1}^n{L_t}$，所以对应矩阵V的更新导数为：</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{V}}=\sum_{t=1}^n(\frac{\partial{L_t}}{\partial{o_t}}{\varphi}'(o_t))\times{s_t^T}</script><h4 id="4-2-2-矩阵U和W的更新"><a href="#4-2-2-矩阵U和W的更新" class="headerlink" title="4.2.2 矩阵U和W的更新"></a>4.2.2 矩阵U和W的更新</h4><p>因为RNN和BP网络不同的是各个神经元之间(state)存在通信，因此再计算梯度的时候和BP网络计算梯度有一定的差异。可以通过循环来计算各个梯度，时间t从n到1进行循环。</p><p>首先计算某个时间点上的梯度：</p><script type="math/tex; mode=display">\frac{\partial{L_t}}{\partial{s_t^,}}=\frac{\partial{L_t}}{\partial{Vs_t}}\times\frac{\partial{V^Ts^T}}{\partial{s_t}}\times\frac{\partial{s_t}}{\partial{s_t^,}}=V^T(\frac{\partial{L_t}}{\partial{o_t}}\varphi'(o_t^,))\\\frac{\partial{L_t}}{\partial{s_{k-1}^,}}=\frac{\partial{L_t}}{\partial{s_{k}^,}}\times\frac{\partial{s_k^,}}{\partial{s_{k-1}^,}}=W^T\times(\frac{\partial{L_t}}{\partial{s_{k}^,}}varphi'(s_{k-1}^,)), k=1,2,3,4,......,t</script><p>利用巨补梯度计算U和W的梯度：</p><script type="math/tex; mode=display">\frac{\partial{L_t}}{\partial{U}}+=\sum_{k=1}^t\frac{\partial{L_t}}{\partial{s_k^,}}\times\frac{\partial{s_k^,}}{\partial{U}}=\sum_{k=1}^t\frac{\partial{L_t}}{\partial{s_k^,}}\times{x_t^T}\\\frac{\partial{L_t}}{\partial{W}}+=\sum_{k=1}^t\frac{\partial{L_t}}{\partial{s_k^,}}\times\frac{\partial{s_k^,}}{\partial{W}}=\sum_{k=1}^t\frac{\partial{L_t}}{\partial{s_k^,}}\times{s_{t-1}^T}</script><h3 id="4-3-训练问题"><a href="#4-3-训练问题" class="headerlink" title="4.3 训练问题"></a>4.3 训练问题</h3><p>从上面的参数更新可以看出，在更新权重时都需要计算一个激活函数的倒数，如果时间长度较长时，而梯度时累积的，因此会造成度消失或梯度爆炸</p><p>RNN的作用主要是存在记忆的功能，但是梯度问题又反应出了不能使用太长的时间长度，即不能记忆太久的信息，这样就存在一定矛盾，改进的思路主要有：</p><ol><li>使用一些trick，比如合适的励函数不使用tanh, sigmod等函数，使用relu类似的)，初始化，BN等</li><li>改进RNN中tate的传递方式比如RNN的升级版本LSTM模型。</li></ol><h2 id="5-RNN结构详解"><a href="#5-RNN结构详解" class="headerlink" title="5. RNN结构详解"></a>5. RNN结构详解</h2><p><img src="/uploads/RNNs结构详解图.png" srcset="/img/loading.gif" alt="RNN结构详解"></p><h2 id="6-RNN基于mnist的实现"><a href="#6-RNN基于mnist的实现" class="headerlink" title="6. RNN基于mnist的实现"></a>6. RNN基于mnist的实现</h2><p>基于mnist数据实现RNN模型，对结果的预测。mnist的图片为2828的矩阵，因此在RNN输入时：<code>time_step=28</code>，<code>depth=28</code>，目标序列的长度为10，RNN种的隐藏神经元个数在本实例中定义为128，即：<code>state_size=128</code>。</p><pre><code class="lang-python">&quot;&quot;&quot;Recurrent Neural Network. A Recurrent Neural Network (RNN) implementation example using TensorFlow library.&quot;&quot;&quot;from __future__ import division, print_functionfrom tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfimport osos.environ[&#39;TF_MIN_GPU_MULTIPROCESSOR_COUNT&#39;] = &#39;2&#39;mnist = input_data.read_data_sets(&#39;/opt/workspace/project/deep-st-nn/data/MNIST_DATA&#39;, one_hot=True)# defined train parameterslearning_rate = 0.02train_step = 10000batch_size = 100# defined network parameterstime_steps = 28  # the mnist image size 28  28depth = 28  # singe input vector length: depthhidden_num = 128  # state_sizenum_classes = 10  # mnist total class# graph inputswith tf.name_scope(&quot;inputs&quot;):    X = tf.placeholder(tf.float32, shape=(None, time_steps, depth), name=&quot;X&quot;)    Y = tf.placeholder(tf.float32, shape=(None, num_classes), name=&quot;Y&quot;)    with tf.name_scope(&quot;variables&quot;), tf.device(&quot;/cpu:0&quot;):        # defined variables        weights = {&quot;out&quot;: tf.Variable(tf.random_normal((hidden_num, num_classes)), trainable=True,                                      name=&quot;out_weights&quot;)}        bias = {&quot;out&quot;: tf.Variable(tf.random_normal([num_classes]),                                   name=&quot;out_bias&quot;)}def rnn_model(x, weights_, bias_):    &quot;&quot;&quot;    parameters    ----------        x: tensor, model inputs, shape is [batch_size, time_steps, depth]        weights_: dict variable, the output transform parameter, shape is [state_size, output_length or class_num]        bias_: dict variable, the output transform bias, shape is [output_length or class_num]    return    ------        tensor, model output, the shape is [batch_size, class_num]    &quot;&quot;&quot;    # inputs_ = tf.unstack(x, axis=1)  # to list [batch_size, depth]    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_num, forget_bias=1.0)    # cell = tf.nn.rnn_cell.BasicRNNCell(hidden_num)    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32,                                        initial_state=cell.zero_state(batch_size=batch_size, dtype=tf.float32))    outputs = tf.unstack(outputs, axis=1)    return tf.matmul(outputs[-1], weights_.get(&quot;out&quot;)) + bias_.get(&quot;out&quot;)def mul_rnn_model(x, weights_, bias_):    &quot;&quot;&quot;    parameters    ----------        x: tensor, model inputs, shape is [batch_size, time_steps, depth]        weights_: dict variable, the output transform parameter, shape is [state_size, output_length or class_num]        bias_: dict variable, the output transform bias, shape is [output_length or class_num]    return    ------        tensor, model output, the shape is [batch_size, class_num]    &quot;&quot;&quot;    with tf.name_scope(&quot;hidden&quot;):        mul_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicRNNCell(hidden_num) for _ in range(3)])        # dropout wrapper        dropper_cell = tf.nn.rnn_cell.DropoutWrapper(mul_cell, output_keep_prob=1, state_keep_prob=0.7)        outputs, states = tf.nn.dynamic_rnn(dropper_cell, x, dtype=tf.float32)        outputs = tf.unstack(outputs, axis=1)        # tf.summary.histogram(&quot;hidden_last_ouput&quot;, outputs[-1])        return tf.matmul(outputs[-1], weights_.get(&quot;out&quot;)) + bias_.get(&quot;out&quot;)with tf.name_scope(&quot;loss&quot;):    with tf.device(&quot;/gpu:1&quot;):        logits = mul_rnn_model(X, weights, bias)  # rnn_model(X, weights, bias)        prediction = tf.nn.softmax(logits)        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))with tf.name_scope(&quot;train&quot;):    with tf.device(&quot;/gpu:0&quot;):        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)        train_op = optimizer.minimize(loss_op)correct_predict = tf.equal(tf.argmax(prediction, axis=1), tf.argmax(Y, axis=1))accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))tf.summary.scalar(&quot;loss&quot;, loss_op)tf.summary.scalar(&quot;acc&quot;, accuracy)tf.summary.histogram(&quot;weight_out&quot;, weights.get(&quot;out&quot;))tf.summary.histogram(&quot;bias_out&quot;, bias.get(&quot;out&quot;))merged = tf.summary.merge_all()init_variable = tf.global_variables_initializer()# starting train# config = tf.ConfigProto(allow_soft_placement=True)with tf.Session() as sess:    sess.run(init_variable)    writer = tf.summary.FileWriter(&quot;.&quot; + &#39;/rnn&#39;, sess.graph)    for step in range(1, train_step+1):        batch_x, batch_y = mnist.train.next_batch(batch_size)        batch_x = batch_x.reshape((batch_size, time_steps, depth))        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})        if step % 100 == 0:            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})            print(&quot;The step: %d; Minibatch loss: %f; Train accuracy: %f&quot; % (step, loss, acc))        res = sess.run(merged, feed_dict={X: batch_x, Y: batch_y})        writer.add_summary(res, global_step=step)    print(&quot;Optimization Finished!&quot;)    # test model    test_len = 100    test_datas = mnist.test.images[:test_len].reshape((-1, time_steps, depth))    test_lable = mnist.test.labels[:test_len]    print(&quot;Test accuracy: %f&quot; % (sess.run(accuracy, feed_dict={X: test_datas, Y: test_lable})))</code></pre><h2 id="7-TensorFlow中RNN的实现"><a href="#7-TensorFlow中RNN的实现" class="headerlink" title="7.TensorFlow中RNN的实现"></a>7.TensorFlow中RNN的实现</h2><p>在TensorFlow中对RNN模型进行了基本实现，下面对tensorflow中的实现细节进行说明，主要包括cell`的实现环(多步执行)实现，以及多层rnn的实现。并对cell中的输入，输出进行详解。</p><p>RNN模型中的基本<code>cell</code>主要通过<code>tf.nn.rnn_cell.RNNCell</code>, <code>tf.nn.rnn_cell.BasicRNNCell</code> ,<code>tf.nn.rnn_cell.BasicLSTMCell</code>三个模块实现，后面两个cell模型就是基于前面模块的实现，<code>RNNCell</code>是一个抽象类(<code>abstract class</code>)。主要需要实现<code>state_size</code>，<code>output_size</code>，<code>build</code>等方法。下面从RNN中最基本的cell开始。</p><h3 id="7-1-单步RNN：RNNCell"><a href="#7-1-单步RNN：RNNCell" class="headerlink" title="7.1 单步RNN：RNNCell"></a>7.1 单步RNN：RNNCell</h3><p>RNNCell是RNN模型中的基本单元，这个cell就是上图中的一个长方形模块，也是tensorflow中实现RNN的基本单元，每个RNNCell都有一个<code>call</code>方法以及<code>__call__</code>方法，使用方式：<code>output, next_state = call(inputs, state)</code>，调用一次<code>call</code>方法就会计算当前时间步的输出和状态两个值，调用一次<code>RNNCell.call</code>方法相当于在时间轴上推进了一步。上面说了<code>BasicRNNCell</code> 和<code>BasicLSTMCell</code>是基于<code>RNNCell</code>抽象类的实现，因此一般在使用时都只是使用后面两个cell。首先用代码测试cell：</p><pre><code class="lang-python">import osimport tensorflow as tfimport numpy as npos.environ[&#39;TF_MIN_GPU_MULTIPROCESSOR_COUNT&#39;] = &#39;2&#39;cell = tf.nn.rnn_cell.BasicRNNCell(12, reuse=True)  # defined rnn cell, the parameter num_units(hidden_units_num or state_size)state_size = cell.state_size  # the state size in rnn modelprint(&quot;The rnn model state size is %d.&quot; % state_size)  # 12rnn_inputs = tf.placeholder(tf.float32, shape=(100, 10), name=&quot;rnn_inputs_x&quot;)  # 10 is vector depthinputs = rnn_inputsh0 = cell.zero_state(100, tf.float32)  # size: 100, 12out1, h1 = cell(inputs, h0)  # inputs size is (batch_size, depth)print(&quot;The state size:&quot;, h1.shape)# ------------------output----------------------------#The rnn model state size is 12.The state size: (100, 12) The output size: (100, 12)The state shape: (100, 12)</code></pre><p>上面的代码中定义了一个基本的<code>RNNCell</code>，传入了一个参数<code>num_units=12</code>，这个参数是定义<code>cell</code>中的隐藏单元的数量，即上图中的<code>state_size</code>大小，而另外一个参数<code>reuse=True</code>主要是觉得cell的参数是否共享。<code>state size</code>可以通过cell的<code>state_size</code>来获取。从上面的程序中也可以看出是和输入的<code>`num_units</code>值相同。</p><p><code>state</code>的初始化，在前面也说过，当在第一个时刻时，状态通过0来初始化一个<code>h0(state)</code>来输入到第一个时刻，cell中也提供了初始化的方法<code>zero_state(batch_size, dtype)</code>，初始化后<code>h0</code>的<code>shape</code>是<code>(batch_size, state_size)</code>。</p><p><code>RNNCell</code>的<code>call</code>方法主要接收两个参数<code>inputs and state</code>，参数<code>inputs</code>主要是某时刻的输入值，<code>shape</code>是<code>(batch_size, depth)</code>，这儿的<code>depth</code>就是上面图中的<code>k</code>值。参数<code>state</code>就是通过<code>cell</code>的方法<code>zero_state(batch_size, dtype</code>生成的<code>state</code>。调用<code>call</code>后返回<code>output, state</code>，它们的size都是<code>(batch_size, state_size)</code>.</p><p>下面看下<code>RNNCell</code>的<code>call</code>方法源码：</p><pre><code class="lang-python">def call(self, inputs, state):    gate_inputs = math_ops.matmul(array_ops.concat([inputs, state], 1), self._kernel)    # _kernel shape: (depth+state_size, state_size)    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)  # _bias size: state_size    output = self._activation(gate_inputs)  # default `tanh` function    return output, output</code></pre><p>可以看出，在实现过程中实际上是对<code>inputs</code>和<code>state</code>进行了<code>concat</code>，形成了<code>(batch_size, depth+state_size)</code>大小的tensor。然后再和参数进行乘积运算(_kernel的size是<code>(depth+state_size, state_size)</code>)，再加上了一个bias(size是：<code>(state_size)</code>)。再通过激活函数运算，再没有给定激活函数情况下，tensorflow默认使用的是<code>tanh</code>。</p><p>上面整个过程单步的RNNCell就运算结束了。下面进行多部RNNCell计算。</p><h3 id="7-2-循环起来：一次执行多步RNNCell"><a href="#7-2-循环起来：一次执行多步RNNCell" class="headerlink" title="7.2 循环起来：一次执行多步RNNCell"></a>7.2 循环起来：一次执行多步RNNCell</h3><p>单步RNNCell只能计算一个步骤，如果序列长度为100，那么就需要循环调用100次的RNNCell，如x1，h0得到o1，h1；x2，h1得到o2，h2；x3，h2得到o3，h3，这样依次执行。而TensorFlow中也提供一个函数(<code>tf.nn.dynamic_rnn</code>)来实现这个过程。这个函数是直接通过（h0, x1, x2, …,x100）得到(h1, h2, h3, h4, …, h100)等。下面看<code>dynamic_rnn</code>的参数：</p><pre><code class="lang-python">def dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None,                 parallel_iterations=None, swap_memory=False, time_major=False, scope=None):    &quot;&quot;&quot;    cell: An instance of RNNCell.    inputs: The RNN inputs.      If `time_major == False` (default), this must be a `Tensor` of shape:`[batch_size, max_time, ...]`, or a nested tuple of such elements.      If `time_major == True`, this must be a `Tensor` of shape: `[max_time, batch_size, ...]`, or a nested tuple of such elements.    initial_state: (optional) An initial state for the RNN.      If `cell.state_size` is an integer, this must be a `Tensor` of appropriate type and shape `[batch_size, cell.state_size]`.      If `cell.state_size` is a tuple, this should be a tuple of tensors having shapes `[batch_size, s] for s in cell.state_size`.    &quot;&quot;&quot;</code></pre><p>参数<code>cell</code>接收RNNCell实例化对象，如<code>BasicRNNCell</code> ，<code>BasicLSTMCell</code>实例化对象，也可以接收<code>MultiRNNCell</code>实例化对象等。参数<code>inputs</code>为输入序列，<code>inputs</code>的<code>shape</code>是<code>(batch_size, time_steps, depth)</code>，其中<code>depth</code>就是上图中的<code>k</code>，这个参数的shape主要取决于另外也给参数<code>time_major</code>，默认为<code>False</code>；这儿输入后再进入到cell的<code>inputs</code>时进行了转换，可以通过<code>unstack</code>的方式转换成<code>time_steps(bathc_size, depth)</code>，cell每次接收的inputs是<code>(batch_size, depth)</code>。下面是一个完整测试：</p><pre><code class="lang-python"># defined rnn cell, the parameter num_units(hidden_units_num or state_size)cell = tf.nn.rnn_cell.BasicRNNCell(12, reuse=None)  state_size = cell.state_size  # the state size in rnn modelprint(&quot;The rnn model state size is %d.&quot; % state_size)  # 12rnn_inputs = tf.placeholder(tf.float32, shape=(100, 20, 10), name=&quot;rnn_inputs_x&quot;)  # 20 is time steps, 10 is vector depthinputs = tf.unstack(rnn_inputs, axis=1)h0 = cell.zero_state(100, tf.float32)  # size: 100, 12out1, h1 = cell(inputs[0], h0)  # inputs size is (batch_size, depth)print(&quot;The state size:&quot;, h1.shape, &quot;The output size:&quot;, out1.shape)# dynamic_rnn: one time execute many time stepsoutput, state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=h0) print(&quot;The state shape:&quot;, state.shape)  # (100, 12)</code></pre><p>上面得到得output得shape是：<code>(batch_size, time_steps, cell.output_size)</code>，在上面得例子中，输出得shape是：<code>(100, 20, 12)</code>。</p><h3 id="7-3-MultiRNNCell：多层RNNCell"><a href="#7-3-MultiRNNCell：多层RNNCell" class="headerlink" title="7.3 MultiRNNCell：多层RNNCell"></a>7.3 MultiRNNCell：多层RNNCell</h3><p>上面的过程都只是单层的RNN模型，但是在实际应用中会使用多层RNN模型，因此需要基于RNNCell来堆叠多层的RNN，第一层RNN输出的(h1, h2, h3, …)，作为下一层的输入，这样以此类推。在TensorFlow中通过<code>MultiRNNCell</code>实现该功能：</p><pre><code class="lang-python">def get_a_cell(num):    return tf.nn.rnn_cell.BasicRNNCell(num_units=num)# 用tf.nn.rnn_cell MultiRNNCell创建3层RNNcell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell(i) for i in [128, 256, 128]])  # 3层RNNprint(cell.state_size)  # (128, 256, 128)inputs = tf.placeholder(np.float32, shape=(32, 100))  # 32是batch_size, 10为depthh0 = cell.zero_state(32, np.float32)  # 通过zero_state得到一个全0的初始状态output_multi, h1 = cell.call(inputs, h0)print(h1)  # tuple中含有3个32x128的向量</code></pre><p>上面的测试代码中，创建了3层的<code>RNN</code>模型，每层的<code>state_size</code>是<code>(128, 256, 128)</code>，<code>MultiRNNCell</code>得到的cell其实也是<code>RNNCell</code>的子类，然后调用<code>call</code>方法后输出了<code>output</code>和<code>h1</code>，输出后的<code>h1</code>是一个tuple，其中每个元素对应的是一个tensor，tensor的大小分别是<code>((32, 128), (32, 256), (32, 128))</code>。</p><p><code>MultiRNNCell</code>只是创建了单步的多层RNN，但在实际应用中肯定是多步的，因此还需要进行多步执行多层RNN，每层的time_steps是相同的。在TensorFlow中还是通过<code>dynamic_rnn</code>来实现的，下面是TensorFlow的官方示例：</p><pre><code class="lang-python"># create 2 LSTMCellsrnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]# create a RNN cell composed sequentially of a number of RNNCellsmulti_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)# &#39;outputs&#39; is a tensor of shape [batch_size, max_time, 256]# &#39;state&#39; is a N-tuple where N is the number of LSTMCells containing a tf.contrib.rnn.LSTMStateTuple for each celloutputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell, inputs=data, dtype=tf.float32)</code></pre><h3 id="7-4-误区"><a href="#7-4-误区" class="headerlink" title="7.4 误区"></a>7.4 误区</h3><h4 id="7-4-1-误区一：cell中的ouput"><a href="#7-4-1-误区一：cell中的ouput" class="headerlink" title="7.4.1 误区一：cell中的ouput"></a>7.4.1 误区一：cell中的ouput</h4><p>从上面单步RNN中的源码中可以看出，output和state都是一样的，这儿cell的output并非在实际应用中的Y值。还需要经过处理、计算才能输出最终的output。在RNNCell中的output只是当前cell的output，并非最终模型输出的Y值，因此Y值得size和cell得output_size也并非相等。需要对cell得output再定义新得变换才能转换成希望得输出Y。</p><h4 id="7-4-2-误区二：RNN中的dropout"><a href="#7-4-2-误区二：RNN中的dropout" class="headerlink" title="7.4.2 误区二：RNN中的dropout"></a>7.4.2 误区二：RNN中的dropout</h4><p>在RNN模型中也可以通过 dropout来有效得防止过拟合，在RNN中的dropout和其他的有一定的差异，在RNN中在时间序列方向不进行dropout，也就是在循环的部分不会进行dropout，如下图中，实线的部分不会进行dropout，在虚线的部分进行dropout。在实现时主要通过dropoutwrapper实现dropout功能。</p><p><img src="\uploads\dropoutwrapper.png" srcset="/img/loading.gif" alt="RNN简化结构"></p><h2 id="8-Refrence"><a href="#8-Refrence" class="headerlink" title="8.Refrence"></a>8.Refrence</h2><p>(1).<a href="http://www.tensorflownews.com/category/course/" target="_blank" rel="noopener">基于TensorFlow关于各种网络结构的实战案例</a></p><p>(2).<a href="https://www.jianshu.com/p/39a99c88a565" target="_blank" rel="noopener">详解循环神经网络(Recurrent Neural Network)</a></p><p>(3).<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">零基础入门循环神经网络</a></p><p>(4).<a href="https://zhuanlan.zhihu.com/p/32755043" target="_blank" rel="noopener">循环神经网络RNN介绍1：什么是RNN、为什么需要RNN、前后向传播详解、Keras实现</a></p><p>(5).<a href="https://blog.csdn.net/chenvast/article/details/79133127" target="_blank" rel="noopener">深度学习-TensorFlow实现RNN</a></p><p>(6).<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/" target="_blank" rel="noopener">循环神经网络RNN基础</a></p><p>(7).<a href="https://blog.csdn.net/mydear_11000/article/details/52414342" target="_blank" rel="noopener">解读TensorFlow下实现的RNN</a></p><p>(8).<a href="http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/#1-%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B" target="_blank" rel="noopener">RNN-循环神经网络-02Tensorflow中的实现 </a></p><p>(9).<a href="http://lawlite.me/2017/06/21/RNN-LSTM%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-03Tensorflow%E8%BF%9B%E9%98%B6%E5%AE%9E%E7%8E%B0/" target="_blank" rel="noopener">RNN-LSTM循环神经网络-03Tensorflow进阶实现</a></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>tensorflow</tag>
      
      <tag>SGD</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorBoard的使用教程</title>
    <link href="/2018/04/26/TensorBoard%E7%9A%84%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"/>
    <url>/2018/04/26/TensorBoard%E7%9A%84%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>TensorFlow 可用于训练大规模深度神经网络所需的计算，使用该工具涉及的计算往往复杂而深奥。很多人也认为NN为一个黑盒，对训练过程种的很多东西不太了解。因此Google推出了基于TensorFlow的深度学习可视化工具TensorBoard 。您可以用 TensorBoard 来展现 TensorFlow 图，绘制图像生成的定量指标图以及显示附加数据（如其中传递的图像），也可以通过该工具了解模型训练过程种参数的收敛过程，判断模型是否过拟合，是否是欠拟合等。</p><h2 id="1-summary"><a href="#1-summary" class="headerlink" title="1.summary"></a>1.summary</h2><p>首先明确一点,<code>summary</code>也是<code>op</code>。要对某些变量或值进行可视化，就必须在创建图的过程中将这些变量或标量给记录下来，后续将训练过程得记录给写入log，再通过tensorboard进行展示。</p><h3 id="1-1-标量可视化"><a href="#1-1-标量可视化" class="headerlink" title="1.1 标量可视化"></a>1.1 标量可视化</h3><p>如果我们想对标量在训练中可视化，可以使用<code>tf.summary.scalar()</code>，比如损失loss，accuracy等。这个API得接口如下所示，主要用于标量，会得到一个标量得summary：</p><pre><code class="lang-python">tf.summary.scalar(name, tensor, collections=None, family=None)&quot;&quot;&quot;调用这个函数来观察Tensorflow的Graph中某个节点parameters----------tensor: 想要在TensorBoard中观察的节点name: 为该节点设置名字，在TensorBoard中我们观察的曲线将会以name命名&quot;&quot;&quot;</code></pre><a id="more"></a><h3 id="1-2-参数可视化"><a href="#1-2-参数可视化" class="headerlink" title="1.2 参数可视化"></a>1.2 参数可视化</h3><p>使用<code>tf.summary.histogram()</code>可以对参数进行可视化，如权重，偏置项等。可以通过该API可视化参数的分布情况，每次迭代的分布情况等。</p><pre><code class="lang-python">tf.summary.histogram(name, values, collections=None, family=None)&quot;&quot;&quot;return------A scalar Tensor of type string. The serialized Summary protocol buffer.parameters----------    name: A name for the generated node. Will also serve as a series name in TensorBoard.    values: A real numeric Tensor. Any shape. Values to use to build the histogram.    collections: Optional list of graph collections keys. The new summary op is added to these collections.     Defaults to [GraphKeys.SUMMARIES].    family: Optional; if provided, used as the prefix of the summary tag name, which controls the tab name used for display on Tensorboard.&quot;&quot;&quot;</code></pre><h3 id="1-3-其他-images-text-audio-的可视化"><a href="#1-3-其他-images-text-audio-的可视化" class="headerlink" title="1.3 其他(images, text, audio)的可视化"></a>1.3 其他(images, text, audio)的可视化</h3><p>TensorBoard也可以对图片，文本和音频等近可视化，具体API可以查看官方文档。</p><h2 id="2-对所有的summary进行merge"><a href="#2-对所有的summary进行merge" class="headerlink" title="2.对所有的summary进行merge"></a>2.对所有的summary进行merge</h2><p>在 TensorFlow 中，只有当您运行指令时，指令才会执行，或者另一个 op 依赖于指令的输出时，指令才会运行。我们刚才创建的这些总结节点都围绕着您的图：您目前运行的 op 都不依赖于这些节点的结果。因此，为了生成总结信息，我们需要运行所有这些总结节点。这样的手动操作是枯燥而乏味的，因此可以使用 <a href="https://www.tensorflow.org/api_docs/python/tf/summary/merge_all?hl=zh-cn" target="_blank" rel="noopener"><code>tf.summary.merge_all</code></a> 来将这些操作合并为一个 op，从而生成所有总结数据，生成一个<code>summary</code>对象，供后面<code>File_Writer</code>的<code>add_summary</code>使用。<strong>和初始化所有的变量用法相似</strong>。</p><p>然后您可以执行该合并的总结 op，它会在特定步骤将所有总结数据生成一个序列化的 <code>summary protobuf</code> 对象。最后，要将此总结数据写入磁盘，请将此总结 protobuf 对象传递给 <a href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter?hl=zh-cn" target="_blank" rel="noopener"><code>tf.summary.FileWriter</code></a>。</p><h2 id="3-FileWriter"><a href="#3-FileWriter" class="headerlink" title="3.FileWriter"></a>3.FileWriter</h2><p>这个步骤主要是将summary的结果（protobuf序列化后的数据）写入磁盘。后面启动TensorBoard服务时好使用该数据，在前端进行可视化。下面在训练过程中会通过<code>add_summary(summary, global_step=None)</code>将summary添加到FileWriter对象中。</p><pre><code class="lang-python">Class FileWriter:    &quot;&quot;&quot;    Writes Summary protocol buffers to event files.    &quot;&quot;&quot;    def __init__(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None, filename_suffix=None)        &quot;&quot;&quot;        parameters        ----------        logdir: string, directory where event file will be written.        graph: A Graph object, such as sess.graph.        max_queue: Integer. 在向disk写数据之前，最大能够缓存event的个数         flush_secs: Number. 每多少秒像disk中写数据，并清空对象缓存.        graph_def: DEPRECATED: Use the graph argument instead.        filename_suffix: A string. Every event file&#39;s name is suffixed with suffix.        &quot;&quot;&quot;# examplewriter = tf.summary.FileWriter(&quot;.&quot; + &#39;/train&#39;, session.graph)</code></pre><h3 id="3-1-add-summary"><a href="#3-1-add-summary" class="headerlink" title="3.1 add_summary"></a>3.1 add_summary</h3><p>因为前面说明了summary也是一个OP，因此在需要session中，每个训练步（或者多个训练步）将summary对象添加到FileWriter中。summary对象是通过前面merge步骤中生成。因此一般会执行下面两个步骤：</p><pre><code class="lang-python">summary = session.run(merged, feed_dict={x: X, y: Y})wiriter.add_summary(summary, global_step=train_step)</code></pre><p>下面再看下<code>add_summary</code>的api接口：</p><pre><code class="lang-python">def add_summary(summary, global_step=None):    &quot;&quot;&quot;    parameters    ----------    summary: A Summary protocol buffer, optionally serialized as a string.    global_step: Number. Optional global step value to record with the summary.    &quot;&quot;&quot;</code></pre><blockquote><p><strong>note</strong>：</p><ol><li>如果使用<code>writer.add_summary(summary，global_step)</code>时没有传<code>global_step</code>参数,会使<code>scarlar_summary</code>变成一条直线。</li><li>只要是在计算图上的<code>Summary op</code>，都会被<code>merge_all</code>捕捉到， 不需要考虑变量生命周期问题。</li><li>如果执行一次，<code>disk</code>上没有保存<code>Summary</code>数据的话，可以尝试下<code>file_writer.flush()</code></li></ol></blockquote><h2 id="4-启动TensorBoard服务"><a href="#4-启动TensorBoard服务" class="headerlink" title="4.启动TensorBoard服务"></a>4.启动TensorBoard服务</h2><p>训练完成后，就可以进入到命令行启动TensorBoard，可以指定到某个logdir，指定TensorBoard服务监听的IP，监听的端口等。只需要再命令行指定，如下：</p><pre><code class="lang-shell">tensorboard --logdir tain --host 0.0.0.0 --port 88888tensorboard --logdir /tmp/train</code></pre><p>启动服务后就可以再浏览器中查看深度网络的可视化。</p><h2 id="5-配合name-scope使用"><a href="#5-配合name-scope使用" class="headerlink" title="5.配合name_scope使用"></a>5.配合<code>name_scope</code>使用</h2><p>当网络比较复杂时，整体网络的图看上去就会比较混乱，为了解决这个问题，TensorFlow中加入了name_scope，这样TensorBoard的可视化网络看上去更有层次性。如下使用：</p><pre><code class="lang-python">with tf.name_scope(&quot;weight&quot;):    w = tf.Variable(initial_value=tf.random_uniform([784, 10], -1, 1), name=&quot;weight&quot;)with tf.name_scope(&quot;bias&quot;):    b = tf.Variable(initial_value=tf.random_uniform(shape=[10]), name=&quot;bias&quot;)</code></pre><h2 id="6-Refrence"><a href="#6-Refrence" class="headerlink" title="6.Refrence"></a>6.Refrence</h2><p>[1]. <a href="https://jhui.github.io/2017/03/12/TensorBoard-visualize-your-learning/" target="_blank" rel="noopener">TensorBoard - Visualize your learning</a></p><p>[2].<a href="https://blog.csdn.net/u012436149/article/details/53184847" target="_blank" rel="noopener">TensorFlow可视化</a></p><p>[3].<a href="http://www.tensorfly.cn/tfdoc/how_tos/summaries_and_tensorboard.html" target="_blank" rel="noopener">TensorBoard可视化学习</a></p><p>[4].<a href="https://www.youtube.com/watch?v=eBbEDRsCmv4" target="_blank" rel="noopener">Hands on TensorBoard</a></p>]]></content>
    
    
    <categories>
      
      <category>tensorflow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tensorflow</tag>
      
      <tag>tensorboard</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS_7(1708)下基于Tensorflow对双GPU深度学习环境配置</title>
    <link href="/2018/04/25/CentOS7(1708)%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8F%8C%E6%98%BE%E5%8D%A1%E7%9A%84TensorFlow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <url>/2018/04/25/CentOS7(1708)%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8F%8C%E6%98%BE%E5%8D%A1%E7%9A%84TensorFlow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<p>在工作种需要对tensorflow的GPU环境进行配置，主要是基于centos7下的单机多显卡环境进行部署，因此首先需要对centos7进行安装。其次对显卡驱动的安装，因为是多显卡，在安装多显卡驱动的时候和但显卡驱动安装有一定的区别。下面对整个过程进行详细的说明，如果遇到类似的环境部署，可以参考该文档。</p><h2 id="1-Linux系统centos7-1708的安装"><a href="#1-Linux系统centos7-1708的安装" class="headerlink" title="1.Linux系统centos7-1708的安装"></a>1.Linux系统centos7-1708的安装</h2><p>通过镜像写入软件将系统镜像写入到U盘，再对整个系统进行安装。在安装过程中通过U盘启动，在选择时有<em>UEFI</em>模式和非<em>UEFI</em>模式，选择非<em>UEFI</em>模式的U盘启动。进入到安装选择界面。</p><h3 id="1-1-修改镜像挂载地址"><a href="#1-1-修改镜像挂载地址" class="headerlink" title="1.1 修改镜像挂载地址"></a>1.1 修改镜像挂载地址</h3><p>选择<code>Install CentOS7</code>选项后，进入到安装过程，如果整个过程顺利，则可以继续，本1.1就可以忽略。如出现问题：<code>failed to map image memory......</code>的情况，那么就需要修改镜像挂载地址。在出现该错误后继续，间隔1-2分钟后会出现命令行。可以查看dev下的目录(<code>ls /dev</code>)。我的U盘被挂载到了<code>/dev/sdb4</code>，需要把该目录记录下来，后续会使用。</p><p>再重启计算机，进入到U盘启动后的界面，选择<code>Install CentOS7</code>，然后按<code>tab</code>进入到编辑模式，修改其中的命令行：<code>Linuxefi /images/pxeboot/vmlinuzinst.stage22=hd:LABEL=CentOS\x207\x20x\86_64 quiet</code>，将修改为：<code>Linuxefi /images/pxeboot/vmlinuzinst.stage22=hd:/dev/sdb4 quiet</code>，再通过<code>Enter</code>快捷键执行安装。然后进入到图形安装界面。如果还是不能进入到图形界面，安装出错，请参考下面。</p><h3 id="1-2-图形界面出错问题解决"><a href="#1-2-图形界面出错问题解决" class="headerlink" title="1.2 图形界面出错问题解决"></a>1.2 图形界面出错问题解决</h3><p>如果图形界面安装出现问题(<code>x startup failed falling back to text mod</code>)，一般是安装基础的图形界面出问题。解决方法是重启计算机，U盘启动，不选择<code>Install CentOS7</code>，选择<code>Troubleshooting --&gt;</code>，进入到选项，并选择第一项，修改镜像地址(tab或e修改，修改的地方和1.1中的一样)。</p><h3 id="1-3-配置SSH服务"><a href="#1-3-配置SSH服务" class="headerlink" title="1.3 配置SSH服务"></a>1.3 配置SSH服务</h3><p>安装完系统后配置ssh服务，本文中将ssh端口修改为22222，需要开放22222端口。ssh具体配置就不详细说明。</p><h2 id="2-Linux查看显卡的相关命令"><a href="#2-Linux查看显卡的相关命令" class="headerlink" title="2.Linux查看显卡的相关命令"></a>2.Linux查看显卡的相关命令</h2><p><code>lspci</code>命令查看硬件接口信息，可以通过<code>lspci |grep -i vga</code>来查询显卡，可以看出电脑的所有显卡，显示显卡型号。也可以显示显卡比较详细的信息：<code>lspci -vnn | grep VGA -A 12</code><br><a id="more"></a></p><h2 id="3-安装nvidia显卡驱动"><a href="#3-安装nvidia显卡驱动" class="headerlink" title="3.安装nvidia显卡驱动"></a>3.安装nvidia显卡驱动</h2><pre><code class="lang-shell">rpm -Uvh &lt;http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm&gt;  # 添加源yum install nvidia-detect  # 安装nvidia-detect命令nvidia-detect -v  # 检测显卡型号yum update  # 更新yumyum update kernel kernel-devel  # 更新核lsmod|grep nouveau</code></pre><p>修改<code>/lib/modprobe.d/dist-blacklist.conf</code>文件，在文件中加入：<code>blacklist nouveau options nouveau modeset=0</code></p><h3 id="3-1-更改并重新生成grub2"><a href="#3-1-更改并重新生成grub2" class="headerlink" title="3.1 更改并重新生成grub2"></a>3.1 更改并重新生成grub2</h3><p>打开 <code>/etc/default/grub</code>文件，在其中的：<code>GRUB_CMDLINE_LINUX=”rd.lvm.lv=vg_centos/lv_root rd.lvm.lv=vg_centos/lv_swap rhgb quiet”</code> quiet后面加入<code>rdblacklist=nouveau</code>，保存。</p><pre><code class="lang-shell">grub2-mkconfig -o /boot/grub2/grub.cfg</code></pre><p>我们首先把现有的移动到其它路径下以作为留手备份，打开终端执行：</p><p><code>sudo mv /boot/initramfs-$(uname -r).img /你喜欢的路径，</code>然后重建它，执行：<code>sudo dracut /boot/initramfs-$(uname -r).img $(uname -r)</code>即可。</p><h3 id="3-2-安装NVIDIA驱动"><a href="#3-2-安装NVIDIA驱动" class="headerlink" title="3.2 安装NVIDIA驱动"></a>3.2 安装NVIDIA驱动</h3><p>官网<a href="http://www.nvidia.com/Download/index.aspx?lang=en-us" target="_blank" rel="noopener">下载</a>驱动，下载下来时.run的文件，然后运行<code>sh *.run</code>即可。然后遇到提示什么的直接<code>yes</code>即可。在安装了驱动后提供了驱动自动更新的命令：<code>nvidia-installer --update</code>。安装完后查看<code>/etc/X11/xorg.conf</code>的内容，会发现 Device 的 Driver 设置会成为NVidia。</p><p>再通过<code>lspci |grep -i vga</code>查看显卡，发现intel的集成显卡不见了，因为前面将集成显卡禁用掉了。因此只能看现在的独立显卡。</p><h3 id="3-3-安装Bumblebee"><a href="#3-3-安装Bumblebee" class="headerlink" title="3.3 安装Bumblebee"></a>3.3 安装Bumblebee</h3><pre><code class="lang-shell"> yum -y install bumblebee</code></pre><p>在Bumblebee官方wiki上对cuda使用进行了说明，如果使用cuda可以不用Bumblebee，因此也可以不用安装Bumblebee，不用配置切换。</p><h2 id="4-安装CUDA-toolkit"><a href="#4-安装CUDA-toolkit" class="headerlink" title="4.安装CUDA-toolkit"></a>4.安装CUDA-toolkit</h2><p>下载地址在<a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=CentOS&amp;target_version=7&amp;target_type=runfilelocal" target="_blank" rel="noopener">这里</a>选择合适的版本，本次选用的cuda9.1的最新版本安装，下载后放入某个目录下面，并运行<code>sh ./cuda_9.1.85_387.26_linux.run</code>进行安装，安装过程中一定要注意某些选项：</p><pre><code class="lang-tex">Do you accept the previously read EULA?accept/decline/quit: acceptInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 387.26?(y)es/(n)o/(q)uit: nInstall the CUDA 9.1 Toolkit?(y)es/(n)o/(q)uit: yEnter Toolkit Location [ default is /usr/local/cuda-9.1 ]:Do you want to install a symbolic link at /usr/local/cuda?(y)es/(n)o/(q)uit:Do you want to install a symbolic link at /usr/local/cuda?(y)es/(n)o/(q)uit: yInstall the CUDA 9.1 Samples?(y)es/(n)o/(q)uit: yEnter CUDA Samples Location [ default is /root ]: /usr/local/cuda-9.1/examples</code></pre><p>如果出现部分lib缺失的情况，那么就需要安装相应的依赖库，主要出现以下的情况</p><pre><code class="lang-tex">Installing the CUDA Toolkit in /usr/local/cuda-9.1 ...Missing recommended library: libGLU.soMissing recommended library: libX11.soMissing recommended library: libXi.soMissing recommended library: libXmu.so</code></pre><p>在通过命令<code>yum install mesa-libGLU-devel libXi-devel libXmu-devel</code>来安装依赖库并重新安装cuda-toolkit。并按照上面的过程进行安装。安装完后会提示：</p><pre><code>lease make sure that -   PATH includes /usr/local/cuda-9.1/bin -   LD_LIBRARY_PATH includes /usr/local/cuda-9.1/lib64, or, add /usr/local/cuda-9.1/lib64 to /etc/ld.so.conf and run ldconfig as rootTo uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-9.1/bin</code></pre><p>配置cuda，将cuda/bin和cuda/lib分布添加到PATH和LD_LIBRARY_PATH。如将环境变量添加到<code>/etc/profile</code>中：<code>export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-9.1/lib64:/usr/local/cuda/lib64</code>，<code>export PATH=$PATH:/usr/local/cuda-9.1/bin:/usr/local/cuda/bin</code>，<code>export CUDA_HOME=/usr/local/cuda-9.1</code>并<code>source /etc/profile</code>。如果要对cuda进行测试，进入到examples目录下的对NVIDIA_CUDA-9.1_Samples下的文件进行编译。编译后运行某个example进行测试。如下：</p><pre><code>[root@localhost deviceQuery]# ./deviceQuery./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking)Detected 2 CUDA Capable device(s)Device 0: &quot;GeForce GTX 1060 6GB&quot;  CUDA Driver Version / Runtime Version          9.1 / 9.1  CUDA Capability Major/Minor version number:    6.1  Total amount of global memory:                 6078 MBytes (6373572608 bytes)  (10) Multiprocessors, (128) CUDA Cores/MP:     1280 CUDA Cores  GPU Max Clock rate:                            1785 MHz (1.78 GHz)  Memory Clock rate:                             4004 Mhz  Memory Bus Width:                              192-bit  L2 Cache Size:                                 1572864 bytes  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers  Total amount of constant memory:               65536 bytes  Total amount of shared memory per block:       49152 bytes  Total number of registers available per block: 65536  Warp size:                                     32  Maximum number of threads per multiprocessor:  2048  Maximum number of threads per block:           1024  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)  Maximum memory pitch:                          2147483647 bytes  Texture alignment:                             512 bytes  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)  Run time limit on kernels:                     No  Integrated GPU sharing Host Memory:            No  Support host page-locked memory mapping:       Yes  Alignment requirement for Surfaces:            Yes  Device has ECC support:                        Disabled  Device supports Unified Addressing (UVA):      Yes  Supports Cooperative Kernel Launch:            Yes  Supports MultiDevice Co-op Kernel Launch:      Yes  Device PCI Domain ID / Bus ID / location ID:   0 / 2 / 0  Compute Mode:     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;Device 1: &quot;GeForce GTX 1050 Ti&quot;  CUDA Driver Version / Runtime Version          9.1 / 9.1  CUDA Capability Major/Minor version number:    6.1  Total amount of global memory:                 4040 MBytes (4235919360 bytes)  ( 6) Multiprocessors, (128) CUDA Cores/MP:     768 CUDA Cores  GPU Max Clock rate:                            1392 MHz (1.39 GHz)  Memory Clock rate:                             3504 Mhz  Memory Bus Width:                              128-bit  L2 Cache Size:                                 1048576 bytes  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers  Total amount of constant memory:               65536 bytes  Total amount of shared memory per block:       49152 bytes  Total number of registers available per block: 65536  Warp size:                                     32  Maximum number of threads per multiprocessor:  2048  Maximum number of threads per block:           1024  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)  Maximum memory pitch:                          2147483647 bytes  Texture alignment:                             512 bytes  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)  Run time limit on kernels:                     No  Integrated GPU sharing Host Memory:            No  Support host page-locked memory mapping:       Yes  Alignment requirement for Surfaces:            Yes  Device has ECC support:                        Disabled  Device supports Unified Addressing (UVA):      Yes  Supports Cooperative Kernel Launch:            Yes  Supports MultiDevice Co-op Kernel Launch:      Yes  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0  Compute Mode:     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;&gt; Peer access from GeForce GTX 1060 6GB (GPU0) -&gt; GeForce GTX 1050 Ti (GPU1) : No&gt; Peer access from GeForce GTX 1050 Ti (GPU1) -&gt; GeForce GTX 1060 6GB (GPU0) : NodeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 9.1, NumDevs = 2Result = PASS</code></pre><p>上面<code>Result = PASS</code>说明校验通过。</p><pre><code>[root@localhost bandwidthTest]# ./bandwidthTest[CUDA Bandwidth Test] - Starting...Running on... Device 0: GeForce GTX 1060 6GB Quick Mode Host to Device Bandwidth, 1 Device(s) PINNED Memory Transfers   Transfer Size (Bytes)        Bandwidth(MB/s)   33554432                     6374.1 Device to Host Bandwidth, 1 Device(s) PINNED Memory Transfers   Transfer Size (Bytes)        Bandwidth(MB/s)   33554432                     6448.6 Device to Device Bandwidth, 1 Device(s) PINNED Memory Transfers   Transfer Size (Bytes)        Bandwidth(MB/s)   33554432                     144068.5Result = PASSNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</code></pre><p><code>Result = PASS</code>说明通信正常。</p><h2 id="5-安装cuDNN"><a href="#5-安装cuDNN" class="headerlink" title="5.安装cuDNN"></a>5.安装cuDNN</h2><p>首先下载和CUDA对应版本的cuDNN的版本。下载地址在<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">这里</a>，再上传到服务器上某个目录，进行操作。首先下载的文件为压缩格式的，主要操作过程如下：</p><pre><code class="lang-shell"># 切换到压缩包位置cp cudnn-9.1-linux-x64-v7.1.solitairetheme8 cudnn-9.1-linux-x64-v7.1.tgztar -xvf cudnn-9.1-linux-x64-v7.1.tgzcp ./lib64/* /usr/local/cuda-9.1/lib64/cp ./include/* /usr/local/cuda-9.1/include/chmod a+r /usr/local/cuda-9.1/include/cudnn.h /usr/local/cuda-9.1/lib64/libcudnn*</code></pre><p>通过上面的过程就安装配置完成了。</p><h2 id="6-安装tensorflow-gpu"><a href="#6-安装tensorflow-gpu" class="headerlink" title="6.安装tensorflow-gpu"></a>6.安装tensorflow-gpu</h2><h3 id="6-1-Anaconda安装"><a href="#6-1-Anaconda安装" class="headerlink" title="6.1 Anaconda安装"></a>6.1 Anaconda安装</h3><p>在清华大学的镜像网站（<a href="https://mirrors.tuna.tsinghua.edu.cn/）上下载最新的Anaconda版本，我们下载的是5.1.0版本，python版本为3.6.4，然后进行安装：" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/）上下载最新的Anaconda版本，我们下载的是5.1.0版本，python版本为3.6.4，然后进行安装：</a></p><pre><code class="lang-shell">sh Anaconda3-5.1.0-Linux-x86_64.sh# prefix处输入/opt/anaconda3，将anaconda安装到/opt/anaconda目录下# 并将python和anaconda的bin目录加入到.bashrc下</code></pre><p>安装了anaconda后，将pip，numpy，scipy，matplotlib，pandas，jupyter-notebook等都集成了，再安装一个查看gpu状态的工具：</p><pre><code class="lang-shell">pip install gpustat</code></pre><h3 id="6-2-安装Tensorflow的GPU版本"><a href="#6-2-安装Tensorflow的GPU版本" class="headerlink" title="6.2 安装Tensorflow的GPU版本"></a>6.2 安装Tensorflow的GPU版本</h3><p>在安装了anaconda后，可以安装一个查看GPU状态的工具，通过pip进行安装即可：<code>pip install gpustat</code>。安装tensorflow直接执行<code>conda install tensorflow-gpu</code>，会自动安装tensorflow的GPU版本并将cuda的相关动态链接库安装好，cuda相关的动态库都已经安装在了<code>${CONDA_HOME}/anaconda3/lib</code>。</p><p>由于conda上面支持的<code>tensorflow-gpu</code>版本，因此conda安装<code>tensorflow-gpu</code>会将cudnn，cuda以及mkl一起安装，版本会自动对应，因此相对来说比较容易，且不容易出错。</p><p>还可以通过pip来安装最新的<code>tensorflow</code>版本：<code>pip install tensorflow-gpu</code>，pip会找到相应的版本安装。安装后可能会出现GPU不可用的情况，这种情况基本是tensorflow安装的版本问题。</p><p>安装完成后进行测试：</p><pre><code class="lang-shell">&gt;&gt;&gt; hello = tf.constant(&#39;Hello, Tensorflow&#39;)&gt;&gt;&gt; sess = tf.Session()2018-04-16 02:26:55.745130: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-04-16 02:26:56.924933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2018-04-16 02:26:56.925576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845pciBusID: 0000:02:00.0totalMemory: 5.94GiB freeMemory: 5.86GiB2018-04-16 02:26:56.992033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2018-04-16 02:26:56.992277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties:name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392pciBusID: 0000:01:00.0totalMemory: 3.94GiB freeMemory: 3.89GiB2018-04-16 02:26:56.992325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1408] Ignoring visible gpu device (device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1) with Cuda multiprocessor count: 6. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.2018-04-16 02:26:56.992342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 02018-04-16 02:26:57.159155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:2018-04-16 02:26:57.159213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 12018-04-16 02:26:57.159237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N2018-04-16 02:26:57.159245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N2018-04-16 02:26:57.159377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5649 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:02:00.0, compute capability: 6.1)&gt;&gt;&gt; print(sess.run(hello))b&#39;Hello, Tensorflow&#39;</code></pre><h3 id="6-3-安装keras"><a href="#6-3-安装keras" class="headerlink" title="6.3 安装keras"></a>6.3 安装keras</h3><p>安装keras较简单，直接用pip安装即可：<code>pip install keras</code>，会自动安装到site-packages，进入到/root/.keras/keras.json文件修改backed为tensorflow：</p><pre><code class="lang-json">{    &quot;floatx&quot;: &quot;float32&quot;,    &quot;epsilon&quot;: 1e-07,    &quot;backend&quot;: &quot;tensorflow&quot;,    &quot;image_data_format&quot;: &quot;channels_last&quot;}</code></pre><h2 id="7-配置jupyter-notebook或jupyter-lab"><a href="#7-配置jupyter-notebook或jupyter-lab" class="headerlink" title="7.配置jupyter-notebook或jupyter-lab"></a>7.配置jupyter-notebook或jupyter-lab</h2><p>jupyter配置好后，可以远程连接服务器上的jupyter-server，方便多人使用anaconda的环境。首先，需要对jupyter的cnfig进行配置，先生成一个配置文件：</p><pre><code class="lang-shell">jupyter notebook --generate-config# Writing default config to: /root/.jupyter/jupyter_notebook_config.py</code></pre><p>在设置服务器登陆的密码：</p><pre><code class="lang-python">In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password:Verify password:Out[2]: &#39;sha1:1c5cf71ad9a3:dad7ae0af719426816841d239f5e8247176dd0adsf&#39;</code></pre><p>将生成的<code>sha1:...</code>复制下来在配置文件中进行配置(<code>vim ~/.jupyter/jupyter_notebook_config.py</code>)，配置项主要有：</p><pre><code class="lang-shell">c.NotebookApp.password =c.NotebookApp.port = 8888c.NotebookApp.allow_root = Truec.NotebookApp.open_browser = Falsec.NotebookApp.notebook_dir = &#39;/opt/workspace&#39;</code></pre><p>开放相应的端口：</p><pre><code class="lang-shell">[root@localhost cuda]# firewall-cmd --zone=public --add-port=8888/tcp --permanent[root@localhost cuda]# firewall-cmd --zone=public --add-port=8889/tcp --permanent[root@localhost cuda]# firewall-cmd --reload</code></pre><h2 id="8-可能出现的错误及解决办法"><a href="#8-可能出现的错误及解决办法" class="headerlink" title="8.可能出现的错误及解决办法"></a>8.可能出现的错误及解决办法</h2><h3 id="8-1-出现错误-39-GLIBC-2-23-39-not-fund"><a href="#8-1-出现错误-39-GLIBC-2-23-39-not-fund" class="headerlink" title="8.1 出现错误&#39;GLIBC_2.23&#39; not fund"></a>8.1 出现错误<code>&#39;GLIBC_2.23&#39; not fund</code></h3><p><code>ImportError: /lib64/libm.so.6: version GLIBC_2.23&#39; not found (required by /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)</code></p><pre><code class="lang-shell">mkdir ~/glibccd ~/glibc wget http://ftp.gnu.org/gnu/glibc/glibc-2.23.tar.gztar zxvf glibc-2.23.tar.gzcd glibc-2.23mkdir buildcd build../configure --prefix=/opt/glibc-2.23make -j4sudo make installexport LD_LIBRARY_PATH=/opt/glibc-2.23/lib</code></pre><p>这个错误修复后可能会出现python无法使用的情况。python最初是根据glibc-2.17安装的，因此python会出现问题。但是这种放肆能解决上面的错误。在其他场景能通过这种方式。</p><h3 id="8-2出现错误：InvalidArgumentError"><a href="#8-2出现错误：InvalidArgumentError" class="headerlink" title="8.2出现错误：InvalidArgumentError"></a>8.2出现错误：<code>InvalidArgumentError</code></h3><p>出现错误如下，只检测到了CPU但是没有检测到GPU。基本是tensorflow的版本和cuda和cudnn的版本未匹配。如果可以通过conda来安装就通过conda安装，会自动匹配对应的版本。</p><pre><code class="lang-tex">(see above for traceback): Cannot assign a device for operation &#39;matmul/bias&#39;: Operation was illuminated assigned to /device:GPU:1 but available devices are [/job:localhost/replica:0/task:0/device :CPU:0 ]. Make sure the device specification refers to a valid device`</code></pre><p>还有另外一个问题，只能检测到一个CPU，另外一个CPU不在GPU列表中，这种情况下主要是因为tensorflow单机默认只是用一个GPU，因此需要进行指定某个GPU，但是需要设置一个环境变量<code>TF_MIN_GPU_MULTIPROCESSOR_COUNT</code>。在系统种可以设置一个环境变量<code>export TF_MIN_GPU_MULTIPROCESSOR_COUNT=2</code>；在工程种应用时需要通过代码设置下环境变量，如下：</p><pre><code class="lang-python">import osos.environ[&#39;TF_MIN_GPU_MULTIPROCESSOR_COUNT&#39;] = &#39;2&#39;</code></pre><h3 id="8-3-出现FutureWarning警告"><a href="#8-3-出现FutureWarning警告" class="headerlink" title="8.3 出现FutureWarning警告"></a>8.3 出现FutureWarning警告</h3><p>出现这个警告<code>FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type</code>.解决办法：更新<em>h5py</em>的版本到<em>2.8.0rc1</em>，通过<code>pip install h5py==2.8.0rc1</code>命令来更新。</p><h2 id="9-Refrence"><a href="#9-Refrence" class="headerlink" title="9.Refrence"></a>9.Refrence</h2><p>1.<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/204.html" target="_blank" rel="noopener">Linux驱动程序安装范例</a></p><p>2.<a href="http://infonook.org/centos-bumblebee-nvidia/" target="_blank" rel="noopener">CentOS 7(1708) Intel+Nvidia双显卡笔记本安装Nvidia驱动并用Bumblebee控制独显</a></p><p>3.<a href="https://wiki.archlinux.org/index.php/Bumblebee_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87" target="_blank" rel="noopener">Bumblebee wiki</a>)</p><p>4.<a href="http://notes.maxwi.com/2017/02/26/ubuntu-cuda8-env-set/" target="_blank" rel="noopener">CUDA的安装配置</a></p><p>5.<a href="http://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux" target="_blank" rel="noopener">cuDNN官方配置</a></p><p>6.<a href="https://ying-zhang.github.io/cloud/2017/setup-tensorflow-gpu-centos7/" target="_blank" rel="noopener">Centos7安装tensorflow深度学习环境</a></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tensorflow</tag>
      
      <tag>gpu</tag>
      
      <tag>deeplearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AWS上EC2主机(ubuntu)下对某个端口的开放</title>
    <link href="/2018/04/18/AWS%E4%B8%8AEC2%E4%B8%BB%E6%9C%BA%E4%B8%8B%E5%AF%B9%E6%9F%90%E4%B8%AA%E7%AB%AF%E5%8F%A3%E7%9A%84%E5%BC%80%E6%94%BE/"/>
    <url>/2018/04/18/AWS%E4%B8%8AEC2%E4%B8%BB%E6%9C%BA%E4%B8%8B%E5%AF%B9%E6%9F%90%E4%B8%AA%E7%AB%AF%E5%8F%A3%E7%9A%84%E5%BC%80%E6%94%BE/</url>
    
    <content type="html"><![CDATA[<p>首先用telnet来看下本地的端口是否开启：<code>telnet ip 7711</code>，如果是正常的那么需要对该端口进行开放。首先需要对本地的防火墙端口进行开放。其他需要对AWS上的安全组进行设置，设置为对应的端口或所有端口开放权限。本文中不对AWS安全组的设置说明，自行google即可。</p><pre><code class="lang-shell">iptables-save &gt; /${DIR_PATH)}/iptables.rules</code></pre><p><a id="more"></a>编辑<code>/${DIR_PATH)}/iptables.rules</code></p><pre><code class="lang-tex"># Generated by iptables-save v1.6.0 on Wed Apr 18 03:08:40 2018*mangle:PREROUTING ACCEPT [1427:334897]:INPUT ACCEPT [1427:334897]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [1092:144305]:POSTROUTING ACCEPT [1092:144305]:FORWARD_direct - [0:0]:INPUT_direct - [0:0]:OUTPUT_direct - [0:0]:POSTROUTING_direct - [0:0]:PREROUTING_ZONES - [0:0]:PREROUTING_ZONES_SOURCE - [0:0]:PREROUTING_direct - [0:0]:PRE_public - [0:0]:PRE_public_allow - [0:0]:PRE_public_deny - [0:0]:PRE_public_log - [0:0]-A PREROUTING -j PREROUTING_direct-A PREROUTING -j PREROUTING_ZONES_SOURCE-A PREROUTING -j PREROUTING_ZONES-A INPUT -j INPUT_direct-A FORWARD -j FORWARD_direct-A OUTPUT -j OUTPUT_direct-A POSTROUTING -j POSTROUTING_direct-A PREROUTING_ZONES -g PRE_public-A PRE_public -j PRE_public_log-A PRE_public -j PRE_public_deny-A PRE_public -j PRE_public_allowCOMMIT# Completed on Wed Apr 18 03:08:40 2018# Generated by iptables-save v1.6.0 on Wed Apr 18 03:08:40 2018*security:INPUT ACCEPT [1327:356187]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [1095:144849]:FORWARD_direct - [0:0]:INPUT_direct - [0:0]:OUTPUT_direct - [0:0]-A INPUT -j INPUT_direct-A FORWARD -j FORWARD_direct-A OUTPUT -j OUTPUT_directCOMMIT# Completed on Wed Apr 18 03:08:40 2018# Generated by iptables-save v1.6.0 on Wed Apr 18 03:08:40 2018*raw:PREROUTING ACCEPT [1433:361586]:OUTPUT ACCEPT [1095:144849]:OUTPUT_direct - [0:0]:PREROUTING_direct - [0:0]-A PREROUTING -j PREROUTING_direct-A OUTPUT -j OUTPUT_directCOMMIT# Completed on Wed Apr 18 03:08:40 2018# Generated by iptables-save v1.6.0 on Wed Apr 18 03:08:40 2018*nat:PREROUTING ACCEPT [115:6083]:INPUT ACCEPT [19:1084]:OUTPUT ACCEPT [9:920]:POSTROUTING ACCEPT [9:920]:OUTPUT_direct - [0:0]:POSTROUTING_ZONES - [0:0]:POSTROUTING_ZONES_SOURCE - [0:0]:POSTROUTING_direct - [0:0]:POST_public - [0:0]:POST_public_allow - [0:0]:POST_public_deny - [0:0]:POST_public_log - [0:0]:PREROUTING_ZONES - [0:0]:PREROUTING_ZONES_SOURCE - [0:0]:PREROUTING_direct - [0:0]:PRE_public - [0:0]:PRE_public_allow - [0:0]:PRE_public_deny - [0:0]:PRE_public_log - [0:0]-A PREROUTING -j PREROUTING_direct-A PREROUTING -j PREROUTING_ZONES_SOURCE-A PREROUTING -j PREROUTING_ZONES-A OUTPUT -j OUTPUT_direct-A POSTROUTING -j POSTROUTING_direct-A POSTROUTING -j POSTROUTING_ZONES_SOURCE-A POSTROUTING -j POSTROUTING_ZONES-A POSTROUTING_ZONES -g POST_public-A POST_public -j POST_public_log-A POST_public -j POST_public_deny-A POST_public -j POST_public_allow-A PREROUTING_ZONES -g PRE_public-A PRE_public -j PRE_public_log-A PRE_public -j PRE_public_deny-A PRE_public -j PRE_public_allowCOMMIT# Completed on Wed Apr 18 03:08:40 2018# Generated by iptables-save v1.6.0 on Wed Apr 18 03:08:40 2018*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [677:90233]:FORWARD_IN_ZONES - [0:0]:FORWARD_IN_ZONES_SOURCE - [0:0]:FORWARD_OUT_ZONES - [0:0]:FORWARD_OUT_ZONES_SOURCE - [0:0]:FORWARD_direct - [0:0]:FWDI_public - [0:0]:FWDI_public_allow - [0:0]:FWDI_public_deny - [0:0]:FWDI_public_log - [0:0]:FWDO_public - [0:0]:FWDO_public_allow - [0:0]:FWDO_public_deny - [0:0]:FWDO_public_log - [0:0]:INPUT_ZONES - [0:0]:INPUT_ZONES_SOURCE - [0:0]:INPUT_direct - [0:0]:IN_public - [0:0]:IN_public_allow - [0:0]:IN_public_deny - [0:0]:IN_public_log - [0:0]:OUTPUT_direct - [0:0]:ufw-after-forward - [0:0]:ufw-after-input - [0:0]:ufw-after-logging-forward - [0:0]:ufw-after-logging-input - [0:0]:ufw-after-logging-output - [0:0]:ufw-after-output - [0:0]:ufw-before-forward - [0:0]:ufw-before-input - [0:0]:ufw-before-logging-forward - [0:0]:ufw-before-logging-input - [0:0]:ufw-before-logging-output - [0:0]:ufw-before-output - [0:0]:ufw-reject-forward - [0:0]:ufw-reject-input - [0:0]:ufw-reject-output - [0:0]:ufw-track-forward - [0:0]:ufw-track-input - [0:0]:ufw-track-output - [0:0]-A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A INPUT -i lo -j ACCEPT-A INPUT -j INPUT_direct-A INPUT -j INPUT_ZONES_SOURCE-A INPUT -j INPUT_ZONES-A INPUT -p icmp -j ACCEPT-A INPUT -m conntrack --ctstate INVALID -j DROP-A INPUT -j REJECT --reject-with icmp-host-prohibited-A INPUT -j ufw-before-logging-input-A INPUT -j ufw-before-input-A INPUT -j ufw-after-input-A INPUT -j ufw-after-logging-input-A INPUT -j ufw-reject-input-A INPUT -j ufw-track-input-A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -i lo -j ACCEPT-A FORWARD -j FORWARD_direct-A FORWARD -j FORWARD_IN_ZONES_SOURCE-A FORWARD -j FORWARD_IN_ZONES-A FORWARD -j FORWARD_OUT_ZONES_SOURCE-A FORWARD -j FORWARD_OUT_ZONES-A FORWARD -p icmp -j ACCEPT-A FORWARD -m conntrack --ctstate INVALID -j DROP-A FORWARD -j REJECT --reject-with icmp-host-prohibited-A FORWARD -j ufw-before-logging-forward-A FORWARD -j ufw-before-forward-A FORWARD -j ufw-after-forward-A FORWARD -j ufw-after-logging-forward-A FORWARD -j ufw-reject-forward-A FORWARD -j ufw-track-forward-A OUTPUT -j OUTPUT_direct-A OUTPUT -j ufw-before-logging-output-A OUTPUT -j ufw-before-output-A OUTPUT -j ufw-after-output-A OUTPUT -j ufw-after-logging-output-A OUTPUT -j ufw-reject-output-A OUTPUT -j ufw-track-output-A FORWARD_IN_ZONES -g FWDI_public-A FORWARD_OUT_ZONES -g FWDO_public-A FWDI_public -j FWDI_public_log-A FWDI_public -j FWDI_public_deny-A FWDI_public -j FWDI_public_allow-A FWDO_public -j FWDO_public_log-A FWDO_public -j FWDO_public_deny-A FWDO_public -j FWDO_public_allow-A INPUT_ZONES -g IN_public-A IN_public -j IN_public_log-A IN_public -j IN_public_deny-A IN_public -j IN_public_allow-A IN_public_allow -p tcp -m tcp --dport 22 -m conntrack --ctstate NEW -j ACCEPTCOMMIT# Completed on Wed Apr 18 03:08:40 2018</code></pre><p>将需要开放的端口(端口号自行修改)加入到该文件中的COMMIT前：</p><pre><code>-A IN_public_allow -p tcp -m tcp --dport 7711 -m conntrack --ctstate NEW -j ACCEPT-A IN_public_allow -p tcp -m tcp --dport 7712 -m conntrack --ctstate NEW -j ACCEPT-A IN_public_allow -p tcp -m tcp --dport 7713 -m conntrack --ctstate NEW -j ACCEPT-A IN_public_allow -p tcp -m tcp --dport 7714 -m conntrack --ctstate NEW -j ACCEPT</code></pre><p>恢复规则：<code>iptables-restore ./iptables.rules</code>，再使用使用ufw命令来reload：<code>sudo ufw reload</code>。下面可以在其他机器上来远程ping下端口是否正常：<code>telnet ip 7711</code>。</p><p>上面的过程可以解决<code>telnet: connect to address ip: No route to host</code>的问题。</p><pre><code class="lang-shell">[root@localhost ~]# telnet 13.59.15.195 7712Trying 13.59.15.96...telnet: connect to address 13.59.15.195: No route to host</code></pre>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python科学计算加速</title>
    <link href="/2018/04/14/python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%8A%A0%E9%80%9F/"/>
    <url>/2018/04/14/python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%8A%A0%E9%80%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="1-使用GPU"><a href="#1-使用GPU" class="headerlink" title="1.使用GPU"></a>1.使用GPU</h2><p>使用GPU有两种方式：numba和pycuda，可以通过这两个package和python实现CUDA程序。</p><h3 id="1-1-numba"><a href="#1-1-numba" class="headerlink" title="1.1 numba"></a>1.1 numba</h3><p>CUDA官方加速是通过numba来使用GPU或CPU加速，在很早以前是通过numbapro实现的，属于收费版本，后面全部整合成了numba和accelerate，而且免费开放，在Anaconda环境下已经安装了numba，只需要使用即可。使用numba的GPU加速需要进行配置。具体配置方式可查看官方文档。</p><p>Refrence</p><p>[1].numba官方地址：<a href="http://numba.pydata.org/" target="_blank" rel="noopener">http://numba.pydata.org/</a></p><p>[2].numba官方文档：<a href="http://numba.pydata.org/doc.html" target="_blank" rel="noopener">http://numba.pydata.org/doc.html</a></p><p>[3].numba配置CUDA的GPU：<a href="http://blog.csdn.net/u013975830/article/details/78822919" target="_blank" rel="noopener">http://blog.csdn.net/u013975830/article/details/78822919</a><a id="more"></a></p><h3 id="1-2-pycudapy"><a href="#1-2-pycudapy" class="headerlink" title="1.2 pycudapy"></a>1.2 pycudapy</h3><p>cuda也是提供python到gpu的接口，简单可以使用pycuda自身的接口实现，复杂的就需要自己实现cuda程序再通过pycuda实现。</p><p>Refrence</p><p>[1].CUDA和pycuda环境的配置：<a href="http://blog.csdn.net/zhouqingtaostudy/article/details/50896948" target="_blank" rel="noopener">http://blog.csdn.net/zhouqingtaostudy/article/details/50896948</a></p><p>[2].pycuda的官网：<a href="https://documen.tician.de/pycuda/" target="_blank" rel="noopener">https://documen.tician.de/pycuda/</a>    <a href="https://mathema.tician.de/software/pycuda/" target="_blank" rel="noopener">https://mathema.tician.de/software/pycuda/</a></p><p>[3].pycuda的tutorial翻译：<a href="http://blog.cycleuser.org/pycuda-tutorial-zhong-wen-ban.html" target="_blank" rel="noopener">http://blog.cycleuser.org/pycuda-tutorial-zhong-wen-ban.html</a></p><p>[4].pycuda的notebook：<a href="http://nbviewer.jupyter.org/github/Kivy-CN/Duke-STA-633-CN/tree/master/%E5%B7%B2%E7%BB%8F%E7%BF%BB%E8%AF%91%E7%9A%84%E8%AE%B2%E5%BA%A7%E5%86%85%E5%AE%B9/Topic21_GPU_Computing/" target="_blank" rel="noopener">http://nbviewer.jupyter.org/github/Kivy-CN/Duke-STA-633-CN/tree/master/已经翻译的讲座内容/Topic21_GPU_Computing/</a></p><p>[5].pycuda的翻译：<a href="https://zhuanlan.zhihu.com/p/32062796" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32062796</a></p><h2 id="2-使用C加速"><a href="#2-使用C加速" class="headerlink" title="2.使用C加速"></a>2.使用C加速</h2><p>python中可以使用c实现部分功能来进行加速，比如Cython，Cython也可以和numpy配合使用，来对numpy进行加速。</p><h3 id="2-1-Cython加速"><a href="#2-1-Cython加速" class="headerlink" title="2.1 Cython加速"></a>2.1 Cython加速</h3><p>(1) 使用Cython和numpy结合    将两者结合使用可以有效的加速python中的核心代码，下面通过示例说明怎么结合。下面通过cython的形式实现一个矩阵点乘的方法，具体代码如下，文件名为<code>multi.pyx</code>：</p><pre><code class="lang-python">from __future__ import divisioncimport cythonimport numpy as npcimport numpy as npDTYPE = np.float64ctypedef np.float64_t DTYPE_t@cython.boundscheck(False)def multi_product(np.ndarray a, np.ndarray b):    &quot;&quot;&quot;    Parameters    ----------        a: np.ndarray, the shape of (m, n)        b: np.ndarray, the shape of (m, n)    return    ------        np.ndarray, return the array shape of (m, n)    &quot;&quot;&quot;    cdef int a_array_r = a.shape[0]    cdef int a_array_c = a.shape[1]    cdef int b_array_r = b.shape[0]    cdef int b_array_c = b.shape[1]    if a_array_r != b_array_r or a_array_c != b_array_c:        raise Exception(&quot;ValueError: operands could not be broadcast together with shapes %s %s&quot;                       %(str((a_array_r, a_array_c)), str((b_array_r, b_array_c))))     out = np.zeros((a_array_r, a_array_c))    for _r_ in range(a_array_r):        for _c_ in range(a_array_c):             out[_r_, _c_] = a[_r_, _c_] * b[_r_, _c_]    return out</code></pre><p>创建一个setup.py的文件，类似c++中的makefile配置文件，setup.py文件内容主要时配置编译，具体代码如下所示：</p><pre><code class="lang-python">import numpyfrom distutils.core import setup, Extensionfrom Cython.Distutils import build_extsetup(    cmdclass={&#39;build_ext&#39;: build_ext},    ext_modules=[                 Extension(&quot;multi&quot;, [&quot;multi.pyx&quot;],                 include_dirs=[numpy.get_include()])              ])</code></pre><p>上面setup.py文件中的include_dirs非常重要，否则在后面编译pyx文件时会出现一些错误：找不到”numpy/arrayobject.h”,因此需要在setup.py文件中将依赖的.h文件给添加上。下面通过命令进行编译即可：</p><pre><code class="lang-shell">python setup.py build_ext --inplace</code></pre><p> 编译后会生成<code>.so</code>文件(linux，windows下为dll文件)。后面通过python直接引用该文件，调用其中的方法即可。下面对使用cython实现的点乘方法和numpy包的<code>multiply</code>方法效率进行对比：</p><pre><code class="lang-python">import timeitprint(timeit.timeit(stmt=&quot;multi.multi_product(x, y)&quot;, setup=&quot;import multi; import numpy as np; x = np.array([[1, 2, 3], [2.4, 89.0, 11]]); y = np.array([[1, 2, 3], [2.4, 89.0, 11]])&quot;, number=10000))print(timeit.timeit(stmt=&quot;np.multiply(x, y)&quot;, setup=&quot;import numpy as np; x = np.array([[1, 2, 3], [2.4, 89.0, 11]]); y = np.array([[1, 2, 3], [2.4, 89.0, 11]])&quot;, number=10000))# -------- output -------------------0.031960667001840190.006172515997604933</code></pre><p>效率上还是比不上numpy，主要是因为numpy在这方面做了深度优化。但是比纯python实现的方法要快很多。<br>Refrence</p><p>[1]Cython官方文档：<a href="http://cython.org/" target="_blank" rel="noopener">http://cython.org/</a></p><p>[2].Cython基础教程中文版：<a href="https://moonlet.gitbooks.io/cython-document-zh_cn/content/ch1-basic_tutorial.html" target="_blank" rel="noopener">https://moonlet.gitbooks.io/cython-document-zh_cn/content/ch1-basic_tutorial.html</a></p><p>[3].Cython中<code>def</code>, <code>cdef</code>和<code>cpdef</code>区别和使用：<a href="http://notes-on-cython.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">http://notes-on-cython.readthedocs.io/en/latest/index.html</a></p><p>[4].<a href="http://www.cnblogs.com/lidyan/p/7474244.html" target="_blank" rel="noopener">Cython中def,cdef,cpdef的区别</a>: <a href="http://www.cnblogs.com/lidyan/p/7474244.html" target="_blank" rel="noopener">http://www.cnblogs.com/lidyan/p/7474244.html</a></p><h3 id="2-2-PyPy-加速"><a href="#2-2-PyPy-加速" class="headerlink" title="2.2 PyPy 加速"></a>2.2 <a href="http://pypy.org/" target="_blank" rel="noopener">PyPy</a> 加速</h3><p>PyPy是用<a href="https://baike.baidu.com/item/Python" target="_blank" rel="noopener">Python</a>实现的Python解释器。</p><p>[1].用 Psyco 让 Python 运行得像 C 一样快：<a href="https://www.ibm.com/developerworks/cn/linux/sdk/python/charm-28/" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/sdk/python/charm-28/</a></p><h3 id="2-3-numexpr加速"><a href="#2-3-numexpr加速" class="headerlink" title="2.3 numexpr加速"></a>2.3 numexpr加速</h3><p>使用numexpr[<a href="http://numexpr.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">http://numexpr.readthedocs.io/en/latest/index.html</a>]</p><p>Refrence</p><p>[1].优化 Python 性能：PyPy、Numba 与 Cython比较：<a href="https://www.zhihu.com/question/24695645" target="_blank" rel="noopener">https://www.zhihu.com/question/24695645</a></p><p>[2].numpy的c-api接口：<a href="http://python.usyiyi.cn/documents/NumPy_v111/reference/c-api.html" target="_blank" rel="noopener">http://python.usyiyi.cn/documents/NumPy_v111/reference/c-api.html</a></p><p>[3].python中使用C代码-以numpy为例：<a href="https://segmentfault.com/a/1190000000479951" target="_blank" rel="noopener">https://segmentfault.com/a/1190000000479951</a></p><p>[4].<a href="https://github.com/johnnylee/python-numpy-c-extension-examples" target="_blank" rel="noopener">python-numpy-c-extension-examples</a>：<a href="https://github.com/johnnylee/python-numpy-c-extension-examples" target="_blank" rel="noopener">https://github.com/johnnylee/python-numpy-c-extension-examples</a></p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>numba</tag>
      
      <tag>cython</tag>
      
      <tag>cuda</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Youth青春-塞缪尔·厄尔曼</title>
    <link href="/2018/01/22/Youth%E9%9D%92%E6%98%A5-%E5%A1%9E%E7%BC%AA%E5%B0%94%C2%B7%E5%8E%84%E5%B0%94%E6%9B%BC/"/>
    <url>/2018/01/22/Youth%E9%9D%92%E6%98%A5-%E5%A1%9E%E7%BC%AA%E5%B0%94%C2%B7%E5%8E%84%E5%B0%94%E6%9B%BC/</url>
    
    <content type="html"><![CDATA[<blockquote><p>青春不是年华，而是心境；青春不是桃面、红唇、柔膝，而是坚强的意志、恢宏的想象、炽热的感情；青春是生命深泉的自在涌流。</p><p>青春气贯长虹，勇锐盖过怯弱，进取压倒苟安。如此锐气，二十后生有之，六旬男子则更多见。年岁有加，并非垂老；理想丢弃，方堕暮年。</p><p>岁月悠悠，衰微只及肌肤；热忱抛却，颓废必致灵魂。忧烦、惶恐、丧失自信，定使心灵扭曲，意气如灰。</p><p>无论年届花甲，抑或二八芳龄，心中皆有生命之欢乐，好奇之冲动，孩童般天真久盛不衰。</p><p>你我心中都有一台天线，只要你从天上、人间接受美好、希望、欢乐、勇气和力量的信号，你就会青春永驻，风华常存。</p><p> 一旦天线坠下，锐气便被冰雪覆盖，玩世不恭、自暴自弃油然而生，即使年方二十，实则垂垂老矣；然而只要竖起天线，捕捉乐观信号，即使八十高龄，行将告别尘寰，你也会觉得年轻依旧，希望永存。<a id="more"></a></p></blockquote><p>下面是英文版本的原文：</p><p><strong>YOUTH</strong>  - Samuel Ullman</p><p>Youth is not a time of life; it is astateof mind; it is not a matter of rosy cheeks, red lips and supple knees; itis amatter of the will, a quality of the imagination, a vigor of the emotions;itis the freshness of the deep springs of life.</p><p>Youth means a tempera-mental predominanceofcourage over timidity, of the appetite for adventure over the love of ease.Thisoften exists in a man of 60 more than a boy of 20. Nobody grows old merelyby anumber of years. We grow old by deserting our ideals.</p><p>Years may wrinkle the skin, but to giveupenthusiasm wrinkles the soul. Worry, fear, self-distrust bows the heartandturns the spring back to dust.</p><p>Whether 60 or 16, there is in everyhumanbeing’s heart the lure of wonder, the unfailing childlike appetite of what’snextand the joy of the game of living.</p><p>In the center of your heart and myheartthere is a wireless station: so long as it receives messages of beauty,hope,cheer, courage and power from men and from the Infinite, so long are youyoung.</p><p>When the aerials are down, and yourspiritis covered with snows of cynicism and the ice of pessimism, then you aregrownold, even at 20, but as long as your aerials are up, to catch wavesofoptimism, there is hope you may die young at 80.</p><p>《青春》的作者塞缪尔·厄尔曼是位犹太人，1840年生于德国，儿时随家西渡，移居美国，年逾70开始写作。他以教育家和社会活动家而闻名，在文学创作方面也很有才华。二战期间，太平洋战争打得正酣，麦克阿瑟将军常常从繁忙中抬起头，注视着挂在墙上的镜框，镜框里正是篇文章，塞缪尔·厄尔曼的《青春》。后来，日本人在东京的美军总部发现了它，《青春》便开始在日本流传。</p><p>1988年，日本数百名流聚会东京，纪念厄尔曼的这篇文章。松下公司元老松下幸之助感慨地说：“20年来，《青春》与我朝夕相伴，它是我的座右铭。”</p><p>欧洲一位政界名宿也极力推荐：“无论男女老幼，要想活得风光，就得拜读《青春》。”</p>]]></content>
    
    
    <categories>
      
      <category>诗歌</category>
      
    </categories>
    
    
    <tags>
      
      <tag>诗歌</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用python连接hive数据库并进行数据操作</title>
    <link href="/2017/10/13/%E4%BD%BF%E7%94%A8python%E8%BF%9E%E6%8E%A5hive%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/"/>
    <url>/2017/10/13/%E4%BD%BF%E7%94%A8python%E8%BF%9E%E6%8E%A5hive%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<p>在数据抽取或者数据存取过程中难免会遇到用其他语言对<code>hive</code>数据库进行操作。如<code>python</code>远程对<code>hive</code>数据库进行操作，需要通过<code>thrift</code>服务进行操作。本文的环境是<code>python==3.6.1</code>， <code>hive==1.1.0</code>，<code>thfit==0.10.0</code></p><h2 id="ThriftServer介绍"><a href="#ThriftServer介绍" class="headerlink" title="ThriftServer介绍"></a>ThriftServer介绍</h2><p>客户端对<code>hive</code>数据库进行操作可以通过<code>Cli</code>进行，在本地这种方式较好，但是在远程操作时会很麻烦，因此提供了<code>hiveserver</code>和<code>hiveserver2</code>，在不启动cli的情况下对<code>hive</code>进行操作，可以在远程通过其他语言(<code>java, python, php</code>等)向hive请求并返回结果。也就是利用<code>thrift</code>进行整个过程操作。</p><p><code>hiveserver2</code>主要是向远程调用提供了接口，通过<code>thrift rpc</code>实现，进行远程的操作。可以实现远程并行操作<code>hive</code>数据库。启动<code>hiveserver2</code>如下：</p><pre><code class="lang-shell">nohup hive --service hiveserver2 &amp;</code></pre><p>在启动后<code>hiveserver2</code>默认监听的端口为<code>10000</code>，可以在hive的配置文件(<code>hive-site.xml</code>)中查看或者修改该端口。可以通过命令查看该端口是否被监听：<code>netstat -antp | grep 10000</code><a id="more"></a></p><h2 id="连接hive的框架"><a href="#连接hive的框架" class="headerlink" title="连接hive的框架"></a>连接hive的框架</h2><p>在连接<code>hive</code>时，可以选择<code>thrift</code>本身，<code>pyhive</code>，<code>pyhs2</code>和<code>impyla</code>。在使用过程中首先尝试了<code>thrift</code>本身，但是通过配置后在连接返回：<code>thrift.transport.TTransport.TTransportException: None</code>，据网友说这是连接<code>hiveserver2</code>出现的问题。因此弃用<code>thrift</code>直接连接<code>hive</code>，选择其他三个。</p><p><code>pyhs2</code>是以前hive官方推荐使用的库，主要依赖了<code>thrift</code>和<code>sasl</code>。但是这个库后面没有维护了，因此在最新的<code>python</code>和<code>hive</code>下有很多问题，因此弃用。</p><p><code>impyla</code>是通过<code>impala</code>来对操作<code>hive</code>，目前使用的环境中<code>impala</code>没有启用，因此该库就放弃使用了。因此只剩下的<code>pyhive</code>。但是网上的使用者反应，推荐使用<code>pyhive</code>和<code>impyla</code>。</p><h2 id="配置pyhive的运行环境"><a href="#配置pyhive的运行环境" class="headerlink" title="配置pyhive的运行环境"></a>配置pyhive的运行环境</h2><p>因为通过比较选择了<code>pyhive</code>和<code>hive</code>进行交互，因此需要在客户端部署<code>pyhive</code>，不需要在服务端安装<code>pyhive</code>。本文中主要是针对<code>linux</code>系统上的部署，不考虑<code>windows</code>上的部署。</p><ol><li>安装依赖包<code>sasl</code>的环境. 当安装了下面的<code>sasl</code>相关的部署包才能正确安装sasl</li></ol><pre><code class="lang-shell"># ubuntusudo apt-get install sasl2-bin libsasl2-2 libsasl2-dev libsasl2-modules# centossudo yum install cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-md5 cyrus-sasl-plain# 使用pip安装python下的saslpip install sasl==0.2.1</code></pre><ol><li>安装<code>thrift</code>的<code>python</code>包：<code>pip install thrift==0.10.0</code></li><li>安装<code>thrift_sasl</code>，者个包依赖了<code>sasl</code>和<code>thrift</code>：<code>pip install thrift_sasl====0.3.0</code></li><li>如果是<code>ubuntu</code>，则会多出一个步骤，否则在引用<code>sasl</code>包时可能会报错：<code>_ZTVNSt7__cxx1118basic_stringstreamIcSt11char_traitsIcESaIcEEE</code>，因此需要对<code>python</code>使用的<code>libgcc</code>进行更新，因为我部署的<code>python</code>环境是<code>anaconda3</code>因此直接执行<code>conda install libgcc</code>即可。</li><li>安装<code>pyhive</code>，<code>pyhive</code>的安装很简单：<code>pip install pyhive==0.5.0</code></li></ol><p>经过上面的安装可以进行包的测试：</p><pre><code class="lang-shell">python -c &quot;import sasl&quot;python -c &quot;import thrift&quot;python -c &quot;import pyhive&quot;</code></pre><p>测试成功后就可以连接服务端操作hive：</p><pre><code class="lang-python">from pyhive import hivefrom TCLIService.ttypes import TOperationStatecursor = hive.connect(&#39;localhost&#39;).cursor()cursor.execute(&#39;SELECT * FROM table LIMIT 10&#39;, async=True)status = cursor.poll().operationStatewhile status in (TOperationState.INITIALIZED_STATE, TOperationState.RUNNING_STATE):    logs = cursor.fetch_logs()    for message in logs:        print message    status = cursor.poll().operationStateprint cursor.fetchall()</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.cnblogs.com/liu-yao/p/3hive-dethriftserver-fu-wu.html" target="_blank" rel="noopener">hive的hiveserver服务介绍</a></p><p><a href="https://github.com/dropbox/PyHive" target="_blank" rel="noopener">pyhive的github官方地址</a></p><p><a href="https://stackoverflow.com/questions/22838752/hadoop-python-client-driver-for-hiveserver2-fails-to-install" target="_blank" rel="noopener">Python client driver for HiveServer2 fails to install</a></p><p><a href="https://stackoverflow.com/questions/27147208/python-hive-thrift-transport-ttransport-ttransportexception-none" target="_blank" rel="noopener">Python Hive: thrift.transport.TTransport.TTransportException: None</a></p><p><a href="http://blog.csdn.net/hjh00/article/details/64917226" target="_blank" rel="noopener">Thift在系统中的配置</a></p><p>[<a href="http://www.cnblogs.com/KattyJoy/p/6540125.html" target="_blank" rel="noopener">CentOS6.5下通过Thrift使用Python连接操作hive 安装配置记录</a>]</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>hive</tag>
      
      <tag>thrift</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>对pyc和pyo文件的理解</title>
    <link href="/2017/10/10/%E5%AF%B9pyc%E5%92%8Cpyo%E6%96%87%E4%BB%B6%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <url>/2017/10/10/%E5%AF%B9pyc%E5%92%8Cpyo%E6%96%87%E4%BB%B6%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="1-python的基本运行机制"><a href="#1-python的基本运行机制" class="headerlink" title="1. python的基本运行机制"></a>1. python的基本运行机制</h2><p>作为<code>Python</code>爱好者，需要了解<code>.py</code>脚本的<strong>基本运行机制</strong>及<strong>特性</strong>：在很多工作上<code>Python</code>的运行流程基本上<strong>取决于用户</strong>，因此源码不需要编译成二进制代码(否则无法实现大部分贴近用户的特性)，而直接从源码运行程序。当我们运行<code>python</code>文件程序的时候，<code>Python</code>解释器将<strong>源码</strong>转换为<strong>字节码</strong>，然后再由<strong>解释器</strong>来执行这些字节码。<br>因此总的来说，它具有以下三条特性：</p><ol><li>源码<strong>距离底层更远</strong>(根据官方文档的解释。不说，也感觉得到)</li><li>运行时<strong>都需要生成字节码</strong>，交由<strong>虚拟机</strong>执行。(虚拟机在哪儿！你们也不看看各自都是用什么软件执行的！没错，就是解释器，别和我说是<code>IDLE</code>啊。虚拟机具体实现了由<code>switch-case</code>语句构成的框架函数<em><code>PyEval_EvalFrameEx</code></em>，刚刚说的字节码就是这货执行的)</li><li>每次执行脚本，虚拟机<strong>总要多出加载和链接的流程</strong>。<a id="more"></a></li></ol><h2 id="2-执行脚本的过程"><a href="#2-执行脚本的过程" class="headerlink" title="2. 执行脚本的过程"></a>2. 执行脚本的过程</h2><p>那么，有人要问了：不是说，运行时总要生成字节码么！那，字节码都去哪儿了？先说说，虚拟机它是怎么执行脚本的</p><ol><li>完成模块的加载和链接；</li><li>将源代码翻译为<code>PyCodeObject</code>对象(这货就是字节码)，并将其写入内存当中(方便<code>CPU</code>读取，起到加速程序运行的作用)；</li><li>从上述内存空间中读取指令并执行；</li><li>程序结束后，根据命令行调用情况(即运行程序的方式)决定是否将<code>PyCodeObjec</code>t写回硬盘当中(也就是直接复制到<code>.pyc</code>或<code>.pyo</code>文件中)；</li><li>之后若再次执行该脚本，则先检查本地是否有上述字节码文件。有则执行，否则重复上述步骤。</li></ol><p>在我们点击(或输入命令)运行脚本，并悠闲地喝咖啡时，虚拟机做了这么多的事情。不过，你有没有发现<code>.pyc</code>或<code>.pyo</code>文件是否生成，是取决于我们如何运行程序的。</p><h2 id="3-pyc和pyo文件的生成"><a href="#3-pyc和pyo文件的生成" class="headerlink" title="3. pyc和pyo文件的生成"></a>3. pyc和pyo文件的生成</h2><p>虚拟机也是讲究效率的。毕竟对于比较大的项目，要将<code>PyCodeObject</code>写回硬盘也是不可避免地要花些时间的，而且它又不知道你是不是也就只执行一次，之后就对刚刚跑完的脚本弃之不顾了呢。不过，它其实也有贴心的一面。比如，</p><ul><li>若你在命令行直接输入<code>python path/to/projectDir</code>(假设<code>projectDir</code>目录含有<code>__main__.py</code>文件，以及其他将要调用的模块)，那么程序运行结束后便自动为当前目录下所有的脚本生成字节码文件，并保存于本地新文件夹<code>__pycache__</code>当中。(这也有可能是<code>IDE</code>写小项目时自动生成<code>.pyc</code>文件的原因，不过问题描述略微暧昧。详情参见上面知乎问题板块)</li></ul><p>或者是，在命令行输入<code>python path/to/projectDir/__main__.py</code>，则生成除<code>__main__.py</code>外脚本的字节码文件。不过总的来说，上述这两种行为都大大缩短了项目运行前的准备时间（毕竟分工明确的程序，规模应该不会太小，复用率也不会太低。除非吃饱了撑着，搞出这么多事情</p><ul><li>模块在每次导入前总会<strong>检查其字节码文件的修改时间是否与自身的一致</strong>。若是则直接从该字节码文件读取内容，否则源模块重新导入，并在最后生成同名文件覆盖当前已有的字节码，从而完成内容的更新(详见<code>import.py</code>)。这样，就避免了修改源代码后与本地字节码文件产生冲突(当然，设计者也不会这么傻。</li></ul><ul><li><strong><code>.pyc文件</code></strong>是由<code>.py</code>文件经过编译后生成的字节码文件，其<strong>加载速度相对于之前的<code>.py</code>文件有所提高</strong>，而且还可以<strong>实现源码隐藏</strong>，以及<strong>一定程度上的反编译</strong>。比如，<code>Python3.3</code>编译生成的<code>.pyc</code>文件，<code>Python3.4</code>就别想着去运行啦！</li><li><p><strong><code>.pyo</code>文件</strong>也是<strong>优化</strong>(注意这两个字，便于后续的理解)编译后的程序(<strong>相比于<code>.pyc</code>文件更小</strong>)，也可以<strong>提高加载速度</strong>。但对于嵌入式系统，它可将所需模块编译成<code>.pyo</code>文件以<strong>减少容量</strong>。</p><p>但总的来说，作用上是几乎与原来的<code>.py</code>脚本没有区别的，也就是然并卵 (当然，并非毫无作用。比如，我个人觉得用处最大的地方就是<strong>防止别人偷看我的代码</strong>。毕竟<code>.py</code>源文件是直接以源码的形式呈现给大家的)。呃…这么说，好像又有点自相矛盾的赶脚。</p></li></ul><p><code>Python</code>选项</p><ul><li><em><code>-O</code></em>，表示<strong>优化生成.pyo字节码</strong>(这里又有优化两个字，得注意啦！)</li><li><em><code>-OO</code></em>，表示进一步<strong>移除*</strong><code>-O</code><em>选项生成的字节码文件中的<strong>文档字符串</strong>(这是在作用效果上解释的，而不是说从</em><code>-O</code>*选项得到的文件去除)</li><li><em><code>-m</code></em>，表示<strong>导入并运行指定的模块</strong></li></ul><p>对此，我们可以使用如下格式运行<code>.py</code>文件来生成<code>.pyc</code>文件(以下调用均假设<code>/path/to</code>目录含有<code>.py</code>脚本)：</p><pre><code class="lang-python">python -m py_compile /path/to/需要生成.pyc的脚本.py #若批量处理.py文件#则替换为/path/to/{需要生成.pyc的脚本1,脚本2,...}.py#或者/path/to/</code></pre><p>效果等同于下面:</p><pre><code class="lang-python">import py_compilepy_compile.compile(r&#39;/path/to/需要生成.pyc的脚本.py&#39;) #同样也可以是包含.py文件的目录路径#此处尽可能使用raw字符串，从而避免转义的麻烦。比如，这里不加“r”的话，你就得对斜杠进行转义</code></pre><h2 id="3-py-compile模块"><a href="#3-py-compile模块" class="headerlink" title="3. py_compile模块"></a>3. py_compile模块</h2><p><code>py_compile</code>是<code>Python</code>的自带模块，这里面就两个函数。其下的<code>py_compile.compile*(*file*[, *cfile*[, *dfile*[, *doraise*]]])</code>可将<code>.py</code>文件编译生成<code>.pyc</code>文件，对应的参数解释如下</p><ol><li><em><code>file</code></em>，表示需要生成<code>.pyc</code>或<code>.pyo</code>文件的<strong>源脚本名</strong>(字符串)；</li><li><em><code>cfile</code></em>，表示需要生成<code>.pyc</code>或<code>.pyo</code>文件的<strong>目标脚本名</strong>。呃…好像没有区别，也就是源脚本——-目标脚本。当然，它默认是以<code>.pyc</code>为扩展名的路径名的字符串。此外，当且仅当所使用的解释器允许编译成<code>.pyo</code>文件，才能以<code>.pyo</code>结尾。</li><li><em><code>dfile</code></em>，表示编译出错时，<strong>将报错信息中的名字<code>file</code>替换为<code>dfile</code></strong>。</li><li><em><code>doraise</code></em>，<strong>设置是否忽略异常</strong>。若为<code>True</code>，则抛出<code>PyCompileError</code>异常；否则直接将错误信息写入<code>sys.stderr</code>（什么！不知道<code>sys.stderr</code>！温馨提示：<code>sys.stderr</code>是<code>Python</code>自带的<strong>标准错误输出</strong>)</li></ol><pre><code class="lang-python">python -O -m py_compile /path/to/需要生成.pyo的脚本.py</code></pre><p>那么，有人要问了：为什么不是像生成<code>.pyc</code>文件那样采用<em><code>python -O /path/to/</code>需要生成<code>pyo</code>的脚本<code>.py</code></em>形式的调用？<br>忘记说明这一点了，很多博客以及书籍都像我上面那样解释<em><code>-O</code></em>选项的作用，但详细来解释的话是：    </p><p>-O <strong>选项，将<code>.pyc</code>文件优化(注意我一直强调的优化二字，这里就用到啦！)</strong>为<code>.pyo</code>文件，而不是将<code>.py</code>文件优化编译为<code>.pyo</code>文件。(其直接的结果是优化编译后的文件略微小于<code>.pyc</code>文件，也就是减肥了。现在，大家知道<code>.pyo</code>文件为什么小的原因了吧！)</p><h2 id="4-注意"><a href="#4-注意" class="headerlink" title="4. 注意"></a>4. 注意</h2><p>以上无论是生成<code>.pyc</code>还是<code>.pyo</code>文件，都将在当前脚本的目录下生成一个含有字节码的文件夹<strong><code>__pycache__</code></strong>。</p><p>可能还有人会问，<strong><code>pyd</code>文件</strong>又是什么，别在意，那只是<code>Python</code>的动态链接库。如果要深究，还得扯上<code>C++</code>的知识</p><p>再啰嗦一句：生成字节码的方法多了去了，不止以上这几种。比如，你们不妨试试将上面命令行调用中的<em><code>py_compile</code></em>改成<em><code>compileall</code></em>，而代码行中的<em>`py_compile.compile</em><code>改成*</code>compileall.compile_file<code>*或*</code>compileall.compile_dir`*，又或者直接使用带有编译功能的IDE生成字节码。</p><p>再再啰嗦一句：知道<code>Python</code>运行机制，并不是我们一般人所必须的。但是，了解其加速程序运行以及优化代码的设计思想，对于我们在日后构造缓存系统、如何减少不必要的运行时间，以及同步更新工作内容等问题上起到很大的借鉴作用。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>compile</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于MLE的logistic模型</title>
    <link href="/2017/09/05/%E5%9F%BA%E4%BA%8EMLE%E7%9A%84logistic%E6%A8%A1%E5%9E%8B/"/>
    <url>/2017/09/05/%E5%9F%BA%E4%BA%8EMLE%E7%9A%84logistic%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>主要介绍逻辑回归（<code>Logistic Regression</code>）和最大似然估计。逻辑回归是机器学习算法中最常用的算法，用于分类，属于有监督学习。在有监督学习中，如果因变量为有限个离散值，则预测问题就转变成分类问题，从这个角度也可以说分类问题是预测问题的一个特例。此时，通过样本集训练获得的有监督学习模型，即逻辑回归模型就称为分类模型或分类决策函数，也称为分类器（<code>Classifier</code>）。<br><a id="more"></a></p><h2 id="1-逻辑回归"><a href="#1-逻辑回归" class="headerlink" title="1. 逻辑回归"></a>1. 逻辑回归</h2><p>拟合决策边界(这里的决策边界不局限于线性，还可以是多项式，平面，曲面等)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。对于某个分类任务，如判定一封邮件是否为垃圾邮件，我们需要通过分类器预测分类结果：是（记为1）or不是（记为0）。如果我们考虑0就是“不发生”，1就是“发生”，那么我们可以将分类任务理解成估计事件发生的概率p，通过事件发生概率的大小来达到分类目的。因此，我们需要使得预测结果即概率值限定在0～1之间，很明显，如果我们仍然采用线性回归$p=f(x,\theta)=\theta^T x$作为分类器，其预测值可能会远远大于1或者远远小于0，不符合我们预期想法。所以，逻辑回归引入logistic函数，将分类器输出界定在［0，1］之间，其一般形式可表示为$f(x, \theta)=g(\theta^T x)$。那么问题来了，我们到底需要选择什么样形式的logistic函数g(z)？</p><h3 id="logistic函数的由来"><a href="#logistic函数的由来" class="headerlink" title="logistic函数的由来"></a>logistic函数的由来</h3><p>需要强调的是，我们不选用线性回归的原因是线性回归不能保证预测值的范围位于［0，1］之间。所以，针对该问题最简单的方法是移除因变量值域的限制，这样，我们就需要对概率形式做某种变换。</p><p>首先，选用发生比(the odds of experiencing an event)代替概率，发生比是事件发生概率和不发生概率之间的比值，记为$odds = \frac{p}{1-p}$，通过该变换，我们可以将［0，1］之间的任意数映射到$[0,\infty]$之间的任意实数。但是，线性回归的输出还可以是负数，我们还需要另一步变换将$[0,\infty]$的实数域映射到这个实数域R空间；</p><p>然后，在众多非线性函数中，log函数的值域为整个实数域且单调，因此，我们可以计算优势比的对数，另$\eta = log(odds)=log \frac{p}{1-p}=logit(p)$。</p><p>经过以上两步，我们可以去除分类问题对因变量值域的限制，如果概率等于0，那么优势比则为0，logit值为$-\infty$；相反，如果概率等于1，优势比为$\infty$，logit值为$\infty$。因此，logit函数可以将范围为［0，1］的概率值映射到整个实域空间，当概率值小于0.5时，logit值为负数，反之，logit值为正数。</p><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>解决了分类问题对分类器预测的因变量值域的限制，我们就可以采用线性回归对一一映射后的概率值进行线性拟合，即$logit(p)=log(\frac{p}{1-p})=\eta=f_{\theta}(x)=\theta^T \cdot x$。因为logit变换是一一映射，所以存在logit的反变换（antilogit），我们可以求得p值的解析表达式：</p><script type="math/tex; mode=display">p=antilogit(x)=logit^{\eta}=\frac{e^{\eta}}{1+e^{\eta}}=\frac{e^{\theta^T \cdot x}}{1+e^{\theta^T \cdot x}}</script><script type="math/tex; mode=display">1-p=1-antilogit(x)=1-logit^{\eta}=1-\frac{e^{\eta}}{1+e^{\eta}}=1-\frac{e^{\theta^T \cdot x}}{1+e^{\theta^T \cdot x}}＝\frac{1}{1+e^{\theta^T \cdot x}}</script><p>众所周知，上式即为logistic回归的一般表达式，其采用的logit变换一般记为sigmoid函数：</p><script type="math/tex; mode=display">g(x)=\frac{e^{\theta^T x}}{1+e^{\theta^T x}}</script><p>讲到这里，还有一个问题是<strong>为什么要选用sigmoid函数</strong>呢？为什么不选用其他函数，如probit函数？</p><p>其实，无论是sigmoid函数还是probit函数都是广义线性模型的连接函数（link function）中的一种。选用联接函数是因为，从统计学角度而言，普通线性回归模型是基于响应变量和误差项均服从正态分布的假设，且误差项具有零均值，同方差的特性。但是，例如分类任务（判断肿瘤是否为良性、判断邮件是否为垃圾邮件），其响应变量一般不服从于正态分布，其服从于二项分布，所以选用普通线性回归模型来拟合是不准确的，因为不符合假设，所以，我们需要选用广义线性模型来拟合数据，通过标准联接函数(canonical link or standard link function)来映射响应变量，如：正态分布对应于恒等式，泊松分布对应于自然对数函数，二项分布对应于logit函数（二项分布是特殊的泊松分布）。</p><h2 id="2-最大似然估计"><a href="#2-最大似然估计" class="headerlink" title="2. 最大似然估计"></a>2. 最大似然估计</h2><p>通过上述分析，我们可以获得logistic回归的表达式：</p><script type="math/tex; mode=display">P(y=1|X)=\frac{exp(\theta^T \cdot x)}{1+exp(\theta^T \cdot x)}</script><p>以及<script type="math/tex">P(y=0|X)=\frac{1}{1+exp(\theta^T \cdot x)}</script><br>其中$\theta$是未知参数。假设有一组观测样本，那么现在的任务就变成给定一组数据和一个参数待定的模型下，如何估计模型参数的问题。而这里想要讲述的最大似然估计就是估计未知参数的一种方法，最大似然估计（Maximum Likelihood Method）是建立在各样本间相互独立且样本满足随机抽样（可代表总体分布）下的估计方法，它的核心思想是如果现有样本可以代表总体，那么最大似然估计就是找到一组参数使得出现现有样本的可能性最大，即从统计学角度需要使得所有观测样本的联合概率最大化，又因为样本间是相互独立的，所以所有观测样本的联合概率可以写成各样本出现概率的连乘积，如下式：</p><script type="math/tex; mode=display">\begin{aligned}\prod^m\_{i=1}\underbrace{P(y^{(i)}=1|x^{(i)})}\_{where \, i\in m , and , y^{(i)}=1} \cdot \underbrace{P(y^{(i)}=0|x^{(i)})}\_{where \, i \in m \, and \, y^{(i)}=0}\\\\& = \prod^m\_{i=1} P(y^{(i)}=1|x^{(i)})^{y(i)} \cdot P(y^{(i)}=0|x^{(i)})^{1-y(i)}\end{aligned}</script><p>上面为什么是这样呢？当y=1的时候，后面那一项是不是没有了，那就只剩下x属于1类的概率，当y=0的时候，第一项是不是没有了，那就只剩下后面那个x属于0的概率（1减去x属于1的概率）。所以不管y是0还是1，上面得到的数，都是(x, y)出现的概率。那我们的整个样本集，也就是n个独立的样本出现的似然函数为（因为每个样本都是独立的，所以n个样本出现的概率就是他们各自出现的概率相乘）。</p><p>通过观察我们可以看出，当样本响应变量为1时，上式等于$P(y^{(i)}=1｜x^{(i)})$；当样本响应变量为0时，上式等于$P(y^{(i)}=0｜x^{(i)})$，是所有样本边际分布概率的连乘积，通常被称之为似然函数$\ell(\theta)$。</p><p>最大似然估计的目标是求得使得似然函数$\ell(\theta)$最大的参数$\theta$的组合，理论上讲我们就可以采用梯度上升算法求解该目标函数（似然函数）的极大值。</p><h3 id="为什么采用最大似然？—非凸函数"><a href="#为什么采用最大似然？—非凸函数" class="headerlink" title="为什么采用最大似然？—非凸函数"></a>为什么采用最大似然？—非凸函数</h3><p>我们知道之前线性回归可以采用梯度下降算法求解是因为线性回归的代价函数（均方误差函数）是凸函数，为碗状，而凸函数具有良好的性质（对于凸函数来说局部最小值点即为全局最小值点）使得我们一般会将非凸函数转换为凸函数进行求解。因此，最大似然估计采用自然对数变换，将似然函数转换为对数似然函数，其具体形式为：</p><script type="math/tex; mode=display">\begin{aligned}log(\ell(\theta)) & = log(\prod\_{i=1}^m P(y^{(i)}=1｜x^{(i)})^{y(i)} \cdot P(y^{(i)}=0｜x^{(i)})^{1-y(i)}) \\\\& = \sum^m \lgroup y^{(i)} log(g(x)) + (1-y^{(i)})log(1-g(x)) \rgroup\end{aligned}</script><p>相对于求解对数似然函数的最大值，我们当然可以将该目标转换为对偶问题，即求解代价函数$J(\theta)=-log(\ell(\theta))$的最小值。因此，我们定义logistic回归的代价函数为：</p><script type="math/tex; mode=display">cost=J(\theta)=-log(\ell(\theta))＝－\frac{1}{m} \sum^m\_{i=1} \lgroup y^{(i)} log(g(x)) + (1-y^{(i)})log(1-g(x)) \rgroup</script><p>从代价函数的直观表达上来看，当$y^{(i)}=1, g(x)=1$时(预测类别和真实类别相同)，$J(\theta｜x^{(i)})=0$；当$y^{(i)}=1, g(x) \rightarrow 0$时(预测类别和真实类别相反)，$J(\theta｜x^{(i)}) \rightarrow \infty$（注意对数函数前有个负号）。这意味着，当预测结果和真实结果越接近时，预测产生的代价越小，当预测结果和真实结果完全相反时，预测会产生很大的惩罚。该理论同样适用于$y^{(i)}=0$的情况。</p><p>实际上，最小二乘估计是最大似然估计的一种，有心的人还记得上面提到过的线性回归必须满足的条件，即误差项均服从正态分布的假设，如果线性回归记为$y=\theta x + \epsilon$的话，对于误差函数ϵϵ，其服从正态分布，$\epsilon \sim N(0, \sigma^2)$，因此利用正态分布的性质，我们可以得到$y-\theta x \sim N(0, \sigma^2) \Rightarrow y \sim N(\theta x, \sigma^2)$。</p><p>因此，根据极大似然估计的定义，我们要获得产生样本yy可能性最大的一组参数θθ，因此，似然函数可写为：</p><script type="math/tex; mode=display">\ell(\theta)=\prod^m\_{i=1} \frac{1}{\sqrt{2\pi}\sigma} exp(- \frac{(y^{(i)}-\theta x)^2}{2 \sigma})</script><p>与logistic回归类似，我们仍然将似然函数变换为对数似然函数求解极值，此时，</p><script type="math/tex; mode=display">log(\ell(\theta))=mlog(\frac{1}{\sqrt{2\pi}}) + \sum^m\_{i=1} -\frac{(y^{(i)}-\theta x)^2}{2 \sigma}</script><p>综上所述，要让$log(\ell(\theta))$最大，我们需要让$\sum^m_{i=1}(y^{(i)}-\theta x)^2$最小，该式即为我们经常提及的线性回归的代价函数，所以，当线性回归的求解过程也利用最大似然估计的思想。</p><h2 id="3-最大似然估计求解"><a href="#3-最大似然估计求解" class="headerlink" title="3. 最大似然估计求解"></a>3. 最大似然估计求解</h2><p>有了代价函数，我们自然可以选用梯度下降算法或者其他优化算法对目标函数进行求解。在R中，我们也可采用optim函数，利用模拟退火算法或者单纯形算法求解。对于梯度下降算法，我们可以通过求解目标函数的一阶偏导数获得梯度，为：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial J}{\partial \theta} & = -\sum\_{i=1}^m ( \frac{y}{g(x)} \cdot g’(x) - \frac{1-y}{1-g(x)}\cdot g’(x)) \\\\& = -\sum^m( \frac{y-yg(x)-g(x)+yg(x)}{g(x)(1-g(x))} \cdot g’(x) )  \\\\& = -\sum^m \frac{y-g(x)}{g(x)(1-g(x))} \cdot (-\frac{x e^{\theta x}}{(1+e^{\theta x})^2}) \\\\& = -\sum^m \frac{g(x)-y}{g(x)(1-g(x))} \cdot \frac{e^{\theta x}}{1+e^{\theta x}} \cdot \frac{1}{1+e^{\theta x}} \cdot x \\\\& = -\sum^m \frac{g(x)-y}{g(x)(1-g(x))} \cdot g(x) \cdot (1-g(x)) \cdot x \\\\& = -\sum^m (g(x)-y) \cdot x \\\\\end{aligned}</script><p>因此，梯度更新的表达式为：</p><script type="math/tex; mode=display">\theta := \alpha \sum^m\_{i=1} (g(x)-y) \ cdot x</script><p>至此，logistic回归和最大似然估计原理部分就over。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="http://blog.csdn.net/han_xiaoyang/article/details/49332321" target="_blank" rel="noopener">机器学习系列(2)_从初等数学视角解读逻辑回归</a></li><li><a href="https://chenrudan.github.io/blog/2016/01/09/logisticregression.html" target="_blank" rel="noopener">浅析Logistic Regression</a></li><li><a href="http://wenjunoy.com/2016/01/logistic-sigmoid-function.html" target="_blank" rel="noopener">Logistic函数（sigmoid函数）</a></li><li><a href="http://blog.csdn.net/zouxy09/article/details/20319673" target="_blank" rel="noopener">机器学习算法与Python实践之（七）逻辑回归（Logistic Regression）</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MLE</tag>
      
      <tag>logistic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>spark中job运行过程详解和job,stage,task解析</title>
    <link href="/2017/08/30/spark%E4%B8%ADjob%E8%BF%90%E8%A1%8C%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3%E5%92%8Cjob-stage-task%E8%A7%A3%E6%9E%90/"/>
    <url>/2017/08/30/spark%E4%B8%ADjob%E8%BF%90%E8%A1%8C%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3%E5%92%8Cjob-stage-task%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>了解了spark的基础后一直想清楚在spark整个执行过程。在spark客户端提交任务后，从<code>job，stage，task</code>，在到<code>worker</code>上执行，整过流程必须清楚，对后续的调优，spark中job运行原理都有很大的帮助。下面对spark中job运行过程进行详细解析，整个过程如下图所示。</p><p><img src="/uploads/spark运行流程解析.png" srcset="/img/loading.gif" alt="enter description here"><br><a id="more"></a></p><h2 id="Job运行流程"><a href="#Job运行流程" class="headerlink" title="Job运行流程"></a>Job运行流程</h2><p>在client端提交Job，而Job会生成一个<code>sparkcontext</code>对象，该对象向集群申请Executor资源，将job分解成可并行的task，然后将task分发到各个<code>executor</code>上执行，执行完成后将<code>executor</code>的结果全部返回到<code>sparkcontext</code>中。下面对整个过程进行详细说明。</p><h3 id="Job到DAGScheduler过程"><a href="#Job到DAGScheduler过程" class="headerlink" title="Job到DAGScheduler过程"></a>Job到DAGScheduler过程</h3><p>Job的概念：包含很多<code>task</code>的并行计算，可以认为是<code>Spark RDD</code> 里面的<code>action</code>，每个<code>action</code>的触发会生成一个job。而这个过程中主要是将RDD中的依赖关系形成DAG，并将DAG传递到<code>DAGScheduler</code>中。</p><h3 id="DAGScheduler对DAG的处理"><a href="#DAGScheduler对DAG的处理" class="headerlink" title="DAGScheduler对DAG的处理"></a>DAGScheduler对DAG的处理</h3><p><code>DAGSchedule</code>对DAG进行<code>stage</code>的划分，并在每个<code>Stage</code>内化为出一系列可并行处理的<code>task</code>,再将<code>task</code>传递到下一层; <code>DAGScheduler</code>会记录哪个RDD或者<code>Stage</code>会被物化，从而寻找一个最佳调度方案。将<code>TaskSet</code>提交给<code>Task Tracker</code>。重新提交输出<code>lost</code>的<code>Stage</code>。在这个过程中涉及到了很重要的环节—<code>stage</code>的划分,后面会着重进行说明。</p><h3 id="task运行"><a href="#task运行" class="headerlink" title="task运行"></a>task运行</h3><p>在1.2中生成了一系列的<code>task</code>，<code>task</code>和底层的资源交互，而这个资源交互的协调人就是<code>TaskScheduler</code>，<code>taskset</code>提交到<code>TaskScheduler</code>等待调度执行。<code>TaskScheduler</code>提供了对外的接口，<code>TaskScheduler</code>对接的是不同的<code>SchedulerBackend</code>的实现(比如<code>mesos</code>，<code>yarn</code>，<code>standalone</code>)，都是通过<code>TaskScheduler</code>来进行协作。<code>TaskScheduler</code>在初始化后会启动<code>SchedulerBackend</code>(yarn, mesos等)，它(<code>SchedulerBackend</code>)负责跟外界打交道，接收<code>Executor</code>的注册信息，并维护<code>Executor</code>的状态，所以说<code>SchedulerBackend</code>是管资源(<code>worker</code>)的，同时它在启动后会定期地去“询问”<code>TaskScheduler</code>有没有任务要运行，<code>TaskScheduler</code>在<code>SchedulerBackend</code>“问”它的时候，会从调度队列中按照指定的调度策略选择<code>TaskSetManager</code>去调度运行。</p><p><code>TaskScheduler</code>支持两种调度策略，一种是<code>FIFO</code>，也是默认的调度策略，另一种是<code>FAIR</code>。这两种调度策略可以通过资料去查具体的实现方式。</p><p>Spark实现了三种不同的<code>TaskScheduler</code>，包括<code>LocalSheduler</code>、<code>ClusterScheduler</code>和<code>MesosScheduler</code>。<code>LocalSheduler</code>是一个在本地执行的线程池，<code>DAGScheduler</code>提交的所有task会在线程池中被执行，并将结果返回给<code>DAGScheduler</code>。<code>TaskScheduler</code>的启动会伴随<code>SparkDeploySchedulerBackend</code>的启动，而<code>backend</code>会将自己分为两个角色：首先是<code>driver</code>，<code>driver</code>是一个<code>local</code>运行的<code>actor</code>，负责与<code>remote</code>的<code>executor</code>进行通行，提交任务，控制<code>executor</code>；其次是<code>StandaloneExecutorBackend</code>，Spark会在每一个slave node上启动一个<code>StandaloneExecutorBackend</code>进程，负责执行任务，返回执行结果。具体过程如下图<code>TaskScheduler</code>部分所示：</p><p><img src="/uploads/TaskScheduler详解.png" srcset="/img/loading.gif" alt="TaskScheduler详解"></p><p>上图中是一个完整的spark任务调度过程中<code>ApplicationMaster</code>、<code>Driver</code>以及<code>Executor</code>的交互过程。<code>Driver</code>初始化<code>SparkContext</code>过程中，会分别初始化<code>DAGScheduler</code>、<code>TaskScheduler</code>、<code>SchedulerBackend</code>以及<code>HeartbeatReceiver</code>，并启动<code>SchedulerBackend</code>以及<code>HeartbeatReceiver</code>。<code>SchedulerBackend</code>通过<code>ApplicationMaster</code>申请资源，并不断从<code>TaskScheduler</code>中拿到合适的<code>Task</code>分发到<code>Executor</code>执行。<code>HeartbeatReceiver</code>负责接收<code>Executor</code>的心跳信息，监控<code>Executor</code>的存活状况，并通知到<code>TaskScheduler</code>。</p><p>上面对spark中任务执行过程有了大概的了解之后，现在可以对其中部分的细节进行了解。如job，task，stage的划分等。其中stage的划分是非常重要的。</p><h2 id="job-stage和task理解"><a href="#job-stage和task理解" class="headerlink" title="job,stage和task理解"></a>job,stage和task理解</h2><h3 id="job的理解"><a href="#job的理解" class="headerlink" title="job的理解"></a>job的理解</h3><p><code>job</code>是<code>rdd</code>的<code>action</code>所触发的一个动作，当<code>rdd</code>执行<code>action</code>的时候即触发一个<code>job</code>。在触发<code>job</code>后，<code>RDD</code>的<code>runJob</code>则在<code>SparkContext</code>的<code>runJob</code>中调用，<code>SparkContext</code>的<code>runJo</code>b底层会调用<code>DAGScheduler</code>的<code>runJob</code>方法。在<code>DAGScheduler</code>会将每个<code>job</code>划分为多个<code>stage</code>，并分析他们之间的关系，会寻找最优的运行策略，再进行下一步操作。</p><p>另外<code>job</code>也分为不含有<code>shuffle</code>和<code>reduce</code>，含有<code>shuffle</code>和<code>reduce</code>的<code>job</code>,对于两种<code>job</code>，第一种<code>job</code>只会产生一个<code>finalStage</code>，而第二种<code>job</code>会产生<code>finalStage</code>和<code>mapStage</code>。</p><h3 id="stage的划分"><a href="#stage的划分" class="headerlink" title="stage的划分"></a>stage的划分</h3><p>一个job会被分成1组或1组以上task，其中每组task就是一个stage，就像map stage和reduce stage一样。另外一个说法：stage是job的组成单元，一个job会被切分为一个或者多个stage。那么stage是怎么划分的呢？</p><p>官方的说明：调度器从<code>DAG</code>图末端出发，逆向遍历整个依赖关系链，遇到<code>ShuffleDependency</code>（宽依赖关系的一种叫法）就断开，遇到<code>NarrowDependency</code>就将其加入到当前<code>stage</code>。从触发<code>action</code>操作的<code>RDD</code>往前倒推，如果发现了某个<code>RDD</code>是宽依赖，那么就会将宽依赖的<code>RDD</code>创建为一个新的<code>stage</code>。那个<code>RDD</code>是新的<code>stage</code>中最后一个<code>RDD</code>，这样依次遍历，知道所有的<code>RDD</code>全部遍历。结合官方的图来解释。</p><p><img src="/uploads/RDD中stage划分.jpg" srcset="/img/loading.gif" alt="RDD中stage划分"></p><p>从图中可以看出在宽依赖关系处就会断开依赖链，划分<code>stage</code>，宽依赖的<code>RDD</code>是<code>stage</code>中的最后一个<code>RDD</code>。这里的<code>stage1</code>不需要计算，只需要计算<code>stage2</code>和<code>stage3</code>，就可以完成整个<code>Job</code>。</p><h4 id="宽依赖和窄依赖的定义"><a href="#宽依赖和窄依赖的定义" class="headerlink" title="宽依赖和窄依赖的定义"></a>宽依赖和窄依赖的定义</h4><p>RDD每一次transformation都会生成一个新的RDD，这样就会建立RDD之间的前后依赖关系，在Spark中，依赖关系被定义为两种类型，分别是窄依赖和宽依赖：</p><ul><li>窄依赖：父RDD的分区最多只会被子RDD的一个分区使用</li><li>宽依赖：父RDD的一个分区会被子RDD的多个分区使用</li></ul><p>下图中为宽依赖和窄依赖的官方说明：</p><p><img src="/uploads/宽依赖和窄依赖.jpg" srcset="/img/loading.gif" alt="宽依赖和窄依赖"></p><p>下面根据stage划分的算法对<code>wordcount</code>进行<code>stage</code>划分示意图：</p><p><img src="/uploads/wordcount的stage划分过程.jpg" srcset="/img/loading.gif" alt="wordcount的stage划分过程"></p><h3 id="task的理解"><a href="#task的理解" class="headerlink" title="task的理解"></a>task的理解</h3><p><code>task</code>实在<code>stage</code>的基础上进行，计算是以<code>partition</code>为单元，<code>task</code>的数量和<code>partition</code>的数据相同。<code>partition</code>的划分依据很多，可以根据<code>key</code>划分，可以自定义，以文件的<code>block</code>来划分等。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://ieevee.com/tech/2016/07/11/spark-scheduler.html" target="_blank" rel="noopener">Task调度算法，FIFO还是FAIR</a></li><li><a href="https://litaotao.github.io/deep-into-spark-exection-model" target="_blank" rel="noopener">深入研究 spark 运行原理之 job, stage, task</a></li><li><a href="http://sharkdtu.com/posts/spark-scheduler.html" target="_blank" rel="noopener">Spark Scheduler内部原理剖析</a></li><li><a href="http://jerryshao.me/architecture/2013/04/21/Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B-scheduler%E6%A8%A1%E5%9D%97/" target="_blank" rel="noopener">Spark源码分析之-scheduler模块</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>spark</tag>
      
      <tag>stage</tag>
      
      <tag>job</tag>
      
      <tag>task</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用scikit-learn来分析模型欠拟合和过拟合问题</title>
    <link href="/2017/08/15/%E5%88%A9%E7%94%A8scikit-learn%E6%9D%A5%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/"/>
    <url>/2017/08/15/%E5%88%A9%E7%94%A8scikit-learn%E6%9D%A5%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大、要么是方差比较大。换句话说，出现的情况要么是欠拟合、要么是过拟合问题。在建模的过程中经常会遇到模型拟合度不够或者过拟合，但是怎么去分析模型是否存在欠拟合和过拟合呢？一般通过模型的学习曲线和评估曲线来分析欠拟合和过拟合。并在实际应用中针对性的提出解决方案。下图为理论上识别欠拟合和过拟合的方法，如下：<br><img src="/uploads/poly_nihe.png" srcset="/img/loading.gif" alt="polynline.png"></p><h3 id="1-使用学习曲线判别偏差和方差问题"><a href="#1-使用学习曲线判别偏差和方差问题" class="headerlink" title="1. 使用学习曲线判别偏差和方差问题"></a>1. 使用学习曲线判别偏差和方差问题</h3><p>如果一个模型过于复杂，则参数太多，可能导致模型过拟合。解决过拟合问题，可以通过增加训练数据集，降低模型复杂度，交叉检验等。根据上图中，我们可以逐渐增加模型训练集，观察模型的偏差和方差变化，确定这种方式是否有效。<br><img src="/uploads/bias_variance.png" srcset="/img/loading.gif" alt="biasvariance.png"><br>上图中左上角的情况，模型偏差较大，训练和测试效果曲线都很差，精度太低，可能是欠拟合，解决欠拟合问题：一般通过增加模型参数，增加特征或者减小正则项等。</p><p>下面通过<code>python</code>中<code>scikit-learn</code>的学习曲线来对欠拟合和过拟合进行说明，主要通过不同的数据量来逐步分析模型效果，看欠拟合和过拟合的情况。<br><a id="more"></a></p><pre><code class="lang-python">import numpy as npimport randomfrom sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import learning_curve, validation_curvefrom matplotlib import pyplot as pltfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import f1_score, make_scorerload_data = load_iris()X, Y = load_data.data, load_data.targetindex = range(150)random.shuffle(index)random_x = X[index, :]random_y = Y[index]estimator = LogisticRegression(penalty=&#39;l2&#39;, C=1.0, fit_intercept=True)f1 = make_scorer(f1_score)train_size, train_scores, test_scores = learning_curve(estimator, X=random_x, y=random_y,                                                       train_sizes=np.linspace(0.1, 1.0, 10), cv=10,                                                        scoring=&#39;accuracy&#39;)train_mean = np.mean(train_scores, axis=1)train_std = np.std(train_scores, axis=1)test_mean = np.mean(test_scores, axis=1)test_std = np.std(test_scores, axis=1)fig = plt.figure(figsize=(9, 6))ax = fig.add_subplot(111)ax.plot(train_size, train_mean, linestyle=&#39;--&#39;, color=&quot;green&quot;, marker=&#39;o&#39;, markersize=5,label=&quot;train line&quot;)ax.fill_between(train_size, train_mean+train_std, train_mean-train_std, alpha=0.3, color=&quot;green&quot;)ax.plot(train_size, test_mean, linestyle=&#39;-&#39;, marker=&#39;s&#39;, markersize=5, color=&quot;blue&quot;, label=&quot;test line&quot;)ax.fill_between(train_size, test_mean+test_std, test_mean-test_std, alpha=0.3, color=&quot;blue&quot;)plt.legend(loc=&quot;lower right&quot;)plt.show()</code></pre><p><img src="/uploads/output_5_0.png" srcset="/img/loading.gif" alt="png"></p><p><code>learning_curve</code>中的<code>train_sizes</code>参数控制产生学习曲线的训练样本的绝对/相对数量，此处，我们设置的<code>train_sizes=np.linspace(0.1, 1.0, 10)</code>，将训练集大小划分为10个相等的区间。<code>learning_curve</code>默认使用分层k折交叉验证计算交叉验证的准确率，我们通过cv设置k。从上图中可以看出模型的效果非常好，在数据量达到80以上时，训练和测试的准确率基本重合。因此基本是没有过拟合情况，而模型的整个准确率在95%以上，因此模型也不存在欠拟合的情况。</p><h3 id="2-用验证曲线解决过拟合和欠拟合"><a href="#2-用验证曲线解决过拟合和欠拟合" class="headerlink" title="2. 用验证曲线解决过拟合和欠拟合"></a>2. 用验证曲线解决过拟合和欠拟合</h3><p>验证曲线可以调整模型的参数，从而解决过拟合或者欠拟合问题，验证曲线和学习曲线近似，但是意义不一样，可以通过<code>validation_curve</code>来实现。还能对模型进行优化，加强模型的精度以及泛化能力。下图(借用网上一张图表示)中则为模型的随着参数的不同，模型效果变化。</p><ol><li>当<code>cross validation error (Jcv)</code> 跟<code>training error(Jtrain)</code>差不多，且<code>Jtrain</code>较大时，当前模型更可能存在欠拟合。</li><li>当<code>Jcv &gt;&gt; Jtrain</code>且<code>Jtrain</code>较小时，当前模型更可能存在过拟合<br><img src="/uploads/param_bias_variance.png" srcset="/img/loading.gif" alt="parambiasvariance.png"></li></ol><pre><code class="lang-python">param_c = [0.001, 0.01, 0.1, 0.5, 0.8, 1, 2, 3, 10]train_scores, test_scores = validation_curve(estimator, X=random_x, y=random_y, param_name=&#39;C&#39;, param_range=param_c, cv=10,                  scoring=&#39;accuracy&#39;)train_mean = train_scores.mean(axis=1)train_std = train_scores.std(axis=1)test_mean = test_scores.mean(axis=1)test_std = test_scores.std(axis=1)fig = plt.figure(figsize=(9, 6))ax = fig.add_subplot(111)ax.plot(param_c, train_mean, marker=&#39;o&#39;, markersize=5, color=&#39;blue&#39;, label=&quot;train_acc&quot;)ax.fill_between(param_c, train_mean+train_std, train_mean-train_std, color=&#39;blue&#39;, alpha=0.3)ax.plot(param_c, test_mean, marker=&#39;*&#39;, markersize=5, color=&#39;green&#39;, label=&quot;test_acc&quot;)ax.fill_between(param_c, test_mean+test_std, test_mean-test_std, color=&#39;green&#39;, alpha=0.3)plt.bar(left=param_c, height=10*(train_mean-test_mean), width=0.2,         color=&#39;0.2&#39;, yerr=test_std, align=&#39;center&#39;)plt.xticks(map(lambda x: round(x, 1), param_c))plt.grid()plt.legend(loc=&quot;center right&quot;)plt.show()</code></pre><p><img src="/uploads/output_8_0.png" srcset="/img/loading.gif" alt="png"></p><h3 id="3-参考文档"><a href="#3-参考文档" class="headerlink" title="3. 参考文档"></a>3. 参考文档</h3><p><a href="http://studyai.site/2016/10/24/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AD%E5%91%A8%20%282%29%E5%81%8F%E5%B7%AEVS%E6%96%B9%E5%B7%AE/" target="_blank" rel="noopener">斯坦福机器学习课程 第六周 (2)偏差VS方差</a><br><a href="http://studyai.site/2016/09/04/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B8%89%E5%91%A8%20%284%29%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%9A%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">斯坦福机器学习课程 第三周 (4)正则化：解决过拟合问题</a><br><a href="http://www.cnblogs.com/instant7/p/4135416.html" target="_blank" rel="noopener">欠拟合还是过拟合的判断</a><br><a href="https://ljalphabeta.gitbooks.io/python-/content/debugging.html" target="_blank" rel="noopener">使用学习曲线和验证曲线 调试算法</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>scikit-learn</tag>
      
      <tag>overfit</tag>
      
      <tag>underfit</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>对梯度下降算法的思考</title>
    <link href="/2017/08/13/%E5%AF%B9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%9D%E8%80%83/"/>
    <url>/2017/08/13/%E5%AF%B9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%9D%E8%80%83/</url>
    
    <content type="html"><![CDATA[<p>梯度下降算法是最优化理论中的最常用方法，目前在深度学习以及机器学习算法中广泛应用，主要为了求目标函数极值的最优解。其中原理很简单沿着目标函数梯度下降的方向搜索极小值。参数按照梯度更新的公式如下：</p><script type="math/tex; mode=display">\theta := \theta - {\alpha} \cdot {\nabla}\_{\theta} {J(\theta)}</script><p>其中$\alpha$为学习率或者衰减因子，每次在更新时都会用$\alpha$为步长来更新参数。因此$\alpha$的取值比较讲究。</p><ol><li>学习率较小时，收敛到极值的速度较慢。</li><li>学习率较大时，容易在搜索过程中发生震荡。<br>如下图所示：<br><img src="/uploads/lr_diff.jpg" srcset="/img/loading.gif" alt="lr取值不同时的收敛情况"><br>上图中，当lr过大时就会出现来回震荡，主要是因为lr取值过大，而在参数更新公式中梯度可正可负，因此导致来回震动。而lr过小时，收敛会逐渐减慢。因此有很多改进的梯度下降法对学习率进行改进，如冲量的梯度下降法。还有学习率衰减因子。如:<script type="math/tex; mode=display">lr\_{i} = lr\_{0} \cdot \frac {1} {1 + \alpha \cdot i}</script>上面的公式中lr会随着迭代的次数逐渐减小。$lr_{0}$为初始学习率，$\alpha$为0-1的一个常数。$\alpha$越小，学习率衰减地越慢，当$\alpha$=0时，学习率保持不变。$\alpha$越大，学习率衰减地越快，当$\alpha$=1时，学习率衰减最快。</li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>梯度下降</tag>
      
      <tag>SGD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>信息论中KL散度详解</title>
    <link href="/2017/07/26/%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%ADKL%E6%95%A3%E5%BA%A6%E8%AF%A6%E8%A7%A3/"/>
    <url>/2017/07/26/%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%ADKL%E6%95%A3%E5%BA%A6%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>在信息论中，对信息的度量主要是通过香农熵，自信息等。而本文中的KL散度(<code>Kullback-Leible</code>散度)则是对信息之间的度量或者两个分布之间的距离的度量，又称为交叉熵，相对熵。</p><h3 id="1-KL散度的定义"><a href="#1-KL散度的定义" class="headerlink" title="1. KL散度的定义"></a>1. KL散度的定义</h3><p>设$p(x)$和$q(x)$是$x$取值的两个概率分布，一般$p(x)$和$q(x)$为密度函数则$p$对$q$的相对熵为：</p><script type="math/tex; mode=display">DL(p||q)=\sum{_{i=1}^{n}} p(x) \log{\frac{p(x)}{q(x)}}</script><p>在一定程度上，KL散度可以度量两个随机变量的距离。KL散度是两个概率分布P和Q差别的非对称性的度量。KL散度是用来度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。<a id="more"></a></p><h3 id="2-KL散度的性质"><a href="#2-KL散度的性质" class="headerlink" title="2. KL散度的性质"></a>2. KL散度的性质</h3><ul><li>虽然KL散度似乎是一个距离或者度量，但是在严格的意义上讲又不是距离的度量，因为他们是不对称的，因此第一个性质就是不对称性，但是可以将公式转换为对称的公式，如下：<script type="math/tex; mode=display">DL(p||q) \ne DL(q||p)</script><script type="math/tex; mode=display">D(p, q) = \frac {DL(p||q + DL(q||p} {2}</script></li><li>KL散度的值始终是非负的:<script type="math/tex; mode=display">DL(p||q) \ge 0</script></li></ul><h3 id="3-KL散度的应用"><a href="#3-KL散度的应用" class="headerlink" title="3. KL散度的应用"></a>3. KL散度的应用</h3><ol><li>相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。所以相对熵（KL散度）可以用于比较文本的相似度，先统计出词的频率，然后计算KL散度就行了。</li><li>在多指标系统评估中，指标权重分配是一个重点和难点，通过相对熵可以处理。</li><li>在用户画像系统中可以进行应用</li><li>推荐系统</li><li>T-SNE降维</li><li>EM算法(最大期望算法)</li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>KL散度</tag>
      
      <tag>机器学习</tag>
      
      <tag>距离</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>spark中遇到的问题汇总</title>
    <link href="/2017/07/24/spark%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"/>
    <url>/2017/07/24/spark%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<ol><li><strong>出现<code>“org.apache.spark.SparkException: Task not serializable”</code>这个错误</strong></li></ol><p>在编写<code>spark</code>程序是出现了<code>org.apache.spark.SparkException: Task not serializable</code>错误，由于在操作<code>RDD</code>算子<code>map</code>中使用到了类中定义的其他函数或者定义的其他变量，而引发的未序列化问题。在实际开发过程中难免会引用定义的其他函数或者变量进行操作。为了解决为序列化的问题需要对类进行序列化处理。下面的代码是未序列化之前：</p><pre><code class="lang-scala">case class TypeMatched(label: Boolean, features:Array[Int])class loadData {    /*    * Created by Miller on 17-7-17.    * Load sandbox&#39;s bson data from location and transform to rdd type    * Use spark analysis data and static data    * */    val dataPath: String = &quot;/opt/Project/spark_project/data/sbx_data/text_data_by_spark.csv&quot;    val sc: SparkContext = new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;LoadSandboxData&quot;))    // This method use to generate RDD    def toRDD: RDD[Array[Int]] = {        sc.textFile(dataPath).map(line =&gt; line.split(&quot;,&quot;).map(x =&gt; x.toInt))    }    private def IntToBoolean(s: Int): Boolean = if (s &gt; 0) true else false    def toTypeData(rdds: RDD[Array[Int]]) = {        rdds.map(array =&gt; {            val _label_ = IntToBoolean(array(0))            val _features_ = array.slice(0, array.length)            TypeMatched(_label_, _features_)        })    }}</code></pre><p><a id="more"></a>在运行是会出现错误：<br><code>Exception in thread &quot;main&quot; org.apache.spark.SparkException: Task not serializable</code><br>解决的方法：<br>(1).将类进行序列化,使用<code>extends Serializable</code>即可.<br>(2).若类中有不能序列化的变量或者函数需要进行特殊处理:<code>@transent</code><br>修改后的代码如下:</p><pre><code class="lang-scala">class loadData extends Serializable {    /*    * Created by Miller on 17-7-17.    * Load sandbox&#39;s bson data from location and transform to rdd type    * Use spark analysis data and static data    * */    val dataPath: String = &quot;/opt/Project/spark_project/data/sbx_data/text_data_by_spark.csv&quot;    @transient    val sc: SparkContext = new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;LoadSandboxData&quot;))    // This method use to generate RDD    def toRDD: RDD[Array[Int]] = {        sc.textFile(dataPath).map(line =&gt; line.split(&quot;,&quot;).map(x =&gt; x.toInt))    }    private def IntToBoolean(s: Int): Boolean = if (s &gt; 0) true else false    def toTypeData(rdds: RDD[Array[Int]]) = {        rdds.map(array =&gt; {            val _label_ = IntToBoolean(array(0))            val _features_ = array.slice(0, array.length)            TypeMatched(_label_, _features_)        })    }}</code></pre><p>由于<code>spark</code>中<code>SparkContext</code>和<code>SparkConf</code>都是不能序列化的变量，因此需要对其标注为不需要进行序列化<code>(@transient)</code>.在处理之后代码能顺利运行了。得到结论：由于<code>Spark</code>程序中的<code>map</code>、<code>filter</code>等算子内部引用了类成员函数或变量导致该类所有成员都需要支持序列化，又由于该类某些成员变量不支持序列化，最终引发<code>Task</code>无法序列化问题。相反地，对类中那些不支持序列化问题的成员变量标注后，使得整个类能够正常序列化，最终消除<code>Task</code>未序列化问题。<br>另外还有一种处理的方式就是将要引用的函数或者变量定义到一个<code>object</code>类中，就像<code>java</code>中的<code>static</code>类。这样也可以不做序列化处理。下面可以修改成这样的代码进行测试：</p><pre><code class="lang-scala">object utilTool {    def IntToBoolean(s: Int): Boolean = if (s &gt; 0) true else false}class loadData {    /*    * Created by Miller on 17-7-17.    * Load sandbox&#39;s bson data from location and transform to rdd type    * Use spark analysis data and static data    * */    val dataPath: String = &quot;/opt/Project/spark_project/data/sbx_data/text_data_by_spark.csv&quot;    val sc: SparkContext = new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;LoadSandboxData&quot;))    // This method use to generate RDD    def toRDD: RDD[Array[Int]] = {        sc.textFile(dataPath).map(line =&gt; line.split(&quot;,&quot;).map(x =&gt; x.toInt))    }    private def IntToBoolean(s: Int): Boolean = if (s &gt; 0) true else false    def toTypeData(rdds: RDD[Array[Int]]) = {        rdds.map(array =&gt; {            val _label_ = utilTool.IntToBoolean(array(0))  // utilTool.IntToBoolean            val _features_ = array.slice(0, array.length)            TypeMatched(_label_, _features_)        })    }}</code></pre><p>解决办法和编程建议：<br>(1)在<code>map</code>或者<code>filter</code>操作中不要引用类(一般是当前类)成员。如果函数可以独立出来，可以写在<code>object</code>类中，这样不需要进行序列化处理。<br>(2)如果不能避免上面1中的情况，就必须要做序列化处理：对类进行序列化，类中不能序列化的成员需要进行特殊标记处理，使得整个类能正常的序列化。</p><ol><li><strong><code>Spark</code>中对象的序列化</strong></li></ol><p>在<code>RDD</code>的<code>API</code>里所引用的所有对象，都必须是可序列化的，因为RDD分布在多台机器是，代码和所引用的对象会序列化，然后复制到多台机器，所以凡是被引用的数据，都必须是可序列化的。例如，如下代码，就会报<code>java.lang.NotSerializableException: scala.util.Random</code> 异常：</p><pre><code class="lang-scala">val rnd = new Random()rdd.mapPartition { x=&gt;  // ...  val i = rnd.nextInt(10)  // ...}</code></pre><p>因为<code>mapPartition</code>里引用了<code>rnd</code>, 而<code>Random</code>对象没有继承自<code>Serialize</code>，是不可序列化的，所以会报异常。把<code>val rnd = new Random()</code>移动到 <code>mapPartion()</code>里面才行。<br>在一个<code>RDD</code>的<code>api</code>里，不能引用另一个<code>RDD</code>。<code>RDD</code>是不可序列化的，在一个<code>RDD</code>的<code>api</code>里，不能引用另一个<code>RDD</code>。如果你在一个<code>RDD</code>的<code>API</code>里，例如<code>map()</code>里，引用了另一个<code>RDD</code>，编译虽然会通过，但是运行会出现<code>java.lang.NullPointerException</code></p><ol><li><strong>在IDEA中写scala时当两个类分别在两个文件中，引用时会出现错误：<code>Error:not found:type xxxxx</code></strong></li></ol><p>当在项目的代码<code>root</code>目录下创建两个类时，其中一个类去调用另外一个类时出现了错误：<code>error:not found type xxxxx</code>. 通过谷歌发现下很多人也遇到了同样的问题。很多处理的方式都最终放入同一个<code>scala</code>文件中。具体原因也不是清楚。通过多番测试解决方案如下:<br><img src="/uploads/tree.png" srcset="/img/loading.gif" alt="项目结构"><br>(1)新建一个包，在包中将创建两个文件。其他代码和之前的代码一致，只需要在两个文件头中加上<code>package</code> 路径<br>原因分析：当将两个<code>scala</code>文件放入到根目录下(如上图中的<code>scala</code>目录下)就会出现上述的错误。当放入一个包内就不会出现这样的错误。而在<code>eclipse</code>中，在根目录创建文件时默认是放入到了一个<code>default package</code>的包下，因此也不会报错。因此可能是因为包和根目录的区别导致这个错误。</p>]]></content>
    
    
    <categories>
      
      <category>spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>spark</tag>
      
      <tag>RDD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>abc中的元类和six模块配合使用</title>
    <link href="/2017/07/20/abc%E4%B8%AD%E7%9A%84%E5%85%83%E7%B1%BB%E5%92%8Csix%E6%A8%A1%E5%9D%97%E9%85%8D%E5%90%88%E4%BD%BF%E7%94%A8/"/>
    <url>/2017/07/20/abc%E4%B8%AD%E7%9A%84%E5%85%83%E7%B1%BB%E5%92%8Csix%E6%A8%A1%E5%9D%97%E9%85%8D%E5%90%88%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p><code>metaclass</code>是类的类或者元类，秉承<code>Python</code>一切皆对象的理念，<code>Python</code>中的类也是一类对象，<code>metaclass</code>的实例就是类(<code>class</code>)，自己写<code>metaclass</code>时需要让其继承自<code>type</code>对象。关于<code>metaclass</code>的介绍，可以看<code>python</code>学习手册中的元类概念部分。<br><code>ABC</code>（抽象基类），主要定义了基本类和最基本的抽象方法，可以为子类定义共有的<code>API</code>，不需要具体实现。<code>abc</code>模块，<code>Python</code>对于<code>ABC</code>的支持模块，定义了一个特殊的<code>metaclass-ABCMeta</code> 还有一些装饰器— <code>@abstractmethod</code>和<code>@abstarctproperty</code> 。<code>abc.ABCMeta</code> 是一个<code>metaclass</code>，用于在<code>Python</code>程序中创建抽象基类。<a id="more"></a></p><p>抽象基类可以不实现具体的方法(也可以实现)，可以留给子类去实现。这类方法在抽象基类中是通过 <code>@abstractmethod</code>去实现的。抽象基类可以被子类直接继承，也可以将其他的类”注册“<code>(register)</code>到其门下当虚拟子类，虚拟子类的好处是你实现的第三方子类不需要直接继承自基类但是仍然可以声称自己子类中的方法实现了基类规定的接口<code>issubclass(), issubinstance()</code>！虚拟子类是通过调用<code>metaclass</code>是 <code>abc.ABCMeta</code> 的抽象基类的 <code>register</code>方法注册到抽象基类门下的，可以实现抽象基类中的部分<code>API</code>接口，也可以根本不实现，但是<code>issubclass(), issubinstance()</code>进行判断时仍然返回真值。</p><p>直接继承抽象基类的子类就没有这么灵活，在<code>metaclass</code>是 <code>abc.ABCMeta</code>的抽象基类中可以声明”抽象方法“和“抽象属性”，直接继承自抽象基类的子类虽然判断<code>issubclass()</code>时为真，但只有完全覆写（实现）了抽象基类中的“抽象”内容后，才能被实例化，而通过注册的虚拟子类则不受此影响。</p><p><code>metaclass</code>为 <code>abc.ABCMeta</code> 的抽象基类如果想要声明“抽象方法”，可以使用<code>abc</code>模块中的装饰器 <code>@abstractmethod</code>，如果想声明“抽象属性”，可以使用<code>abc</code>模块中的<code>@abstractproperty</code> 。最后，为什么要提<code>six</code>模块呢，<code>six</code>模块是<code>Python</code>为了兼容<code>Python 2.x</code> 和<code>Python 3.x</code>提供的一个模块，该模块中有一个针对类的装饰器<code>@six.add_metaclass(MetaClass)</code>可以为两个版本的<code>Python</code>类方便地添加<code>metaclass</code>。这样我们就可以同时利用<code>Python</code>中的<code>abc</code>模块和<code>six</code>模块在类的定义前添加<code>@six.add_metaclass(abc.ABCMeta)</code>来优雅地声明一个抽象基础类了！</p><p>在<code>python2.x</code>和<code>python3.x</code>下都可以通过下面的代码来实现：</p><pre><code class="lang-python">from abc import ABCMeta, abstractmethodimport six@six.add_metaclass(ABCMeta)class MyClass1(object):    def __init__(self, x, y, z):        self.x = x        self.y = y        self.z = z    def add():        return self.x + self.y + self.z    @abstractmethod    def run(self):        &quot;子类中实现该方法&quot;</code></pre><pre><code class="lang-python">class metaTest(six.with_metaclass(ABCMeta, object)):    def __init__(self, x, y, z):        self.x = x        self.y = y        self.z = z    @abstractmethod    def run(self):        &quot;&quot;&quot;需要实现这个类的方法&quot;&quot;&quot;    def __len__(self):        return 3</code></pre><p>而原始在<code>python2</code>和<code>python3</code>中需要使用不同的方式：</p><pre><code class="lang-python"># python2class MyClass(object):    __metaclass__ = ABCMeta    pass# python3class MyClass3(object, metaclass=ABCMeta):    pass</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>six</tag>
      
      <tag>metaclass</tag>
      
      <tag>abc</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>最优化-梯度下降算法</title>
    <link href="/2017/07/14/%E6%9C%80%E4%BC%98%E5%8C%96-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
    <url>/2017/07/14/%E6%9C%80%E4%BC%98%E5%8C%96-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>梯度下降算法是最优化算法之一，在大量的机器学习算法和深度学习中都会被用到。优化是指当改变$x$以最小化或最大化某个函数$f(x)$，该函数一般为算法中的目标函数或损失函数，或者称为代价函数、误差函数等。在优化过程中需要通过导数$f’(x)$或$\frac{dy}{dx}$来描述缩放$x$对输出$f(x)$的变化:$f(x+\theta) \approx f(x)+\theta f’(x)$。对于任意小的$\eta$使得$f(x-\eta sign(f’(x))) \lt f(x)$，因此可以将$x$往导数的相反方向移动一小步来减小$f(x)$，这个过程就是梯度下降。<br><img src="/uploads/grad.jpg" srcset="/img/loading.gif" alt=""><br><a id="more"></a></p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>在最小化多维输入的函数：$f: R^n \rightarrow R^n$，为了使得$f$最小化，在一维情况下需要使用导数，如果是多维的情况下需要使用偏导数(<code>partial derivative</code>)，偏导数$\frac {\partial} {\partial x} f(x)$衡量点$x$在$x_i$增加或减小是$f(x)$的变化。梯度是相对一个向量求导的导数：$f$的导数是包含所有偏导数的向量，可以记作${\nabla}_x f(x)$。梯度的第<code>i</code>个元素是$f$关于$x_i$的偏导数。在$u$(单位向量)方向上的方向导数是函数$f$在$u$方向的斜率。换句话说，方向导数是函数$f(x+\alpha u)$关于$\alpha$的导数。可以根据链式法则，当$\alpha=0$时，$\frac{\partial}{\partial_{\alpha}}f(x+\alpha u)=u^T \nabla_{x}f(x)$。为了最小化$f$希望找到下降最快的方向，计算方向导数</p><script type="math/tex; mode=display">Min\_{u,u^T u}{u^T {\nabla}\_{x}{f(x)}} = Min\_{u,u^T u}{||u||\_2 ||{\nabla}\_x f(x)||\_2 cos{\theta}}</script><p>其中$\theta$是$u$与梯度的夹角。将$||u||_2 = 1$代入，并会略和$u$无关的项，能简化得到${min}_{u}cos{\theta}$。这在$u$ 与梯度方向相反时取得最小。换句话说，梯度向量指向上坡，<br>负梯度向量指向下坡。我们在负梯度方向上移动可以减小$f$。这被称为最速下降法(<code>method of steepest descent</code>) 或梯度下降(<code>gradient descent</code>)。快速下降并更新的公式为</p><script type="math/tex; mode=display">x'=x-{\epsilon}{\nabla}\_x f(x)</script><p>其中$\epsilon$为学习率，描述更新的步长大小的量。当梯度的每个元素为0或者接近0的地方收敛。</p><p>借用维基百科上的说明：</p><blockquote><p>梯度下降算法是基于观察：如果$f(x)$在$a$点处可导且有定义，那么函数$f(x)$在$a$沿着梯度的反向$-\nabla{f(a)}$，如果：</p><script type="math/tex; mode=display">b = a - {\gamma}{\nabla}f(a)</script><p>其中每次$\gamma$的步长更新，且可以改变。对于$\gamma&gt;0$为一个任意小的数成立，那么$f(a) \gt f(b)$。因此可以通过最初的估计到后续过程中进行推断得到$x_{n+1}=x_{n}-\gamma_{n}\nabla{f(x_{n})}, n \ge 0$，因此可以得到$f(x_{0} \ge f(x_1) \ge f(x_2) \ge … \ge f(x_{n+1})$，可以得到下图的形式：</p></blockquote><p><img src="/uploads/Gradient_descent.png" srcset="/img/loading.gif" alt=""></p><p>可以根据线性模型来使用梯度下降算法来求解。其中$h(x)$为拟合的函数，$J(\theta)$为误差函数或损失函数，用来衡量训练数据集用$h(x)$来拟合的效果程度，$\theta$为需要求得参数。假设：</p><script type="math/tex; mode=display">h(x) = \sum{i=1}^{n}{\theta{i}x\_{i}}</script><script type="math/tex; mode=display">J(\theta) = \frac {1} {2m} \sum\_{i=1}^{m} {(y^i - h\_{\theta}(x^i))^2}</script><p>下面是通过梯度下降来进行参数迭代：</p><ol><li>计算$J(\theta)$关于$\theta$的偏导数，也就是计算向量中每一个$\theta$的梯度：</li></ol><script type="math/tex; mode=display">\begin{align}\frac{\partial J(\theta)}{\partial\theta\_j}\\\\& = -\frac1m\sum\_{i=0}^m(y^i-h\_\theta(x^i)) \frac{\partial}{\partial\theta\_j}(y^i-h\_\theta(x^i)) \\\\& = -\frac1m\sum\_{i=0}^m(y^i-h\_\theta(x^i)) \frac{\partial}{\partial\theta\_j}(\sum\_{j=0}^n\theta\_jx\_j^i-y^i) \\\\& = -\frac1m\sum\_{i=0}^m(y^i-h\_\theta(x^i))x^i\_j\end{align}</script><ol><li>沿着梯度的方向更新参数</li></ol><script type="math/tex; mode=display">\theta\_j := \theta\_j + \alpha\frac{\partial J(\theta)}{\partial\theta\_j}:=\theta\_j - \alpha\frac1m\sum\_{i=0}^m(y^i-h\_\theta(x^i))x^i\_j</script><ol><li>迭代直到收敛</li></ol><script type="math/tex; mode=display">\theta\_{j} := \theta\_{j} + \alpha \sum\_{i=1}^{m} (y^{(i)}-h\_{\theta}(x^{(i)}))x\_{j}^{(i)}</script><p>上面的梯度下降是使用了所有的样本。因此在数据量很大的时候，每次迭代都会将训练集遍历一次，内存的开销会很大，因此后续对这种批量梯度下降的方式进行了优化，出现了随机梯度下降等算法。</p><h3 id="通过示例实现批量梯度下降"><a href="#通过示例实现批量梯度下降" class="headerlink" title="通过示例实现批量梯度下降"></a>通过示例实现批量梯度下降</h3><pre><code class="lang-python"># coding:utf-8import numpy as np# 数据x = np.array([[2.1, 1.5], [2.5, 2.3], [3.3, 3.9], [3.9, 5.1], [2.7, 2.7]])y = np.array([2.5, 3.9, 6.7, 8.8, 4.6])# 拟合的函数：f(x) = w1*x1 + w2*x2# 损失函数：J(w) = (sum((f(x) - y)^2))/2m# 初始化权重w = np.random.rand(2)# 初始化 阈值eps, 学习率(学习步长)eps = 0.00001alpha = 0.1loss = 1max_iter = 500iter = 1while loss &gt; eps and iter &lt; 500:    # 初始化 损失函数差值    loss = 0    sigma = np.array([0, 0])    # 更新权重    for i in range(x.shape[0]):        h = y[i] - sum(w * x[i, :])        sigma = sigma + h * x[i, :] / 5    w = w + alpha * sigma    for i in range(x.shape[0]):        loss += (sum(w * x[i, :]) - y[i]) ** 2    print loss, w    iter += 1</code></pre>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>优化</tag>
      
      <tag>梯度下降</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>numba加速python程序运行</title>
    <link href="/2017/07/09/numba%E5%8A%A0%E9%80%9Fpython%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C/"/>
    <url>/2017/07/09/numba%E5%8A%A0%E9%80%9Fpython%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C/</url>
    
    <content type="html"><![CDATA[<p><code>python</code>作为一个动态语言，在运行效率上为一大缺陷，但是作为一门成熟的语言，有很多方式可以提高<code>python</code>的运行效率，如:<code>cpython, pypy,numba,LLVMPy</code>。个人主要用户<code>python</code>处理数据和写一些数据挖掘方面的算法，因此使用<code>numba</code>较为方便。因此对<code>numba</code>说明下，并作为笔记记录以便以后查阅。<code>numba</code>的来源主要，<code>NumPy</code>的创始人<code>Travis Oliphant</code>在离开<code>Enthought</code>之后，创建了<code>CONTINUUM</code>，致力于将<code>Python</code>大数据处理方面的应用。最近推出的<code>Numba</code>项目能够将处理<code>NumPy</code>数组的<code>Python</code>函数<code>JIT</code>编译为机器码执行，从而上百倍的提高程序的运算速度。<a id="more"></a>下面看一个简单的例子：</p><pre><code class="lang-python">import numba as nbfrom numba import jit@jit(&#39;f8(f8[:])&#39;)def sum1d(array):    s = 0.0    n = array.shape[0]    for i in range(n):        s += array[i]    return simport numpy as nparray = np.random.random(10000)%timeit sum1d(array)%timeit np.sum(array)%timeit sum(array</code></pre><p>输出：</p><pre><code>10000 loops, best of 3: 38.9 us per loop10000 loops, best of 3: 32.3 us per loop100 loops, best of 3: 12.4 ms per loop</code></pre><p><code>numba</code>中提供了修饰器利用<code>JIT</code>将其修饰的函数便以为机器函数，变返回一个可在<code>python</code>中调用机器码的包装对象。为了能够将<code>python</code>函数编译成能告诉执行的机器码，需要告诉<code>JIT</code>编译器函数的各个参数和返回值的类型。我们可以通过多种方式指定类型信息，在上面的例子中，类型信息由一个字符串<code>f8(f8[:])</code>指定。其中<code>f8</code>表示8个字节双精度浮点数，括号前面的<code>f8</code>表示返回值类型，括号里的表示参数类型，<code>[:]</code>表示一维数组。因此整个类型字符串表示<code>sum1d()</code>是一个参数为双精度浮点数的一维数组，返回值是一个双精度浮点数。需要注意的是JIT函数只能对指定的类型参数进行运算。</p><p>如果希望<code>JIT</code>能够对所有的类型进行运算，可以所有<code>autojit</code>。但是该函数的运算速度远不及<code>JIT</code>，因为<code>autojit</code>在运算前需要判断类型，因此大大降低了效率。在<code>numba</code>中基本是使用<code>JIT</code>和<code>autojit</code>这两个函数装饰自己的函数。下面是<code>numba</code>模块所支持的所有类型：</p><pre><code>[type for type in dir(numba.types)][&#39;Any&#39;, &#39;Array&#39;, &#39;CPointer&#39;, &#39;CharSeq&#39;, &#39;Complex&#39;, &#39;Dispatcher&#39;, &#39;Dummy&#39;, &#39;Float&#39;, &#39;Function&#39;, &#39;FunctionPointer&#39;, &#39;Integer&#39;, &#39;Kind&#39;, &#39;Macro&#39;, &#39;Method&#39;, &#39;Module&#39;, &#39;Object&#39;, &#39;OpaqueType&#39;, &#39;Optional&#39;, &#39;Prototype&#39;, &#39;Record&#39;, &#39;Tuple&#39;, &#39;Type&#39;, &#39;UniTuple&#39;, &#39;UniTupleIter&#39;, &#39;UnicodeCharSeq&#39;, &#39;VarArg&#39;, &#39;__all__&#39;, &#39;__builtins__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;_autoincr&#39;, &#39;_make_signed&#39;, &#39;_make_unsigned&#39;, &#39;_typecache&#39;, &#39;abs_type&#39;, &#39;absolute_import&#39;, &#39;b1&#39;, &#39;bool_&#39;, &#39;boolean&#39;, &#39;byte&#39;, &#39;c16&#39;, &#39;c8&#39;, &#39;char&#39;, &#39;complex128&#39;, &#39;complex64&#39;, &#39;complex_domain&#39;, &#39;defaultdict&#39;, &#39;division&#39;, &#39;double&#39;, &#39;exception_type&#39;, &#39;f4&#39;, &#39;f8&#39;, &#39;float32&#39;, &#39;float64&#39;, &#39;float_&#39;, &#39;i1&#39;, &#39;i2&#39;, &#39;i4&#39;, &#39;i8&#39;, &#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;int8&#39;, &#39;int_&#39;, &#39;intc&#39;, &#39;integer_domain&#39;, &#39;intp&#39;, &#39;len_type&#39;, &#39;long_&#39;, &#39;longlong&#39;, &#39;neg_type&#39;, &#39;none&#39;, &#39;number_domain&#39;, &#39;numpy&#39;, &#39;print_function&#39;, &#39;print_item_type&#39;, &#39;print_type&#39;, &#39;pyobject&#39;, &#39;range_iter32_type&#39;, &#39;range_iter64_type&#39;, &#39;range_state32_type&#39;, &#39;range_state64_type&#39;, &#39;range_type&#39;, &#39;real_domain&#39;, &#39;short&#39;, &#39;sign_type&#39;, &#39;signed_domain&#39;, &#39;slice3_type&#39;, &#39;slice_type&#39;, &#39;string&#39;, &#39;u1&#39;, &#39;u2&#39;, &#39;u4&#39;, &#39;u8&#39;, &#39;uchar&#39;, &#39;uint&#39;, &#39;uint16&#39;, &#39;uint32&#39;, &#39;uint64&#39;, &#39;uint8&#39;, &#39;uintc&#39;, &#39;uintp&#39;, &#39;ulong&#39;, &#39;ulonglong&#39;, &#39;unsigned_domain&#39;, &#39;ushort&#39;, &#39;void&#39;, &#39;voidptr&#39;]</code></pre><p>下面通过一个例子，利用<code>jit</code>和<code>autojit</code>进行对比，看效率如何：下面的例子中通过读取<code>ecexl</code>中的数据，再对数据进行处理（找出某个元素的索引，并返回其索引值。）</p><pre><code class="lang-python">import xlrdimport numpy as npimport numbafrom time import clock@numba.jit(&#39;int64(char[:])&#39;)   #@autojitdef get_array_table(path):    book = xlrd.open_workbook(filename=path)    table = book.sheets()[0]    nrows = table.nrows    data_array = []    columns = []    for i in range(nrows):        if i == 0:            columns = table.row_values(i)        else:            data_array.append(table.row_values(i))    data = np.array(data_array)    return data@numba.jit(&#39;int64(int64[:])&#39;)   #@autojitdef get_array_index(number): #找到满足条件的元素的索引index，并返回这个索引    index = 0    try：        for k in range(len(data2)):            if data2[k,0] == number:                index = k                busi_amount = sum(data2[index:,0]*data2[index:,1]) #busi amount, 大于阈值业务量的值                user_amount = sum(data2[index:,1])                 #大于阈值的用户量                      break            else:                index = 0                continue        return index    except Exception as e:        print &quot;THere is error:%s ！&quot; % e  if __name__ == &quot;__main__&quot;:    start_time = clock()    path1 = &quot;f:\\TEST1.xlsx&quot;    #file2 in path, sample data, normal user in cp    path2 = &quot;f:\\TEST2.xlsx&quot;    #file1 in path, all data      data1 = get_array_table(path1)    data2 = get_array_table(path2)    all_busi_cnt = sum(data2[:,0]*data2[:,1])    all_user_cnt = sum(data2[:,1])    index = get_array_index(200)    end_time = clock()    print data2[:,0][np.where(data2[:,0] &gt; 200)]    #通过where找出符合条件的数据集    print data2[:,0].tolist().index(200)            #将array转化为list进行index    print data2[index,:]    print &quot;This process have to time %s second!&quot; % (end_time-start_time)</code></pre><p>输出为：</p><pre><code>199[ 200.  261.]This process have to time 2.8969633643 second!#@autojit199[ 200.  261.]This process have to time 8.33246232783 second!</code></pre><p>可以看出<code>JIT</code>比<code>AUTOJIT</code>要快一些。当数据量大时差距会更加的大。另外<code>numba</code>和<code>numpy</code>配合使用效果最佳。可以通过<code>google</code>查询<code>cpython</code> ,<code>pypy</code> 和<code>numba</code>的效率比较,有很多这样文章。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>numba</tag>
      
      <tag>numpy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>蝙蝠侠前传黑暗骑士观后感</title>
    <link href="/2017/07/09/%E8%9D%99%E8%9D%A0%E4%BE%A0%E5%89%8D%E4%BC%A02-%E9%BB%91%E6%9A%97%E9%AA%91%E5%A3%AB%E8%A7%82%E5%90%8E%E6%84%9F/"/>
    <url>/2017/07/09/%E8%9D%99%E8%9D%A0%E4%BE%A0%E5%89%8D%E4%BC%A02-%E9%BB%91%E6%9A%97%E9%AA%91%E5%A3%AB%E8%A7%82%E5%90%8E%E6%84%9F/</url>
    
    <content type="html"><![CDATA[<p>对《蝙蝠侠：黑暗骑士》这部电影是百看不厌，这部电影在当时也是获得了相当高的评价，从这部电影标题就具有两面性，若从布鲁斯韦恩的角度看，黑暗骑士是具有褒义，而从丹特的角度则最终反映了人内容的黑暗面。电影中布鲁斯·韦恩，哈维·丹特，小丑，吉姆·戈登这几个主要的人物，各自将自己的角色演的非常的到位，特别是小丑。在电影中对英雄主意进行了重新定义，从最底层的社会到最上层的社会均展现的淋漓尽致，都反映出了各个阶层的丑陋面貌和各类人的信仰以及精神堡垒等等。对人性的丑恶进行了深刻的思考，特别是通过哈维·丹特在电影中的英雄人物最终沦落到和小丑一样，从光明骑士到两面骑士最终到黑暗骑士。</p><p>首先看这部电影的导演克里斯托弗·诺兰的又一大作，从上一部开战时刻，诺兰再次对蝙蝠侠的到来进行了深入的思考。那么诺兰的背景是怎样的呢？诺兰拥有爱尔兰血统，拥有英美双重国籍，从小就在进行拍摄，诺兰是007的狂热粉，他曾经跟大卫·S·高耶说过，最爱的邦德电影是《女王密使》。在小时候诺兰七岁就用父亲的超8摄影机拍摄自己的玩具兵人，开始了最早的电影创作。孩提时代的他住在芝加哥时，还和日后成为导演、制片人的罗科·贝利克一起拍摄短片。另外在他的手中还有《盗梦空间》,《记忆碎片》,《致命魔术》等等经典的电影。而每部电影有个共同点，在一个想象力非常丰富空间，他叙事手法非常让人出乎意料。<a id="more"></a></p><p>回到电影的本身，从每个人物来分析所反映出的社会现象和一些思考或者人生哲理，首先是蝙蝠侠本身（布鲁斯韦恩），作为电影的男主角，而在当时混乱的髙潭市，蝙蝠侠确实打击了很多犯罪团伙，但是在人们的心中，似乎认为他是真正的英雄，而且搜到黑帮各种的挤兑，而政府也对蝙蝠侠不是非常认可。作为蝙蝠侠本人也认为自己不能完全拯救髙潭市，而是需要一位能够真正站出来的光明骑士。蝙蝠侠在电影中也得到了反思，认为髙潭市是不能通过武力或者暴力来维持正义，而需要以为人们相信的正义者，蝙蝠侠甚至多次对自己身份进行了怀疑，甚至想过放弃，多次向自己的管家和自己公司的负责人询问。在电影中当小丑逼着蝙蝠侠露出真面目的时候，蝙蝠侠选择了挺身而出，此时编剧对这个英雄人物进行了深刻的思考，而不像其他英雄主义的电影。当后面丹特和女友同时被小丑绑架时，蝙蝠侠毅然选择了自己的的女友，这又完全反映出了作为一个英雄内心深处对自己重要的人的保护，而放弃了人们公认的光明骑士。电影中各个阶层都反映出了自己的弱点，如蝙蝠侠对自己重要人的弱点，人性的弱点，其中嘎登身边最信任的女警的弱点，在这种非常混乱的情况下，大部分人选择了黑暗面。<br>在电影中最终哈特倒下，蝙蝠侠和高登知道若哈维谋杀警察的事传出去，会让城市混乱且道德沦陷，而这就是小丑的最大目的。蝙蝠侠说服高登，蝙蝠侠自己扛下这些罪名，为了维持这一光辉形象，让人们想象正义的的存在，这才是英雄的诠释，同时这也是诺兰心中的英雄。最终高登发出了蝙蝠侠的追捕令，砸毁了蝙蝠信号灯，并给了蝙蝠侠黑暗骑士的称号。</p><p>小丑，这个人物在电影中一点不逊于主角，每次事件的制造者，人物心理的分析，事件策划等等都非常的到位，甚至完美。而每次小丑喜欢提到自己的故事，自己的老婆很漂亮，突然变丑后，自己不能面对，其实是不能面对自己的丑陋面。当自己把自己变丑和自己的妻子一起的时候，妻子却发现自己更加不能接受自己的丈夫如此的丑陋。因此导致最后全家分散，另外小丑自己生活在一个完全暴力和一个心理非常受挫的环境中，导致小丑做事情完全无原则性，无规律性。电影中小丑对每个人的心理分析的都非常的到位，如绑架蝙蝠侠女友和光明骑士，最终两座穿上的乘客，以及对丹特的分析等等。他将丹特从一个光明骑士，变为双面骑士，最终变为一个失败者，摧毁人们心中的英雄。这对于小丑这无疑是胜利的。社会最终选择蝙蝠侠而没有选择小丑，不是因为蝙蝠侠是对的，只是因为蝙蝠侠坚守的东西和社会的现实离得更近。所以从某个角度讲，小丑是个彻彻底底的革命派，而蝙蝠侠则是个标准的保守派，并且那种符合蝙蝠侠信仰的社会可能让人更加地感到舒服一点。</p><p>最后两个轮船上人们心理的对峙，当别人的命运掌握在自己的手中，而且能够决定自己的命运的时候，编剧很显然选择了相信对方，这一幕成为了电影的一大亮点。整个过程中的那种纠结，相信和不信任等等。最终显示了光明的一面，人们选择了相信对方，相信正义存在。<br>其他</p>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>电影</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>jupyter-notebook远程登陆配置以及spark多语言环境配置</title>
    <link href="/2017/07/09/jupyter-notebook%E8%BF%9C%E7%A8%8B%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8Aspark%E5%A4%9A%E8%AF%AD%E8%A8%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <url>/2017/07/09/jupyter-notebook%E8%BF%9C%E7%A8%8B%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8Aspark%E5%A4%9A%E8%AF%AD%E8%A8%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<p><code>jupyter notebook</code>是一个基于浏览器的<code>python</code>数据分析工具，使用起来非常方便，具有极强的交互方式和富文本的展示效果。<code>jupyter</code>是它的升级版，它的安装也非常方便，一般<code>Anaconda</code>安装包中会自带。安装好以后直接输入<code>jupyter notebook</code>便可以在浏览器中使用。但是它默认只能在本地访问，如果想把它安装在服务器上，然后在本地远程访问，则需要进行如下配置：<a id="more"></a></p><ol><li>登陆远程服务器</li><li>生成配置文件<code>$jupyter notebook --generate-config</code></li><li><p>生成密码打开<code>ipython</code>，创建一个密文的密码：</p><pre><code class="lang-shell">In [1]: from notebook.auth import passwdIn [2]: passwd()  Enter password:  Verify password:Out[2]: &#39;sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274&#39;</code></pre><p>把生成的密文<code>sha1:ce…</code>复制下来</p></li><li><p>修改默认配置文件<br><code>$vim ~/.jupyter/jupyter_notebook_config.py</code><br>进行如下修改：</p><pre><code class="lang-shell">c.NotebookApp.ip=&#39;*&#39;c.NotebookApp.password = u&#39;sha:ce...  # 刚才复制的那个密文&#39;c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 #随便指定一个端口</code></pre></li><li>启动<code>jupyter notebook：$jupyter notebook</code></li><li>远程访问<br>此时应该可以直接从本地浏览器直接访问<code>http://address_of_remote:8888</code>就可以看到<code>jupyter</code>的登陆界面。</li><li>建立<code>ssh</code>通道<br>如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个<code>ss</code>h通道：在本地终端中输入<code>ssh username@address_of_remote -L127.0.0.1:1234:127.0.0.1:8888</code>便可以在<code>localhost:1234</code>直接访问远程的<code>jupyter</code>了。</li><li><p>给<code>jupyter-notebook</code>服务器换个皮肤</p><pre><code>pip install jupyterthemesjt -l`  #　看看有哪些皮肤，再用jt -t {皮肤名} -cellw 960 -T -tfs 11 -f source -nf ubuntu -tf source -fs 10</code></pre><p>重启<code>jupyter-notebook</code>服务器，即可享受新皮肤。</p></li><li><p>给<code>jupyter</code>安装插件  </p><pre><code># 安装`Jupyter Notebook extensions`conda install -c conda-forge jupyter_contrib_nbextensions orpip install https://github.com/ipython-contrib/IPython-notebook-xtensions/archive/master.zip --userpip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user</code></pre><p>运行<code>Jupyter Notebook</code>, 在打开的<code>Notebook</code>界面里, 你会发现多了一个<code>Nbextensions</code>,点击这个<code>tab</code>勾选<code>Table of Contents</code> (有的版本是<code>toc2</code>). 然后创建或者打开一个<code>Jupter Notebook</code>。</p></li><li><p>部署不同版本的spark jupyter-notes</p></li></ol><p>为了使用<code>jupyter</code>开发<code>spark</code>,需要对<code>spark</code>的<code>jupyter</code>环境进行开发,一般可以使用<code>toree</code>进行安装，方便快捷。也可以安装组件,但是相对复杂。使用<code>apache</code>的<code>toree</code>来安装<code>jupyter-scala</code>和<code>jupyter-spark</code>，很方便实用。既可以安装<code>pyspark</code>还是可以安装<code>scala</code>版本的<code>jupyter-spark</code>。主要安装过程如下：</p><pre><code>pip install toree   # 安装toree`包jupyter toree install --spark_home=$SPARK_HOME  # 用户jupyter配置Apache Toree&quot;&quot;&quot;    $ jupyter kernelspec list    Available kernels:    python3 /Users/myuser/Library/Jupyter/kernels/python3    apache_toree_scala /usr/local/share/jupyter/kernels/apache_toree_scala&quot;&quot;&quot;# 安装jupyter notebook 和 pysparkjupyter toree install --interpreters=PySparkorjupyter toree install --spark_home=$SPARK_HOME --interpreters=PySparkorjupyter toree install --spark_home=$SPARK_HOME --interpreters=Scala,PySpark,SparkR,SQL</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>jupyter</tag>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>spark中创建RDD的几种方式</title>
    <link href="/2017/07/09/spark%E4%B8%AD%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
    <url>/2017/07/09/spark%E4%B8%AD%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>创建<code>RDD</code>主要有三种方式：从外部数据创建<code>RDD</code>，从另外一个<code>RDD</code>创建，对现有的数据列表或者数据集合进行创建。三种方式均能够创建<code>RDD</code>对象。<code>RDD</code>可以包含<code>Scala</code>，<code>Python</code>，<code>Java</code>的任意类型的对象，因此任何数据均可以创建<code>RDD</code>。<code>spark</code>创建<code>RDD</code>时首先需要通过<code>SpakContext</code>作为对象，创建一个<code>sc</code>对象，再通过<code>sc</code>来创建<code>RDD</code>。<code>sc</code>对象之前可以通过<code>sparkconf</code>对象对<code>SpakContex</code>t进行设置。如集群的<code>URL</code>和应用名等等。下面是对<code>SpakContext</code>对象设置。如下：</p><pre><code class="lang-python">from pyspark import SparkContext, SparkConfconf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local&quot;).setSparkHome(&quot;/opt/spark1.6.0/&quot;)sc = SparkContext(conf=conf)</code></pre><p>上面通过<code>SparkConf</code>对象设置了<code>conf</code>. 再创建<code>sc</code>对象，并将设置传给<code>sc</code>对象。<code>SparkConf</code>对象的方法有：<code>set(key,value),setIfMissing(key,value),setMaster(value),setAppName(value)，setSparkHome(value)，setExecutorEnv(key=None, value=None, pairs=None)，setAll(pairs)</code>并对应了<code>get</code>方法。<br><a id="more"></a></p><ul><li><strong>外部数据创建RDD</strong></li></ul><p><code>spark</code>提供很多外部数据创建的接口，如下面所示的<code>textFile</code>.还有<code>hadoopFile</code>,<code>pickleFile</code>以及<code>sqlContext.jsonFile</code>，还提供<code>sql</code>以及<code>hadoop</code>的接口。基本包含了主要的数据格式的读入接口。下面是通过<code>textFile</code>对文本数据进行创建<code>RDD</code>。</p><pre><code class="lang-python">from pyspark import SparkContextfrom pyspark.mllib import classificationfrom pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.evaluation import BinaryClassificationMetricslogFile = &quot;/opt/spark1.6.0/data/mllib/sample_svm_data.txt&quot;sc = SparkContext(&quot;local&quot;, &quot;Simple App&quot;)logData = sc.textFile(logFile)sc.stop()</code></pre><ul><li><strong>从集合数据中创建RDD</strong></li></ul><p>比如从<code>Python</code>的<code>array</code>中创建<code>RDD</code>。这种创建方式为最简单的一种方式。直接通过<code>SpakContext</code>的<code>paralleliza()</code>方法即可。这种方法可以快速的创建<code>RDD</code>对象，再对<code>RDD</code>对象进行转化操作及行动操作等等。下面例子中就是通过<code>paralleliza</code>方法创建<code>RDD</code>。</p><pre><code class="lang-python">#!/usr/bin/env python# _*_ coding:utf8 _*_from sklearn.datasets import load_irisfrom pyspark import SparkContext, SparkConfconf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local&quot;).setSparkHome(&quot;/opt/spark1.6.0/&quot;)sc = SparkContext(conf=conf)data_set = load_iris()data = data_set[&quot;data&quot;]spark_data = sc.parallelize(data)print spark_data.collect()sc.stop()</code></pre><ul><li><strong>从其他RDD创建</strong></li></ul><p>这种创建方式也相对简单，便捷。比如<code>RDD</code>的转化操作便是这样的操作。下面就不再对该中方式进行举例说明。<br>在<code>spark</code>中也提供了<code>dataframe</code>类型的接口。可以创建<code>dataframe</code>,和<code>pandas</code>下的<code>dataframe</code>非常相似，并且有很多相同的操作。创建的方式有很多，可以从数据库中创建，如<code>mysql，hive，hdfs，json</code>等等。下面是各个接口创建<code>dataframe</code>的例子。</p><pre><code class="lang-python"># 从Hive中的users表构造DataFrameusers = sqlContext.table(&quot;users&quot;)# 加载S3上的JSON文件logs = sqlContext.load(&quot;s3n://path/to/data.json&quot;, &quot;json&quot;)# 加载HDFS上的Parquet文件clicks = sqlContext.load(&quot;hdfs://path/to/data.parquet&quot;, &quot;parquet&quot;)# 通过JDBC访问MySQLcomments = sqlContext.jdbc(&quot;jdbc:mysql://localhost/comments&quot;, &quot;user&quot;)# 将普通RDD转变为DataFramerdd = sparkContext.textFile(&quot;article.txt&quot;) \                  .flatMap(lambda line: line.split()) \                  .map(lambda word: (word, 1)) \                  .reduceByKey(lambda a, b: a + b) \wordCounts = sqlContext.createDataFrame(rdd, [&quot;word&quot;, &quot;count&quot;])# 将本地数据容器转变为DataFramedata = [(&quot;Alice&quot;, 21), (&quot;Bob&quot;, 24)]people = sqlContext.createDataFrame(data, [&quot;name&quot;, &quot;age&quot;])# 将Pandas DataFrame转变为Spark DataFrame（Python API特有功能）sparkDF = sqlContext.createDataFrame(pandasDF)# 完整的例子#!/usr/bin/env python# _*_ coding:utf-8 _*_from sklearn.datasets import load_irisfrom pyspark import SparkContext, SparkConf, SQLContextimport pandas as pdconf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local&quot;).setSparkHome(&quot;/opt/spark-1.6.0/&quot;)sc = SparkContext(conf=conf)data_set = load_iris()data = data_set[&quot;data&quot;]df = pd.DataFrame(data=data)s_data = SQLContext(sc).createDataFrame(df)print s_data.count()print s_data.schema, s_data.columnssc.stop()</code></pre><p>若要收集<code>RDD</code>中的信息可以使用<code>token</code>方法，对<code>RDD</code>的对象进行收集，入下：</p><pre><code class="lang-python">for line in s_data.take(10):    print line</code></pre>]]></content>
    
    
    <categories>
      
      <category>spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>spark</tag>
      
      <tag>RDD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>docker部署anaconda开发环境</title>
    <link href="/2017/07/09/docker%E9%83%A8%E7%BD%B2anaconda%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"/>
    <url>/2017/07/09/docker%E9%83%A8%E7%BD%B2anaconda%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<p>由于在工作中使用<code>docker</code>部署一个科学计算的环境<code>anaconda</code>，如：<code>docker run -d -p 50001:22 anaconda2:ssh /usr/sbin/sshd -D</code>，可以简单的用脚本启动一个指定本地端口映射到容器<code>22</code>端口的一个容器环境，特别是在生成多个容器时。因此使用<code>python</code>写一个脚本能够实现该功能。避免在工作中重复使用<code>docker run</code>的命令来生成容器。该脚本还可以扩展一些功能，如获取容器的虚拟<code>IP</code>，获取容器的名称和容器<code>ID</code>，对容器的操作等等。<a id="more"></a>下面为实现的脚本：</p><pre><code class="lang-python">#!/usr/bin/pythonfrom __future__ import print_functionimport commandsfrom optparse import OptionParserimport sysdef check_stats(stats_code):    if not stats_code:        return True    return Falseparse = OptionParser()parse.add_option(&quot;-p&quot;, &quot;--port&quot;, action=&quot;store&quot;, dest=&quot;port&quot;, help=&quot;Config location host port.&quot;)parse.add_option(&quot;-i&quot;, &quot;--image&quot;, action=&quot;store&quot;, dest=&quot;image_name&quot;,                help=&quot;The image&#39;s name, use generate docker container.&quot;)parse.add_option(&quot;-s&quot;, &quot;--stop&quot;, action=&quot;store&quot;, dest=&quot;container_id&quot;,                help=&quot;Stop the container from give container&#39;s id.&quot;)option, args = parse.parse_args()if len(args) == 1:    print(args, &quot;The number of args is 1, must have many parameter.&quot;)if option.port:    stats, process_line = commands.getstatusoutput(&quot;ps aux | grep daemon | grep docker | grep -v grep&quot;)    if not stats:        start_stats, _ = commands.getstatusoutput(&quot;service docker restart&quot;)        if check_stats(start_stats):            print(&quot;The docker daemon service start success.&quot;)        else:            print(&quot;The docker daemon service start error.&quot;)            sys.exit(1)    image_name = option.image_name if option.image_name else &#39;anaconda2-ssh:ssh&#39;    stat, values = commands.getstatusoutput(&quot;docker run -d -p {0}:22 {1} /usr/sbin/sshd -D&quot;.                                            format(option.port, image_name))    if check_stats(stat):        container_id = commands.getoutput(&quot;docker ps -l -q&quot;)        print(container_id)    else:        print(&quot;Failed to create container.&quot;)        sys.exit(1)elif option.container_id:    mask_code, output_value = commands.getstatusoutput(&quot;docker stop {0}&quot;.format(option.container_id))    if check_stats(mask_code):        print(&quot;The container({0}) stop success.&quot;.format(option.container_id))    else:        print(&quot;Failed to stop container({0}).&quot;.format(option.container_id))        sys.exit(1)else:    print(&quot;This script use error. Reference: \n Usage: file.py [options]&quot;)    sys.exit(1)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>anaconda</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用spark读取mysql数据库数据转化为dataframe</title>
    <link href="/2017/07/08/%E4%BD%BF%E7%94%A8spark%E8%AF%BB%E5%8F%96mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%B0%E6%8D%AE%E8%BD%AC%E5%8C%96%E4%B8%BAdataframe/"/>
    <url>/2017/07/08/%E4%BD%BF%E7%94%A8spark%E8%AF%BB%E5%8F%96mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%B0%E6%8D%AE%E8%BD%AC%E5%8C%96%E4%B8%BAdataframe/</url>
    
    <content type="html"><![CDATA[<p>使用<code>spark</code>中的接口连接<code>mysql</code>数据库并读取并查询出数据库中的数据并转化为<code>spark</code>中的<code>dataframe</code>格式的<code>RDD</code>。其中实现了两种方式查询数据，一种单表查询和另外一种的多表关联查询数据。其中使用的方式在下面代码的<code>main</code>函数中。<a id="more"></a></p><pre><code class="lang-java">import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContextclass sparkConnectMySQL(host: String, user: String, password: String) {    private val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;connection&quot;)    private val sc = new SparkContext(conf)    private var port = &quot;3306&quot;    private var database = &quot;situation&quot;    def this(host: String, user: String, password: String, port:String) {        this(host: String, user: String, password: String)        this.port = port    }    def this(host: String, user: String, password: String, port:String, database:String) {        this(host: String, user: String, password: String, port:String)        this.database = database    }    assert(host.isInstanceOf[String], println(&quot;The host must be string type.&quot;))    val jdbcURL = &quot;jdbc:mysql://&quot; + host + &quot;:&quot; + port + &quot;/&quot; + database  // ?user=root&amp;password=root123    val sqlContext = new SQLContext(sc)    def query(tableName:String, sqlScript:String, registerTable:String): org.apache.spark.sql.DataFrame = {        /**          * THis method is query one table, default table name: temp, this is register temp table name.          * tableName =&gt; The table&#39;s truth name in database.          * sqlScript =&gt; mySQL script.          * registerTable =&gt; create temp table name.          */        val execute = sqlContext.read.format(&quot;jdbc&quot;).options(Map(&quot;url&quot; -&gt; this.jdbcURL, &quot;dbtable&quot; -&gt; tableName,            &quot;user&quot; -&gt; user, &quot;password&quot; -&gt; password)).load()        execute.registerTempTable(registerTable)        execute.sqlContext.sql(sqlScript)  /* return DataFrame type class */    }    def queryMany(tableName:Array[String], sqlScript:String, registerTable:Array[String]): org.apache.spark.sql.DataFrame = {        /**          * THis method is query many table by join, default table name: temp, this is register temp table name.          * tableName&#39;s size as same as registerTable size.          * tableName =&gt; The table&#39;s truth name of array in database.          * sqlScript =&gt; mySQL script.          */        val tableZipRegister = tableName.zip(registerTable)        for ((name, temp) &lt;- tableZipRegister) {            val execute = sqlContext.read.format(&quot;jdbc&quot;).options(Map(&quot;url&quot; -&gt; this.jdbcURL, &quot;dbtable&quot; -&gt; name,                &quot;user&quot; -&gt; user, &quot;password&quot; -&gt; password)).load()            execute.registerTempTable(temp)        }        sqlContext.sql(sqlScript)    }}object sparkConnectTest{    def main(args: Array[String]): Unit ={        val scm = new sparkConnectMySQL(host = &quot;10.4.5.125&quot;, user = &quot;root&quot;, password = &quot;root123&quot;)        // use method example:        val dataframe = scm.query(tableName = &quot;BUG_ANALYZER_RESULT&quot;, sqlScript = &quot;select * from TEMP1&quot;, registerTable = &quot;TEMP1&quot;)        val frame = scm.queryMany(tableName = Array(&quot;GUARD_VIRUS_SCAN&quot;, &quot;GUARD_VIRUS_SCAN&quot;),                                  sqlScript = &quot;select a.scan_id,a.imei, b.app_name from temp1 a, temp2 b where a.scan_id = b.scan_id &quot;,                                  registerTable=Array(&quot;temp1&quot;, &quot;temp2&quot;))        dataframe.collect().take(10).foreach(println)        frame.collect().take(10).foreach(println)    }}</code></pre>]]></content>
    
    
    <categories>
      
      <category>spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>spark</tag>
      
      <tag>RDD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>特征可视化和特征降维算法T-SNE模型详解</title>
    <link href="/2017/07/06/%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96%E5%92%8C%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95T-SNE%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/"/>
    <url>/2017/07/06/%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96%E5%92%8C%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95T-SNE%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<blockquote><p>T-SNE(<code>t-distributed stochastic neighbor embedding</code>)是用于降维的一种机器学习算法，是由 Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。此外，<code>t-SNE</code> 是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。t-SNE是由SNE(<code>Stochastic Neighbor Embedding, SNE; Hinton and Roweis</code>, 2002)发展而来。我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。<br><a id="more"></a></p><h2 id="1-SNE"><a href="#1-SNE" class="headerlink" title="1. SNE"></a>1. SNE</h2><h3 id="1-1-基本原理"><a href="#1-1-基本原理" class="headerlink" title="1.1 基本原理"></a>1.1 基本原理</h3><p>SNE是通过仿射(<code>affinitie</code>)变换将数据点映射到概率分布上，主要包括两个步骤：</p><ul><li>SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。</li><li>SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</li></ul></blockquote><p>我们看到<code>t-SNE</code>模型是非监督的降维，他跟<code>kmeans</code>等不同，他不能通过训练得到一些东西之后再用于其它数据（比如<code>kmeans</code>可以通过训练得到k个点，再用于其它数据集，而<code>t-SNE</code>只能单独的对数据做操作，也就是说他只有<code>fit\_transform</code>，而没有<code>fit</code>操作）</p><h3 id="1-2-SNE原理推导"><a href="#1-2-SNE原理推导" class="headerlink" title="1.2 SNE原理推导"></a>1.2 SNE原理推导</h3><p><code>SNE</code>是先将欧几里得距离转换为条件概率来表达点与点之间的相似度。具体来说，给定一个N个高维的数据$x_1,…,x_n$注（意N不是维度）, <code>t-SNE</code>首先是计算概率$p_{ij}$)，正比于$x_i$和$x_j$之间的相似度（这种概率是我们自主构建的），即：</p><script type="math/tex; mode=display">{p\_ {j \mid i} = \frac{\exp(- \mid  \mid  x\_i -x\_j  \mid  \mid  ^2 / (2 \sigma^2\_i ))} {\sum\_{k \neq i} \exp(- \mid  \mid  x\_i - x\_k  \mid  \mid  ^2 / (2 \sigma^2\_i))}}</script><p>这里的有一个参数是$\sigma_i$，对于不同的点$x_i$取值不一样，后续会讨论如何设置。此外设置$p_{x|x}=0$,因为我们关注的是两两之间的相似度。那对于低维度下的$y_i$，我们可以指定高斯分布为方差为$\frac{1}{\sqrt{2}}$，因此它们之间的相似度如下:</p><script type="math/tex; mode=display">{q\_ {j \mid i} = \frac{\exp(- \mid  \mid  x\_i -x\_j  \mid  \mid  ^2)} {\sum\_{k \neq i} \exp(- \mid  \mid  x\_i - x\_k  \mid  \mid  ^2)}}</script><p>同样，设定$q_{i|i}=0$。如果降维的效果比较好，局部特征保留完整，那么$p_{i \mid j} = q_{i \mid j}$, 因此我们优化两个分布之间的距离-KL散度(<code>Kullback-Leibler divergences</code>)，那么目标函数(<code>cost function</code>)如下:</p><script type="math/tex; mode=display">C = \sum\_i KL(P\_i  \mid  \mid  Q\_i) = \sum\_i \sum\_j p\_{j \mid i} \log \frac{p\_{j \mid i}}{q\_{j \mid i}}</script><p>这里的$p_i$表示了给定点$x_i$下，其他所有数据点的条件概率分布。需要注意的是<strong>KL散度具有不对称性</strong>，在低维映射中不同的距离对应的惩罚权重是不同的，具体来说：距离较远的两个点来表达距离较近的两个点会产生更大的<code>cost</code>，相反，用较近的两个点来表达较远的两个点产生的<code>cost</code>相对较小(注意：类似于回归容易受异常值影响，但效果相反)。即用较小的 $q_{j \mid i}=0.2$来建模较大的 $p_{j \mid i}=0.8$，$cost=p \log(\frac{p}{q})=1.11$,同样用较大的$q_{j \mid i}=0.8$来建模较小的$p_{j \mid i}=0.2$, $cost=-0.277$, 因此，<strong><code>SNE</code>会倾向于保留数据中的局部特征</strong>。</p><blockquote><p><strong>思考:了解了基本思路之后，你会怎么选择$\sigma$，固定初始化?</strong></p></blockquote><p>下面我们开始正式的推导SNE。首先不同的点具有不同的$\sigma_i$，$P_i$的熵(<code>entropy</code>)会随着$\sigma_i$的增加而增加。<code>SNE</code>使用困惑度(<code>perplexity</code>)的概念，用二分搜索的方式来寻找一个最佳的$\sigma$。其中困惑度指:</p><script type="math/tex; mode=display">Perp(P\_i) = 2^{H(P\_i)}</script><p>这里的$H(P_i)$是$P_i$的熵，即:</p><script type="math/tex; mode=display">H(P\_i) = -\sum\_j p\_{j \mid i} \log\_2 p\_{j \mid i}</script><p>困惑度可以解释为一个点附近的有效近邻点个数。<code>SNE</code>对困惑度的调整比较有鲁棒性，通常选择<code>5-50</code>之间，给定之后，使用二分搜索的方式寻找合适的$\sigma$。<br>那么核心问题是如何求解梯度了,目标函数等价于$\sum \sum - p log(q)$这个式子与<code>softmax</code>非常的类似，我们知道<code>softmax</code>的目标函数是$\sum -y \log p$，对应的梯度是$y - p$(注：这里的<code>softmax</code>中<code>y</code>表示<code>label</code>，<code>p</code>表示预估值)。 同样我们可以推导<code>SNE</code>的目标函数中的<code>i</code>在<code>j</code>下的条件概率情况的梯度是$2(p_{j \mid i}-q_{j \mid i})(y_i-y_j)$， 同样<code>j</code>在<code>i</code>下的条件概率的梯度是$2(p_{j \mid i}-q_{j \mid i})(y_i-y_j)$, 最后得到完整的梯度公式如下:</p><script type="math/tex; mode=display">\frac{\delta C}{\delta y\_i} = 2 \sum\_j (p\_{j \mid i} - q\_{j \mid i} + p\_{i \mid j} - q\_{i \mid j})(y\_i - y\_j)</script><p>在初始化中，可以用较小的$\sigma$下的高斯分布来进行初始化。为了加速优化过程和避免陷入局部最优解，梯度中需要使用一个相对较大的动量(<code>momentum</code>)。即参数更新中除了当前的梯度，还要引入之前的梯度累加的指数衰减项，如下:</p><script type="math/tex; mode=display">Y^{(t)} = Y^{(t-1)} + \eta \frac{\delta C}{\delta Y} + \alpha(t)(Y^{(t-1)} - Y^{(t-2)})</script><p>这里的$Y^{(t)}$表示迭代<code>t</code>次的解，$\eta$表示学习速率,$\alpha(t)$表示迭代<code>t</code>次的动量。此外，在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。因此，<code>SNE</code>在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。</p><blockquote><p><strong>思考:SNE有哪些不足？ 面对SNE的不足，你会做什么改进？</strong></p></blockquote><h2 id="2-T-SNE"><a href="#2-T-SNE" class="headerlink" title="2. T-SNE"></a>2. T-SNE</h2><p>尽管<code>SNE</code>提供了很好的可视化方法，但是他很难优化，而且存在<code>crowding problem</code>(拥挤问题)。后续中，<code>Hinton</code>等人又提出了<code>t-SNE</code>的方法。与<code>SNE</code>不同，主要如下:</p><ul><li>使用对称版的<code>SNE</code>，简化梯度公式</li><li>低维空间下，使用<code>t</code>分布替代高斯分布表达两点之间的相似度</li></ul><p><code>t-SNE</code>在低维空间下使用更重长尾分布的<code>t</code>分布来避免<code>crowding</code>问题和优化问题。在这里，首先介绍一下对称版的<code>SNE</code>，之后介绍<code>crowding</code>问题，之后再介绍<code>t-SNE</code></p><h3 id="2-1-Symmetric-SNE"><a href="#2-1-Symmetric-SNE" class="headerlink" title="2.1 Symmetric SNE"></a>2.1 Symmetric SNE</h3><p>优化$p_{i \mid j}$和$q_{i \mid j}$的<code>KL</code>散度的一种替换思路是，使用联合概率分布来替换条件概率分布，即<code>P</code>是高维空间里各个点的联合概率分布，<code>Q</code>是低维空间下的，目标函数为:</p><script type="math/tex; mode=display">C = KL(P \mid  \mid Q) = \sum\_i \sum\_j p\_{i,j} \log \frac{p\_{ij}}{q\_{ij}}</script><p>这里的$p_{ii}$,$q_{ii}$为0，我们将这种<code>SNE</code>称之为<code>symmetric SNE</code>(对称<code>SNE</code>)，因为他假设了对于任意<code>i</code>,$p_{ij} = p_{ji}, q_{ij} = q_{ji}$，因此概率分布可以改写为:</p><script type="math/tex; mode=display">p\_{ij} = \frac{\exp(- \mid  \mid x\_i - x\_i \mid  \mid ^2 / 2\sigma^2)}{\sum\_{k \neq l} \exp(- \mid  \mid x\_i-x\_j \mid  \mid ^2 / 2\sigma^2)}  \ \ \ \  q\_{ij} = \frac{\exp(- \mid  \mid y\_i - y\_i \mid  \mid ^2)}{\sum\_{k \neq l} \exp(- \mid  \mid y\_i-y\_j \mid  \mid ^2)}</script><p>这种表达方式，使得整体简洁了很多。但是会引入异常值的问题。比如$x_i$是异常值，那么$∣∣x_i−x_j∣∣^2$会很大，对应的所有的<code>j</code>,$p_{il}$都会很小(之前是仅在$x_i$下很小)，导致低维映射下的$y_i$对<code>cost</code>影响很小。</p><blockquote><p><strong>思考: 对于异常值，你会做什么改进？pipi表示什么？</strong></p></blockquote><p>为了解决这个问题，我们将联合概率分布定义修正为: $p_{ij} = \frac{p_{i \mid j} + p_{j \mid i}}{2}$, 这保证了$\sum_j p_{ij} \gt \frac{1}{2n}$, 使得每个点对于<code>cost</code>都会有一定的贡献。对称<code>SNE</code>的最大优点是梯度计算变得简单了，如下:</p><script type="math/tex; mode=display">\frac{\delta C}{\delta y\_i} = 4 \sum\_j (p\_{ij} - q\_{ij})(y\_i - y\_j)</script><p>实验中，发现对称<code>SNE</code>能够产生和<code>SNE</code>一样好的结果，有时甚至略好一点。</p><h3 id="2-2-Crowding问题"><a href="#2-2-Crowding问题" class="headerlink" title="2.2 Crowding问题"></a>2.2 Crowding问题</h3><p>拥挤问题就是说各个簇聚集在一起，无法区分。比如有一种情况，高维度数据在降维到10维下，可以有很好的表达，但是降维到两维后无法得到可信映射，比如降维如10维中有11个点之间两两等距离的，在二维下就无法得到可信的映射结果(最多3个点)。 进一步的说明，假设一个以数据点$x_i$为中心，半径为r的m维球(三维空间就是球)，其体积是按$r^m$增长的，假设数据点是在m维球中均匀分布的，我们来看看其他数据点与$x_i$的距离随维度增大而产生的变化。<br><img src="/uploads/f1.png" srcset="/img/loading.gif" alt=""><br>从上图可以看到，随着维度的增大，大部分数据点都聚集在m维球的表面附近，与点$x_i$的距离分布极不均衡。如果直接将这种距离关系保留到低维，就会出现拥挤问题。</p><blockquote><p><strong>怎么解决crowding问题呢?</strong></p></blockquote><p><code>Cook et al.(2007)</code> 提出一种<code>slight repulsion</code>的方式，在基线概率分布(<code>uniform background</code>)中引入一个较小的混合因子$\rho$,这样$q_{ij}$就永远不会小于\frac{2 \rho}{n(n-1)}(因为一共了<code>n(n-1)</code>个<code>pairs</code>)，这样在高维空间中比较远的两个点之间的$q_{ij}$总是会比$p_{ij}$大一点。这种称之为<code>UNI-SNE</code>，效果通常比标准的<code>SNE</code>要好。优化<code>UNI-SNE</code>的方法是先让$\rho$为0，使用标准的<code>SNE</code>优化，之后用模拟退火的方法的时候，再慢慢增加$\rho$. 直接优化<code>UNI-SNE</code>是不行的(即一开始$\rho$不为0)，因为距离较远的两个点基本是一样的$q_{ij}$(等于基线分布), 即使$p_{ij}$很大，一些距离变化很难在$q_{ij}$中产生作用。也就是说优化中刚开始距离较远的两个聚类点，后续就无法再把他们拉近了。</p><h3 id="2-3-t-sne"><a href="#2-3-t-sne" class="headerlink" title="2.3 t-sne"></a>2.3 t-sne</h3><p>对称<code>SNE</code>实际上在高维度下 另外一种减轻”拥挤问题”的方法：在高维空间下，在高维空间下我们使用高斯分布将距离转换为概率分布，在低维空间下，我们使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。<br><img src="/uploads/f2.png" srcset="/img/loading.gif" alt=""><br>我们对比一下高斯分布和<code>t</code>分布(如上图), <code>t</code>分布受异常值影响更小，拟合结果更为合理，较好的捕获了数据的整体特征。使用了<code>t</code>分布之后的<code>q</code>变化，如下:</p><script type="math/tex; mode=display">q\_{ij} = \frac{(1 +  \mid  \mid y\_i -y\_j \mid  \mid ^2)^{-1}}{\sum\_{k \neq l} (1 +  \mid  \mid y\_i -y\_j \mid  \mid ^2)^{-1}}</script><p>此外，<code>t</code>分布是无限多个高斯分布的叠加，计算上不是指数的，会方便很多。优化的梯度如下:</p><script type="math/tex; mode=display">\frac{\delta C}{\delta y\_i} = 4 \sum\_j(p\_{ij}-q\_{ij})(y\_i-y\_j)(1+ \mid  \mid y\_i-y\_j \mid  \mid ^2)^{-1}</script><p><img src="/uploads/f3.png" srcset="/img/loading.gif" alt=""><br><code>t-sne</code>的有效性，也可以从上图中看到：横轴表示距离，纵轴表示相似度, 可以看到，对于较大相似度的点，<code>t</code>分布在低维空间中的距离需要稍小一点；而对于低相似度的点，<code>t</code>分布在低维空间中的距离需要更远。这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。</p><p>总结一下，t-SNE的梯度更新有<strong>两大优势</strong>：</p><ul><li>对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。</li><li>这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。</li></ul><h3 id="2-4-算法过程"><a href="#2-4-算法过程" class="headerlink" title="2.4 算法过程"></a>2.4 算法过程</h3><ul><li>Data: $X = {x_1, … , x_n}$</li><li>计算<code>cost function</code>的参数：困惑度<code>Perp</code></li><li>优化参数: 设置迭代次数<code>T</code>， 学习速率$\eta$, 动量$\alpha(t)$</li><li>目标结果是低维数据表示$Y^T = {y_1, … , y_n}$</li><li>开始优化<ul><li>计算在给定<code>Perp</code>下的条件概率$p_{j \mid i}$(参见上面公式)</li><li>令 $p_{ij} = \frac{p_{j \mid i} + p_{i \mid j}}{2n}$</li><li>用N(0, 10^{-4}I)随机初始化<code>Y</code></li><li>迭代，从 <code>t = 1</code> 到 <code>T</code>， 做如下操作:<ul><li>计算低维度下的$q_{ij}$(参见上面的公式)</li><li>计算梯度（参见上面的公式）</li><li>更新$Y^{t} = Y^{t-1} + \eta \frac{dC}{dY} + \alpha(t)(Y^{t-1} - Y^{t-2})$</li></ul></li><li>结束</li></ul></li><li>结束</li></ul><p><strong>优化过程中可以尝试的两个<code>trick</code>:</strong></p><ul><li>提前压缩(<code>early compression</code>):开始初始化的时候，各个点要离得近一点。这样小的距离，方便各个聚类中心的移动。可以通过引入<code>L2</code>正则项(距离的平方和)来实现。</li><li>提前夸大(<code>early exaggeration</code>)：在开始优化阶段，$p_{ij}$乘以一个大于1的数进行扩大，来避免因为$q_{ij}$太小导致优化太慢的问题。比如前50次迭代，$p_{ij}$乘以4</li></ul><h3 id="2-5-不足"><a href="#2-5-不足" class="headerlink" title="2.5 不足"></a>2.5 不足</h3><p><strong>主要不足有四个:</strong></p><ul><li>主要用于可视化，很难用于其他目的。比如测试集合降维，因为他没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。</li><li>t-SNE倾向于保存局部特征，对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间</li><li>t-SNE没有唯一最优解，且没有预估部分。如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-sne中距离本身是没有意义，都是概率分布问题。</li><li>训练太慢。有很多基于树的算法在t-sne上做一些改进</li></ul><h2 id="3-变种"><a href="#3-变种" class="headerlink" title="3. 变种"></a>3. 变种</h2><ul><li><code>multiple maps of t-SNE</code></li><li><code>parametric t-SNE</code></li><li><code>Visualizing Large-scale and High-dimensional Data</code></li></ul><h2 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4. 参考资料"></a>4. 参考资料</h2><ul><li><a href="http://www.datakit.cn/blog/2017/02/05/t_sne_full.html" target="_blank" rel="noopener">t-sne完整学习笔记</a></li><li><a href="http://blog.csdn.net/lzl1663515011/article/details/46328337" target="_blank" rel="noopener">t-sne算法理解</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征过程</tag>
      
      <tag>降维</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>用spark对时间数据进行分组求平均值</title>
    <link href="/2017/07/03/%E7%94%A8spark%E5%AF%B9%E6%97%B6%E9%97%B4%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%88%86%E7%BB%84%E6%B1%82%E5%B9%B3%E5%9D%87%E5%80%BC/"/>
    <url>/2017/07/03/%E7%94%A8spark%E5%AF%B9%E6%97%B6%E9%97%B4%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%88%86%E7%BB%84%E6%B1%82%E5%B9%B3%E5%9D%87%E5%80%BC/</url>
    
    <content type="html"><![CDATA[<h3 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h3><p>数据格式为key,timestrap,value。需求是按照<code>key</code>并对<code>timestrap</code>固定时间间隔计算，再将<code>key</code>和时间间隔作为<code>key</code>进行分组统计<code>value</code>的均值。其中利用了<code>spark</code>的<code>gourpbykey</code>操作，并对时间进行了转换。实现过程如下：</p><pre><code class="lang-java">package org.app.spark/**  * Created by root on 17-7-2.  */import org.apache.spark.rdd.RDDimport org.apache.spark.{HashPartitioner, SparkConf, SparkContext}import java.text.SimpleDateFormatimport java.util.{Calendar, Date}import scala.collection.mutable.ArrayBufferclass ParseTimeSeries(path: String) {    val conf = new SparkConf().setAppName(&quot;ParseSeries&quot;).setMaster(&quot;local[2]&quot;)    val sc = new SparkContext(conf)    def getData: RDD[(String, Date, Int)] = {        val line: RDD[Array[String]] = sc.textFile(path).map(s =&gt; s.split(&quot;,&quot;))        val newLine = line.map(line =&gt; {            val timeString = line.apply(1)            val tf: SimpleDateFormat = new SimpleDateFormat(&quot;yyyy/MM/dd HH:mm&quot;)            val timeFormat: Date = tf.parse(timeString)            (line.apply(0), timeFormat, line.apply(2).toInt)        })        newLine    }    def processSeriesByKey(rdd: RDD[(String, Date, Int)]): RDD[(String, Int, Int)] = {        val toCalendar = Calendar.getInstance()        val array = new ArrayBuffer[(String, Int, Int)]()        for (x &lt;- rdd.collect()) {            toCalendar.setTime(x._2)            val day = toCalendar.get(Calendar.HOUR_OF_DAY)            val flag = day / 10            array.+=((x._1, flag, x._3))        }        sc.parallelize(array)    }}object testParseTimeSeries {    def main(args: Array[String]): Unit = {        val file: String = &quot;/opt/Project/spark_project/data/TimeValue.csv&quot;        val parse = new ParseTimeSeries(file)        val new_rdd = parse.processSeriesByKey(parse.getData)        new_rdd.collect().foreach(println)        val other = new_rdd.keyBy(x =&gt; (x._1, x._2)).map(x =&gt; (x._1, x._2._3)).groupByKey().            map(x =&gt; (x._1, x._2.sum/x._2.count(x=&gt;true).toDouble))        other.foreach(println)        other.aggregate()    }}</code></pre><a id="more"></a><h3 id="部分数据"><a href="#部分数据" class="headerlink" title="部分数据"></a>部分数据</h3><pre><code>a,2016/1/2 12:12,41a,2016/1/2 13:12,19a,2016/1/2 14:12,12a,2016/1/2 15:12,84a,2016/1/2 16:12,67a,2016/1/2 17:12,50a,2016/1/2 18:12,72a,2016/1/2 19:12,82a,2016/1/2 20:12,77a,2016/1/2 21:12,12a,2016/1/2 22:12,16a,2016/1/2 23:12,16a,2016/1/3 0:12,28a,2016/1/3 1:12,67a,2016/1/3 2:12,86a,2016/1/3 3:12,93a,2016/1/3 4:12,6a,2016/1/3 5:12,57a,2016/1/3 6:12,78a,2016/1/3 7:12,39a,2016/1/3 8:12,45a,2016/1/3 9:12,2a,2016/1/3 10:12,71a,2016/1/3 11:12,26a,2016/1/3 12:12,1a,2016/1/3 13:12,99a,2016/1/3 14:12,51a,2016/1/3 15:12,66b,2016/1/3 16:12,67b,2016/1/3 17:12,51b,2016/1/3 18:12,75b,2016/1/3 19:12,90b,2016/1/3 20:12,2b,2016/1/3 21:12,90b,2016/1/3 22:12,95b,2016/1/3 23:12,7b,2016/1/4 0:12,32b,2016/1/4 1:12,14b,2016/1/4 2:12,82b,2016/1/4 3:12,29b,2016/1/4 4:12,74b,2016/1/4 5:12,57b,2016/1/4 6:12,13b,2016/1/4 7:12,75b,2016/1/4 8:12,100b,2016/1/4 9:12,49</code></pre>]]></content>
    
    
    <categories>
      
      <category>spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>spark</tag>
      
      <tag>大数据</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>spark算子aggregate,fold,lookup详解</title>
    <link href="/2017/07/03/spark%E7%AE%97%E5%AD%90aggregate-fold-lookup%E8%AF%A6%E8%A7%A3/"/>
    <url>/2017/07/03/spark%E7%AE%97%E5%AD%90aggregate-fold-lookup%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>定义如下：<br><code>def aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U</code><br>aggregate用户聚合RDD中的元素，先使用seqOp将RDD中每个分区中的T类型元素聚合成U类型，再使用combOp将之前每个分区聚合后的U类型聚合成U类型，特别注意seqOp和combOp都会使用zeroValue的值，zeroValue的类型为U。</p><pre><code>var rdd1 = sc.makeRDD(1 to 10,2)rdd1.mapPartitionsWithIndex {        (partIdx,iter) =&gt; {          var part_map = scala.collection.mutable.Map[String,List[Int]]()            while(iter.hasNext){              var part_name = &quot;part_&quot; + partIdx;              var elem = iter.next()              if(part_map.contains(part_name)) {                var elems = part_map(part_name)                elems ::= elem                part_map(part_name) = elems              } else {                part_map(part_name) = List[Int]{elem}              }            }            part_map.iterator        }      }.collectres16: Array[(String, List[Int])] = Array((part_0,List(5, 4, 3, 2, 1)), (part_1,List(10, 9, 8, 7, 6)))</code></pre><a id="more"></a> <p>第一个分区中包含5,4,3,2,1<br>第二个分区中包含10,9,8,7,6</p><pre><code class="lang-scala">scala&gt; rdd1.aggregate(1)(     |           {(x : Int,y : Int) =&gt; x + y},      |           {(a : Int,b : Int) =&gt; a + b}     |     )res17: Int = 58</code></pre><p>结果为什么是58，看下面的计算过程：</p><p>先在每个分区中迭代执行<code>(x : Int,y : Int) =&gt; x + y</code>并且使用<code>zeroValue</code>的值<code>1</code><br>即：<code>part_0</code>中<code>zeroValue+5+4+3+2+1 = 1+5+4+3+2+1 = 16</code><br><code>part_1</code>中 <code>zeroValue+10+9+8+7+6 = 1+10+9+8+7+6 = 41</code><br>再将两个分区的结果合并<code>(a : Int,b : Int) =&gt; a + b</code>，并且使用<code>zeroValue</code>的值<code>1</code><br>即：<code>zeroValue+part_0+part_1 = 1 + 16 + 41 = 58</code><br>再比如：</p><pre><code>scala&gt; rdd1.aggregate(2)(     |           {(x : Int,y : Int) =&gt; x + y},      |           {(a : Int,b : Int) =&gt; a * b}     |     )res18: Int = 1428</code></pre><p>这次<code>zeroValue=2</code><br><code>part_0</code>中 <code>zeroValue+5+4+3+2+1 = 2+5+4+3+2+1 = 17</code><br><code>part_1</code>中 <code>zeroValue+10+9+8+7+6 = 2+10+9+8+7+6 = 42</code><br>最后：<code>zeroValue*part_0*part_1 = 2 * 17 * 42 = 1428</code><br>因此，<code>zeroValue</code>即确定了<code>U</code>的类型，也会对结果产生至关重要的影响，使用时候要特别注意。</p><h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><pre><code class="lang-scala">def fold(zeroValue: T)(op: (T, T) ⇒ T): T// fold是aggregate的简化，将aggregate中的seqOp和combOp使用同一个函数op。scala&gt; rdd1.fold(1)(     |       (x,y) =&gt; x + y         |     )res19: Int = 58</code></pre><p>结果同上面使用aggregate的第一个例子一样，即：</p><pre><code class="lang-scala">scala&gt; rdd1.aggregate(1)(     |           {(x,y) =&gt; x + y},      |           {(a,b) =&gt; a + b}     |     )res20: Int = 58</code></pre><h3 id="lookup"><a href="#lookup" class="headerlink" title="lookup"></a>lookup</h3><p><code>def lookup(key: K): Seq[V]</code><br>lookup用于(K,V)类型的RDD,指定K值，返回RDD中该K对应的所有V值。</p><pre><code>scala&gt; var rdd1 = sc.makeRDD(Array((&quot;A&quot;,0),(&quot;A&quot;,2),(&quot;B&quot;,1),(&quot;B&quot;,2),(&quot;C&quot;,1)))rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at makeRDD at :21scala&gt; rdd1.lookup(&quot;A&quot;)res0: Seq[Int] = WrappedArray(0, 2)scala&gt; rdd1.lookup(&quot;B&quot;)res1: Seq[Int] = WrappedArray(1, 2)</code></pre>]]></content>
    
    
    <categories>
      
      <category>spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bootstrap,Boosting,Bagging,Randforest几种方法介绍</title>
    <link href="/2017/06/12/Bootstrap-Boosting-Bagging-Randforest%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D/"/>
    <url>/2017/06/12/Bootstrap-Boosting-Bagging-Randforest%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h3 id="bagging-boosting-random-forest和boosting算法介绍"><a href="#bagging-boosting-random-forest和boosting算法介绍" class="headerlink" title="bagging, boosting, random forest和boosting算法介绍"></a>bagging, boosting, random forest和boosting算法介绍</h3><p>这两天在看关于<code>boosting</code>算法时，看到一篇不错的文章讲<code>bootstrap, jackknife, bagging, boosting, random forest</code>都有介绍，以下是搜索得到的原文，没找到博客作者的地址，在这里致谢作者的研究。 一并列出一些找到的介绍<code>boosting</code>算法的资源:</p><ol><li>视频讲义，介绍boosting算法，主要介绍<a href="http://videolectures.net/mlss05us_schapire_b/" target="_blank" rel="noopener">AdaBoosing</a></li><li>在这个网站的资源项里列出了对于boosting算法来源介绍的几篇文章，可以<a href="http://www.boosting.org/tutorials" target="_blank" rel="noopener">下载</a></li><li>一个博客介绍了许多视觉中常用算法，作者的实验和理解，这里附录的链接是关于使用<a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/28/2420936.html" target="_blank" rel="noopener">opencv进行人脸检测</a>的过程和代码，可以帮助理解训练过程是如何完成的</li><li>这里是一个台湾的电子期刊上关于<a href="http://140.113.87.114/cvrc/edm/vol_6/tech1.htm" target="_blank" rel="noopener">AdaBoost的介绍</a> —- <code>Jackknife，Bootstraping, bagging, boosting, AdaBoosting, Rand forest</code> 和 <code>gradient boosting</code>这些术语，我经常搞混淆，现在把它们放在一起，以示区别。(部分文字来自网络，由于是之前记的笔记，忘记来源了，特此向作者抱歉） </li></ol><p><code>Bootstraping</code>: 名字来自成语<code>pull up by your own bootstraps</code>，意思是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想和基本步骤如下： </p><ol><li>采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。</li><li>根据抽出的样本计算给定的统计量T。 </li><li>重复上述N次（一般大于1000），得到N个统计量T。 </li><li>计算上述N个统计量T的样本方差，得到统计量的方差。 </li></ol><p>应该说Bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。Jackknife： 和上面要介绍的Bootstrap功能类似，只是有一点细节不一样，即每次从样本中抽样时候只是去除几个样本（而不是抽样），就像小刀一样割去一部分。<br><a id="more"></a></p><h3 id="下列方法都是上述Bootstraping思想的一种应用。"><a href="#下列方法都是上述Bootstraping思想的一种应用。" class="headerlink" title="下列方法都是上述Bootstraping思想的一种应用。"></a>下列方法都是上述Bootstraping思想的一种应用。</h3><h4 id="bagging算法"><a href="#bagging算法" class="headerlink" title="bagging算法"></a>bagging算法</h4><p><code>bagging：bootstrap aggregating</code>的缩写。让该学习算法训练多轮，每轮的训练集由从初始的训练集中随机取出的n个训练样本组成，某个初始训练样本在某轮训练集中可以出现多次或根本不出现，训练之后可得到一个预测函数序列<code>h_1，⋯ ⋯h_n</code> ，最终的预测函数H对分类问题采用投票方式，对回归问题采用简单平均方法对新示例进行判别。<br>训练R个分类器<code>f_i</code>，分类器之间其他相同就是参数不同。其中<code>f_i</code>是通过从训练集合中(N篇文档)随机取(取后放回)N次文档构成的训练集合训练得到的。对于新文档d，用这R个分类器去分类，得到的最多的那个类别作为d的最终类别。</p><h4 id="boosting算法"><a href="#boosting算法" class="headerlink" title="boosting算法"></a>boosting算法</h4><p>boosting: 其中主要的是<code>AdaBoost(Adaptive Boosting)</code>。初始化时对每一个训练例赋相等的权重<code>1／n</code>，然后用该学算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在后续的学习中集中对比较难的训练例进行学习，从而得到一个预测函数序列<code>h_1,⋯, h_m</code>, 其中<code>h_i</code>也有一定的权重，预测效果好的预测函数权重较大，反之较小。最终的预测函数H对分类问题采用有权重的投票方式，对回归问题采用加权平均的方法对新示例进行判别。（类似<code>Bagging</code>方法，但是训练是串行进行的，第k个分类器训练时关注对前<code>k-1</code>分类器中错分的文档，即不是随机取，而是加大取这些文档的概率。) </p><h4 id="boosting和bagging的区别"><a href="#boosting和bagging的区别" class="headerlink" title="boosting和bagging的区别"></a>boosting和bagging的区别</h4><p><code>Bagging</code>与<code>Boosting</code>的区别：二者的主要区别是取样方式不同。<code>Bagging</code>采用均匀取样，而<code>Boosting</code>根据错误率来取样，因此<code>Boosting</code>的分类精度要优于<code>Bagging</code>。<code>Bagging</code>的训练集的选择是随机的，各轮训练集之间相互独立，而<code>Boostlng</code>的各轮训练集的选择与前面各轮的学习结果有关；<code>Bagging</code>的各个预测函数没有权重，而<code>Boosting</code>是有权重的；<code>Bagging</code>的各个预测函数可以并行生成，而<code>Boosting</code>的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。<code>Bagging</code>可通过并行训练节省大量时间开销。<code>bagging</code>和<code>boosting</code>都可以有效地提高分类的准确性。在大多数数据集中，<code>boosting</code>的准确性比<code>bagging</code>高。在有些数据集中，<code>boosting</code>会引起退化。</p><p><code>Boosting</code>思想的一种改进型<code>AdaBoost</code>方法在邮件过滤、文本分类方面都有很好的性能。 <code>gradient boosting</code>（又叫Mart, Treenet)：<code>Boosting</code>是一种思想，<code>Gradient Boosting</code>是一种实现<code>Boosting</code>的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数(<code>loss function</code>)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度(<code>Gradient</code>)的方向上下降。 </p><h4 id="RandomForest算法"><a href="#RandomForest算法" class="headerlink" title="RandomForest算法"></a>RandomForest算法</h4><p><code>Rand forest</code>： 随机森林，顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 在建立每一棵决策树的过程中，有两点需要注意 – 采样与完全分裂。首先是两个随机采样的过程，<code>random forest</code>对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个<code>feature</code>中，选择m个<code>(m &lt;&lt; M)</code>。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤 – 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现<code>over-fitting</code>。 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个<code>feature</code>中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。 </p><p><code>Rand forest</code>与<code>bagging</code>的区别：</p><ol><li><code>Rand forest</code>是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而<code>bagging</code>一般选取比输入样本的数目少的样本；</li><li><code>bagging</code>是用全部特征来得到分类器，而<code>rand forest</code>是需要从全部特征中选取其中的一部分来训练得到分类器； 一般<code>Rand forest</code>效果比<code>bagging</code>效果好！ </li></ol><p><code>bootstrps bagging boosting</code>这几个概念经常用到，现仔细学习了一下：他们都属于集成学习方法，(如：<code>Bagging，Boosting，Stacking</code>)，将训练的学习器集成在一起,原理来源于PAC学习模型（<code>ProbablyApproximately CorrectK</code>）。Kearns和Valiant指出，在PAC学习模型中，若存在一个多项式级的学习算法来识别一组概念，并且识别正确率很高，那么这组概念是强可学习的；而如果学习算法识别一组概念的正确率仅比随机猜测略好，那么这组概念是弱可学习的。他们提出了弱学习算法与强学习算法的等价性问题，即是否可以将弱学习算法提升成强学习算法。如果两者等价，那么在学习概念时，只要找到一个比随机猜测略好的弱学习算法，就可以将其提升为强学习算法，而不必直接去找通常情况下很难获得的强学习算法。 </p><p>文本分类中使用的投票方法（<code>Voting</code>，也叫组合分类器）就是一种典型的集成机器学习方法。它通过组合多个弱分类器来得到一个强分类器，包括<code>Bagging</code>和<code>Boosting</code>两种方式，二者的主要区别是取样方式不同。<code>Bagging</code>采用均匀取样，而<code>Boosting</code>根据错误率来取样，因此<code>Boosting</code>的分类精度要优于<code>Bagging</code>。投票分类方法虽然分类精度较高，但训练时间较长。<code>Boostin</code>g思想的一种改进型<code>AdaBoost</code>方法在邮件过滤、文本分类方面都有很好的性能。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>boosting</tag>
      
      <tag>bagging</tag>
      
      <tag>randomforest</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>信息增益筛选变量和python实现</title>
    <link href="/2017/06/12/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%AD%9B%E9%80%89%E5%8F%98%E9%87%8F%E5%92%8Cpython%E5%AE%9E%E7%8E%B0/"/>
    <url>/2017/06/12/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%AD%9B%E9%80%89%E5%8F%98%E9%87%8F%E5%92%8Cpython%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="熵的定义"><a href="#熵的定义" class="headerlink" title="熵的定义"></a>熵的定义</h3><p>在了解信息增益之前需要清楚信息熵的定义，熵起源于物理学，后来被香农引入信息论中。熵是信息论中对信息量的一个衡量值，指接收的每条消息中包含的信息的平均量。这里的消息代表了样本，特征。熵意义可以解释为不确定性的度量，特征的随机性或者混乱程度越大那么熵值越大。因此熵被量化特征包含信息量的大小，在机器学习领域被广泛引用。</p><h3 id="熵的计算"><a href="#熵的计算" class="headerlink" title="熵的计算"></a>熵的计算</h3><center> H(X)=-SUM(P(x(i))log(p(x(i)), b)) </center><p>这里的b为对数使用的底数，通常为2，或者自然常熟e, p(i)为X中第i个取值在所在样本中的概率。</p><h3 id="信息增益IG-Y-X"><a href="#信息增益IG-Y-X" class="headerlink" title="信息增益IG(Y|X)"></a>信息增益IG(Y|X)</h3><p>衡量每个属性X区分目标样本Y的能力，当新增一个属性或特征X时，信息熵的变化大小即为信息增益。IG(Y|X)的值越大那么X对于Y越重要。 条件熵：H(Y|X)，当X条件下Y的信息熵。</p><p><center> IG(Y|X)=H(Y)-H(Y|X) </center><br><a id="more"></a><br>下面是通过python实现计算样本特征的信息增益：</p><pre><code class="lang-python"># -*-coding:utf-8-*-import numpy as npfrom collections import Counterclass EntropyGain(object):    &quot;&quot;&quot;    The datasets information entropy, it&#39;s contains feature entropy and information gain. Use to select the bests    feature, if the gain value the bigger the better.    Parameters    -------------------        X: array, the train datasets.        Y: array, class label        alpha: float or int, the best of feature&#39;s threshold value.    Attributes    -------------------        X, y, alpha, like the parameter.    Methods    -------------------        get_feature_class_label: get feature&#39;s value map to class label and apply to entropy value.        select_k_best_feature: select the best of feature apply to alpha.    Samples    -------------------    &gt;&gt; eg = EntropyGain(X, Y, alpha=10)    &gt;&gt; eg.select_k_best_feature()        [(0, 0.8321), (2, 0.7681), (5, 0.7163), ..., (1, 0.4908)]    &quot;&quot;&quot;    def __init__(self, X, Y, alpha=0.80):        self.x = X        self.y = Y        self.alpha = alpha    def __prob_y(self, y_=None):        if y_ is None:            y_ = self.y        if not self.y.size:            print &quot;The variable %s is NoneType object.&quot; % &#39;y_&#39;        n_sample = len(y_)        label_prob = {v: float(c)/n_sample for v, c in Counter(y_.flatten()).iteritems()}        return label_prob    def __get_label_entropy(self, class_prob=None):        &quot;&quot;&quot;        param class_prob: the class label probability value, it&#39;s &#39;collections.Counter&#39; object.        return: float, the class label entropy        &quot;&quot;&quot;        if not class_prob:            print &quot;This variable class_prob is NoneType object.&quot;            class_prob = self.__prob_y(y_=self.y)        return reduce(lambda x1, x2: x1 + x2, [-p * np.log2(p) for v, p in class_prob.iteritems()])    def get_feature_class_label(self, feature_or_index):        &quot;&quot;&quot;        parameter            x: the model train data x            feature_or_index: the datasets x feature or index.            y: class label datasets.        return:            filter label data&#39;s label counter.        &quot;&quot;&quot;        x_ = self.x[:, feature_or_index]        feature_prob_dict = self.__prob_y(y_=x_)        feature_entropy = {v: self.__get_label_entropy(self.__prob_y(y_=self.y[x_ == v])) for v in np.unique(x_)}        return reduce(lambda x1, x2: x1 + x2, [feature_prob_dict[k] * entropy for k, entropy in                                              feature_entropy.iteritems()])    def select_k_best_feature(self):        &quot;&quot;&quot;        parameter:            x: array, the feature datasets.            y: array, class label            alpha: float or int, filter threshold value.        return:            feature index and entropy gain.        &quot;&quot;&quot;        row_num, col_num = self.x.shape        index_map_entropy = dict()        entropy_h0 = self.__get_label_entropy(self.__prob_y(y_=self.y))        print u&quot;初始信息熵为:&quot;, round(entropy_h0, 4)        for _index_ in range(col_num):            index_map_entropy[_index_] = round(entropy_h0 - self.get_feature_class_label(_index_), 4)        if isinstance(self.alpha, int) and self.alpha &lt;= col_num:            return sorted(index_map_entropy.iteritems(), key=lambda d: d[1], reverse=True)[:self.alpha]        if isinstance(self.alpha, float) and 0.0 &lt; self.alpha &lt;= 1:            index_num = int(col_num * self.alpha)            return sorted(index_map_entropy.iteritems(), key=lambda d: d[1], reverse=True)[:index_num]</code></pre>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>特征选择</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>魔法方法__getattr__和__getattribute__的详解</title>
    <link href="/2017/06/06/%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95__getattr__%E5%92%8C__getattribute__%E7%9A%84%E8%AF%A6%E8%A7%A3/"/>
    <url>/2017/06/06/%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95__getattr__%E5%92%8C__getattribute__%E7%9A%84%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h3 id="getattr-和-getattribute-的定义"><a href="#getattr-和-getattribute-的定义" class="headerlink" title="__getattr__() 和 __getattribute__()的定义"></a><code>__getattr__()</code> 和 <code>__getattribute__()</code>的定义</h3><p>python在属性访问上定义了<code>__getattr__()</code> 和 <code>__getattribute__()</code>两种方法。区别是很小，但是必须了解他们之间的区别，否则在使用的过程中会导致严重的后果。首先看这两个方法的定义：</p><ol><li>如果在类中定义了<code>__getattr__()</code>这个方法，那么在对象查询属性时,如果这个属性在类中定义了，如一个已经实例化的对象x定义了属性name，那么在使用x.name时就不会调用<code>__getattr__(</code>name<code>)</code>；会直接返回name已经定义好的值。简单的说：<code>__getattr__()</code>会在对象中不存在这个属性时被调用。</li><li>如果在类定义了<code>__getattribute__()</code>方法，那么它无条件被调用。当实例化的对象每次引用属性和方法时都会先调用它。如果属性查找不到那么就会引发AttributeError的异常。除非在类中同时定义了<code>__getattribute__()</code>和<code>__getattr__()</code>两个方法。</li></ol><p>下面来看一个<code>__setattr__</code>的实例：</p><pre><code class="lang-python">class MagicTest(object):    def __init__(self):        self.name = &quot;Miller&quot;        self.age = 25    def __getattr__(self, name):        return Noneif __name__ == &quot;__main__&quot;:    magic = MagicTest()    print magic.name    print magic.age    print magic.sex# ouputMiller25None</code></pre><a id="more"></a><h3 id="getattribute-的使用"><a href="#getattribute-的使用" class="headerlink" title="__getattribute__的使用"></a><code>__getattribute__</code>的使用</h3><p>从上面的代码中可以很清楚的知道，magic在引用sex不存在的属性时，调用了<code>__getattr__</code>方法。因此返回了None。那么现在存在一个问题：<code>__getattribute__</code>是怎么样的呢？我们把上面的代码稍作修改看会怎么样。如下：</p><pre><code class="lang-python">class MagicTest1(object):    def __init__(self):        self.name = &quot;Miller&quot;        self.age = 25    def __getattribute__(self, name):        print &quot;__getattribute__&quot;        return super(MagicTest1, self).__getattribute__(name)     (1)if __name__ == &quot;__main__&quot;:    magic1 = MagicTest1()    print magic1.name    print magic1.age    print magic1.sex# output__getattribute__Miller__getattribute__25-------------------AttributeError Traceback (most recent call last)AttributeError: `MagicTest1` object has no attribute `sex`  (2)</code></pre><p>可以看出上面的代码中有几个地方很疑惑，在上面我已经进行了标记(1),(2)。首先来看第一个疑惑的地方：<br>(1)为什么要使用super来调用object的<code>__getattribute__</code>方法呢？首先，<code>__getattribute__</code>方法在新式类中才存在，在python2.x需要继承object才会有这个方法，python3.x全是新式类。其次，这样做的好处是防止无线循环查找属性。因为getattribute在访问属性的时候一直会被调用，自定义的getattribute方法里面同时需要返回相应的属性，通过<code>self.__dict__</code>取值会继续向下调用getattribute，造成循环调用.<br>(2)为什么会有AttributeError呢？因在上面代码中，<code>__getattribute__</code>会被无条件调用，当调用sex属性时，去实例对象magic1中查找是否具备该属性：<code>t.__dict__</code>中查找，每个类和实例对象都有一个<code>__dict__</code>的属性，若在<code>t.__dict__</code>中找不到对应的属性，则去该实例的类中寻找，即<code>t.__class__.__dict__</code>。若在实例的类中也招不到该属性，则去父类中寻找，即<code>t.__class__.__bases__.__dict_</code>中寻找。若以上均无法找到，则会调用<code>__getattr__</code>方法，执行内部的命令（若未重载<code>__getattr__</code>方法，则直接报错：AttributeError)。</p><p>总体来说查找过程是:<code>__dict__</code> ——&gt; <code>__class__.__dict__</code> ——&gt; <code>__class__.__bases__.__dict__</code>；如果都查找不到就会调用<code>__getattr__</code>方法。如果没有<code>__getattr__</code>方法则会引发异常AttributeError。<br>下面我们可以打印出对象的属性看里面的值：</p><pre><code class="lang-python">print magic1.__dict__print magic1.__class__.__bases__.__dict__print magic1.__class__.__dict____getattribute__{`age`: 25, `name`: `Miller`}__getattribute__{`__module__`: `__main__`, `__getattribute__`: &lt;function __getattribute__ at 0x00000000042C6E48&gt;, `__dict__`: &lt;attribute `__dict__` of `MagicTest1` objects&gt;, `__weakref__`: &lt;attribute `__weakref__` of `MagicTest1` objects&gt;, `__doc__`: None, `__init__`: &lt;function __init__ at 0x00000000042C6F28&gt;}__getattribute__AttributeError: `tuple` object has no attribute `__dict__`</code></pre><p>可以知道。上面输出是还是调用了<code>__getattribute__</code>方法，因此在输出时才会打印<code>__getattribute__</code>。输出的<code>__dict__</code>如上面输出所示。<br>因为在上面的MagicTest1中我们定义了name和age属性。我改变代码并并显式定义个属性sex并进行赋值看会是什么结果？</p><pre><code class="lang-python">class MagicTest1(object):    def __init__(self):        self.name = &quot;Miller&quot;        self.age = 25    def __getattribute__(self, name):        print &quot;__getattribute__&quot;        if name == &quot;sex&quot;:            return &quot;Women&quot;        return object.__getattribute__(self, name)if __name__ == &quot;__main__&quot;:    magic1 = MagicTest1()    magic1.sex = &quot;man&quot;    print magic1.sex__getattribute__Women</code></pre><p>从结果中可以看出，给sex属性赋值后并没有生效。这是因为在访问属性和方法时会被无条件的调用，因此导致sex属性的值并未生效。因此一定要注意防止这种情况发生。如果定义了类的<code>__getattribute__()</code>方法，你可能还想定义一个<code>__setattr__()</code>方法，并在两者之间进行协同，以跟踪属性的值。否则，在创建实例之后所设置的值将会消失在黑洞中。如下的代码：</p><pre><code class="lang-python">class MagicTest1(object):    def __init__(self):        self.name = &quot;Miller&quot;        self.age = 25    def __getattribute__(self, name):        print &quot;__getattribute__&quot;        if name == &quot;sex&quot;:            try:                return self.__dict__[name]            except:                raise AttributeError(&quot;This object has no attribute!&quot;)        return object.__getattribute__(self, name)    def __setattr__(self, name, value):        self.__dict__[name] = valueif __name__ == &quot;__main__&quot;:    magic1 = MagicTest1()    magic1.sex = &quot;man&quot;    print magic1.sex</code></pre><h3 id="定义三种方法"><a href="#定义三种方法" class="headerlink" title="定义三种方法"></a>定义三种方法</h3><p>假设我们在一个类中定义了<code>__getattribute__</code>，<code>__setattr__</code>，<code>__getattr__</code>这三个方法：</p><pre><code class="lang-python">class CardHolder(object):    acctlen = 8  # Class data    retireage = 59.5    def __init__(self, acct, name, age, addr):        self.acct = acct  # Instance data        self.name = name  # These trigger __setattr__ too        self.age = age  # acct not mangled: name tested        self.addr = addr  # addr is not managed    def __getattr__(self, name):        return None    def __getattribute__(self, name):        superget = object.__getattribute__  # Don`t loop: one level up        if name == &quot;acct&quot;: # On all attr fetches            return superget(self, &#39;acct&#39;)[:-3] + &quot;***&quot;        elif name == &quot;remain&quot;:            return superget(self, &quot;retireage&quot;) - superget(self, &quot;age&quot;)        else:            print &quot;This variable %s is not exists.&quot; % name            return superget(self, name)  # name, age, addr: stored    def __setattr__(self, name, value):        if name == &quot;name&quot;:  # On all attr assignments            value = value.lower().replace(&quot;&quot;, &quot;_&quot;)  # addr stored directly        elif name == &quot;age&quot;:            if value &lt; 0 or value &gt; 150:                raise ValueError(`invalid age`)        elif name == &quot;acct&quot;:            value = value.replace(&quot;-&quot;, &quot;&quot;)            if len(value) != self.acctlen:                raise TypeError(&quot;invald acct number&quot;)        elif name == &quot;remain&quot;:            raise TypeError(&quot;cannot set remain&quot;)        self.__dict__[name] = value  # Avoid loops, orig names</code></pre><p>上面的代码中：在属性赋值时首先会调用<code>__setattr__</code>方法，即使是在<code>__init__</code>中赋值也会调用<code>__setattr__</code>。<code>__getattribute__</code>实现了对部分属性值的计算，在<code>__setattr__</code>也可以实现值的计算或者类型检测等操作。在实际应用中有利于统一接口，使整个类更加的严谨等很多好处。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>当访问某个实例属性时，<code>__getattribute__</code>会被无条件调用，如未实现自己的<code>__getattr__</code>方法，会抛出AttributeError提示找不到这个属性，如果自定义了自己<strong>getattr</strong>方法的话，方法会在这种找不到属性的情况下被调用，比如上面的例子中的情况。所以在找不到属性的情况下通过实现自定义的getattr方法来实现一些功能是一个不错的方式，因为它不会像<code>__getattribute__</code>方法每次都会调用可能会影响一些正常情况下的属性访问。<br>参考文档：</p><ul><li><a href="http://python.jobbole.com/84095/" target="_blank" rel="noopener">Python <code>__getattribute__</code> vs <code>__getattr__</code> 浅谈</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-python-elegance-2.html" target="_blank" rel="noopener">可爱的 Python: Python 之优雅与瑕疵，第 2 部分</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nohup命令及其输出文件</title>
    <link href="/2017/06/06/nohup%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%85%B6%E8%BE%93%E5%87%BA%E6%96%87%E4%BB%B6/"/>
    <url>/2017/06/06/nohup%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%85%B6%E8%BE%93%E5%87%BA%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="nohup命令及其输出文件"><a href="#nohup命令及其输出文件" class="headerlink" title="nohup命令及其输出文件"></a>nohup命令及其输出文件</h3><p>今天在linux上部署wdt程序，在SSH客户端执行./start-dishi.sh,启动成功,在关闭SSH客户端后，运行的程序也同时终止了，怎样才能保证在推出SSH客户端后程序能一直执行呢？通过网上查找资料，发现需要使用nohup命令。</p><p>完美解决方案：<code>nohup ./start-dishi.sh &gt;output 2&gt;&amp;1 &amp;</code><br><a id="more"></a></p><h3 id="现对上面的命令进行下解释"><a href="#现对上面的命令进行下解释" class="headerlink" title="现对上面的命令进行下解释"></a>现对上面的命令进行下解释</h3><ul><li>用途：不挂断地运行命令。</li><li>语法：<code>nohup Command [ Arg ... ] [　&amp; ]</code></li><li>描述：nohup 命令运行由 Command 参数和任何相关的 Arg 参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用 nohup 命令运行后台中的程序。要运行后台中的 nohup 命令，添加 &amp; （ 表示“and”的符号）到命令的尾部。</li></ul><p>操作系统中有三个常用的流：<br>　　0：标准输入流 stdin<br>　　1：标准输出流 stdout<br>　　2：标准错误流 stderr<br>一般当我们用<code>&gt; console.txt</code>，实际是 <code>1&gt;console.txt</code>的省略用法；<code>&lt; console.txt</code> ，实际是 <code>0 &lt; console.txt</code>的省略用法。</p><h3 id="下面步入正题："><a href="#下面步入正题：" class="headerlink" title="下面步入正题："></a>下面步入正题：</h3><pre><code class="lang-shell">&gt;nohup ./start-dishi.sh &gt;output 2&gt;&amp;1 &amp;</code></pre><p>解释:</p><ol><li>带&amp;的命令行，即使terminal（终端）关闭，或者电脑死机程序依然运行（前提是你把程序递交到服务器上)；</li><li><code>2&gt;&amp;1</code>的意思: 这个意思是把标准错误（2）重定向到标准输出中（1），而标准输出又导入文件output里面，所以结果是标准错误和标准输出都导入文件output里面了。 至于为什么需要将标准错误重定向到标准输出的原因，那就归结为标准错误没有缓冲区，而stdout有。这就会导致<code>&gt;output 2&gt;output</code> 文件output被两次打开，而stdout和stderr将会竞争覆盖，这肯定不是我门想要的.<br>这就是为什么有人会写成: <code>nohup ./command.sh &gt;output 2&gt;output</code>出错的原因了.</li></ol><p>最后谈一下/dev/null文件的作用，这是一个无底洞，任何东西都可以定向到这里，但是却无法打开。 所以一般很大的stdou和stderr当你不关心的时候可以利用stdout和stderr定向到这里<code>&gt;./command.sh &gt;/dev/null 2&gt;&amp;1</code><br>It’s not too late to change.</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>shell</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
