<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Miller Wu">
  <meta name="keywords" content="">
  <title>ReID的OSnet网络详解 - Miller&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/5.12.1/css/all.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/mdbootstrap/4.13.0/css/mdb.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />

<link  rel="stylesheet" href="/css/main.css" />


  <link defer rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />


<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Miller's Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/SpainSurfer_EN-AU11271138486_1920x1080.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <p class="mt-3 post-meta">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>
                  星期日, 一月 3日 2021, 9:45 晚上
                </p>
              

              <p class="mt-1">
                
                  
                  <span class="post-meta">
                    <i class="far fa-chart-bar"></i>
                    3.8k 字
                  </span>
                

                
                  
                  <span class="post-meta">
                      <i class="far fa-clock"></i>
                      17 分钟
                  </span>
                

                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5 z-depth-3" id="board">
          <div class="post-content mx-auto" id="post">
            
            <div class="markdown-body">
              <p>最近在做一个关于行人重识别的相关项目，对ReID的一些背景和相关技术进行了一些了解。ReID属于行人检测的延伸或叫做子任务，但比行人检测的难度更大，主要在于ReID是利用计算机视觉技术对特定行人进行跨视域匹配和检索，所谓的跨视域即是图像来源于不同的摄像头，因此可能因为角度，遮挡，光线，换装等导致检测的难度增大。ReID的应用场景较多，如无人超市中的人员监控，刑侦中的目标搜索，车辆追踪等等。</p>
<p>ReID主要是在行人检测之后的识别过程，所以一半会经过两部分任务，第一部分主要完成行人检测，第二部分再进行ReID检测。而ReID检测中主要进行两个步骤：第一步，对目标(输入图像)进行特征提取，即feature extraction；第二步，在特征提取之后计算各个输入之间的相似度或距离，即similarity measurement。通过相似度度量值排序后输出。</p>
<h2 id="开源数据集"><a href="#开源数据集" class="headerlink" title="开源数据集"></a>开源数据集</h2><p>目前，ReID相关开源的数据集较多，如Market1501，cuhk03，viper，dukemtmc，msmt17，grid等等。这些数据集基本包括<code>bounding_box_train</code>，<code>bounding_box_test</code>和<code>query</code>三部分，会将数据划分为train，test，query和gallery，训练集中的人和测试集中的人没有重合。</p>
<p>在Market1501数据集中每个图片命名格式为:<code>1500_c6s3_086567_01.jpg</code>，1500为person_id，c6s3表示摄像机编号和序列编号，086567为视频中所在的帧数位置，01表示该person在该摄像机序列下的第01个图像。一个person在对应的摄像机和序列下可能对应多个图像。其他开源数据集的命名格式类似。</p>
<p>数据下载地址可以参考此处的汇总：<a href="https://blog.csdn.net/qiuchangyong/article/details/82219775" target="_blank" rel="noopener">ReID开源数据集汇总</a></p>
<a id="more"></a>
<h2 id="OSnet"><a href="#OSnet" class="headerlink" title="OSnet"></a>OSnet</h2><p>本次项目中最初使用了resent50网络作为reid阶段的网络，基于开源数据集进行了重新训练，但是在计算相似度时发现提取的特征计算出相似度都比较大，而且每个行人之间的特征比较接近(行人之间的相似度都比较大)。说明模型在提取特征方面存在一些问题，特征局部表示能力可能较弱。因此需要更换网络。</p>
<p>osnet称为全尺度(<strong>omni-scale</strong>)网络，用于ReID的全尺度特征学习，将局部特征和全局特征进行对应和组合。OSnet主要是基于CNN体系建立，基本构建块由多个卷积流构成，每个卷积流检测不同尺度范围的特征，对全尺度特征学习，提出了一种统一的聚合门(aggregation gate, AG)，通过AG将多尺度特征与信道权值动态融合。同时OSnet也是轻量级的网络，全过程由卷积构成，在构建块中将标准卷积分解为点卷积和深度卷积。</p>
<p><img src="/uploads/multi-struct.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<h3 id="深度可分离卷积-Depthwise-Separable-Convolutions"><a href="#深度可分离卷积-Depthwise-Separable-Convolutions" class="headerlink" title="深度可分离卷积(Depthwise Separable Convolutions)"></a>深度可分离卷积(Depthwise Separable Convolutions)</h3><p>该方法主要用来减少网络的参数数量和计算量，在轻量级网络中可以采用深度可分离卷积来设计轻量级结构，主要是将一个原始的标准卷积核$w\in \R ^ {k \times k \times c \times {c’}}$分解为点卷积核深度卷积：$w=u \times v$，其中：</p>
<p>$u \in \R ^ {1 \times 1 \times c \times {c’}}$为一个点卷积，为通道维度上的卷积；$ v \in \R ^ {k \times k \times 1 \times {c’}}$为一个深度卷积，在feature map上用感受视野k来聚合局部信息。其结构对比如下图所示，标准3*3的卷积进行分离后：</p>
<p><img src="/uploads/deep-seg-struct.png" srcset="/img/loading.gif" style="zoom:70%;" /></p>
<p>假设一张图片的大小为w,h,那么标准卷积的参数量为$h \times w \times k \times k \times c \times c’$,在使用分离卷积后参数量变为$h \times w \times (k \times k+c) \times c’$（此处省略了偏置项），参数量会成倍的减少，作者称<code>Lite3x3</code>。还值得注意的是<strong>作者在此处先用的点卷积再使用的深度卷积</strong>。</p>
<h3 id="全尺度残差块"><a href="#全尺度残差块" class="headerlink" title="全尺度残差块"></a>全尺度残差块</h3><p>该部分主要是利用不同尺度的卷积流提取不同尺度的特征，并将不同尺度的特征进行融合。作者提出了一种<code>residual bottleneck</code>结构，是基于<code>baseline residual bottleneck</code>结构进行创建。如下图所示。</p>
<p><img src="/uploads/redidual_bottleneck.png" srcset="/img/loading.gif" style="zoom:70%;" /></p>
<p><code>baseline residual bottleneck</code>（如上图如(a)）在输入$x$后，而bottleneck结构学习残差部分$\bar x$，公式如下，其中$F$主要学习单尺度的特征(scale=3)，其中$1 \times 1$的layer主要用于操作特征的维度，对空间信息没有作用。</p>
<script type="math/tex; mode=display">
y = x + \bar x,   s.t.    \bar x = F(x)</script><h4 id="多尺度特征学习"><a href="#多尺度特征学习" class="headerlink" title="多尺度特征学习"></a>多尺度特征学习</h4><p>作者创建基于上图(a)创建了<code>residual bottleneck</code>块(上图(b))，引入了指数$t$来表示特征尺度，对$F$进行扩展为$F^t$，表示将t个<code>Lite3*3</code>叠加起来，这时候的感受视野也变为$(2t+1) \times (2t+1)$，因此，$\bar x$可以表示为多个尺度之和：</p>
<script type="math/tex; mode=display">
\bar x = \sum_{t=1}^{T} F^t(x), T \ge 1</script><p>T为结构块的分支数，$T=4$时，OSNet bottleneck结构如上图b所示，此时共有4个分支，感受视野分别为$3\times3$，$5\times5$，$7\times7$，$9\times9$；然后会通过聚合门将所有特征进行聚合。整体结构类似于<code>baseline residual bottleneck</code>结构。</p>
<h4 id="统一聚合们"><a href="#统一聚合们" class="headerlink" title="统一聚合们"></a>统一聚合们</h4><p>在多尺度学习中，已经获取到了不同尺度的特征，每个流都可以提供不同尺度的特征，为了学习全尺度特征，作者建议以动态的方式组合不同流的输出，根据输入图像赋予不同尺度特征以不同的权重。作者使用的是一种可学习的网络结构(aggregation gate, AG)，如下所示：</p>
<script type="math/tex; mode=display">
\bar x = \sum_{t=1}^T \alpha{_t} x^t = \sum_{t=1}^T G(x^t) x^t, \qquad s.t. \qquad x_t=F^t(x)</script><p>其中G为一个子网络结构学习得到，子网络G包含三个层：一个全局平均池化层，然后是两个全连接(FC)层。输出后的$\alpha_t$为一个向量，而非标量，实现更加细粒度的融合，对通道的权值进行了优化。这和其他的gate不同，实现了更加抽象的，更细粒度的特征融合。</p>
<p>在每个<code>residual bottleneck</code>块中，AG在各个分支中共享参数，参数的数量独立于分支数目T。即上图b中的虚线部分中的AG是共享参数的。</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>OSNet是通过简单地将提出的的轻量级bottleneck逐层叠加而构建的，不需要在网络的不同深度(阶段)定制块。OSnet的整体网络结构如下表所示。OSnet网络的输入image大小为256*128。在模型实现时为了更好的泛化，引入了instance normalisation layer，作者在论文中并未进行重点说明，但在实现的源码中进行了体现。</p>
<p><img src="/uploads/osnet_struct.png" srcset="/img/loading.gif" alt="osnet_struct" style="zoom:50%;" /></p>
<h2 id="OSnet实现"><a href="#OSnet实现" class="headerlink" title="OSnet实现"></a>OSnet实现</h2><p>OSnet使用最后一层使用了标准的<code>multi-label classifier</code>(FC+ softmax)层，通过SGD优化，数据集中每个person-id作为一个单独类别，并使用<code>cross entropy loss</code>进行训练，学习率从0.065开始，并在不同的epoch衰减0.1倍。在计算相似度或距离度量时采用L2距离，512维的向量作为person-id的特征表示，该向量来自于最后一层FC层的512长度向量输出。</p>
<h3 id="OSnet的pytorch实现"><a href="#OSnet的pytorch实现" class="headerlink" title="OSnet的pytorch实现"></a>OSnet的pytorch实现</h3><p>作者在开源的Torchreid库中对OSnet网络，下面为基础版本OSnet源码。其中部分代码进行了注释。</p>
<pre><code class="lang-python">class ConvLayer(nn.Module):
    &quot;&quot;&quot;实现一个标准的卷积层(conv + bn + relu)&quot;&quot;&quot;
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, IN=False):
        super(ConvLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False, groups=groups)
        # 是否使用instance normalisation
        if IN:
            self.bn = nn.InstanceNorm2d(out_channels, affine=True)
        else:
            self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


class Conv1x1(nn.Module):
    &quot;&quot;&quot;1x1 convolution + bn + relu.&quot;&quot;&quot;
    def __init__(self, in_channels, out_channels, stride=1, groups=1):
        super(Conv1x1, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 1, stride=stride, padding=0, bias=False, groups=groups)
        self.bn = nn.BatchNorm2d(out_channels)  # 直接使用了batch normalisation
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


class Conv1x1Linear(nn.Module):
    &quot;&quot;&quot;1x1 convolution + bn (w/o non-linearity).&quot;&quot;&quot;

    def __init__(self, in_channels, out_channels, stride=1):
        super(Conv1x1Linear, self).__init__()
        self.conv = nn.Conv2d(
            in_channels, out_channels, 1, stride=stride, padding=0, bias=False
        )
        self.bn = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return x


class Conv3x3(nn.Module):
    &quot;&quot;&quot;3x3 convolution + bn + relu.&quot;&quot;&quot;
    def __init__(self, in_channels, out_channels, stride=1, groups=1):
        super(Conv3x3, self).__init__()
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            3,
            stride=stride,
            padding=1,
            bias=False,
            groups=groups
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


class LightConv3x3(nn.Module):
    &quot;&quot;&quot;深度可分解卷积(Light3*3): weight 3x3 convolution. 1x1 (linear) + dw 3x3 (nonlinear).
    &quot;&quot;&quot;

    def __init__(self, in_channels, out_channels):
        super(LightConv3x3, self).__init__()
        # 点卷积
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, 1, stride=1, padding=0, bias=False
        )
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False, groups=out_channels)  # groups设置为了out_channels
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


##########
# Building blocks for omni-scale feature learning
##########
class ChannelGate(nn.Module):
    &quot;&quot;&quot;A mini-network that generates channel-wise gates conditioned on input tensor.
    aggregation gate, AG: 平均池化层+卷积层1(全连接)+Relu+卷积层2(全连接)+Sigmoid, 对通道进行了gate
    &quot;&quot;&quot;

    def __init__(self, in_channels, num_gates=None, return_gates=False, gate_activation=&#39;sigmoid&#39;, reduction=16, layer_norm=False):
        super(ChannelGate, self).__init__()
        if num_gates is None:
            num_gates = in_channels
        self.return_gates = return_gates
        self.global_avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=True, padding=0)
        self.norm1 = None
        if layer_norm:
            self.norm1 = nn.LayerNorm((in_channels // reduction, 1, 1))
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Conv2d(in_channels // reduction, num_gates, kernel_size=1, bias=True, padding=0)
        if gate_activation == &#39;sigmoid&#39;:
            self.gate_activation = nn.Sigmoid()
        elif gate_activation == &#39;relu&#39;:
            self.gate_activation = nn.ReLU(inplace=True)
        elif gate_activation == &#39;linear&#39;:
            self.gate_activation = None
        else:
            raise RuntimeError(
                &quot;Unknown gate activation: {}&quot;.format(gate_activation)
            )

    def forward(self, x):
        input = x
        x = self.global_avgpool(x)
        x = self.fc1(x)
        if self.norm1 is not None:
            x = self.norm1(x)
        x = self.relu(x)
        x = self.fc2(x)
        if self.gate_activation is not None:
            x = self.gate_activation(x)
        if self.return_gates:
            return x
        return input * x


class OSBlock(nn.Module):
    &quot;&quot;&quot;论文中的residual bottleneck block实现: Omni-scale feature learning block.&quot;&quot;&quot;
    def __init__(self, in_channels, out_channels, IN=False, bottleneck_reduction=4, **kwargs):
        super(OSBlock, self).__init__()
        mid_channels = out_channels // bottleneck_reduction
        self.conv1 = Conv1x1(in_channels, mid_channels)  # 先使用了1*1的卷积对通道进行一个reduce的操作
        self.conv2a = LightConv3x3(mid_channels, mid_channels)  # 基于Light3*3构建t=1的卷积流
         # 基于Light3*3构建t=2的卷积流
        self.conv2b = nn.Sequential(  
            LightConv3x3(mid_channels, mid_channels),
            LightConv3x3(mid_channels, mid_channels),
        )
        # 基于Light3*3构建t=3的卷积流
        self.conv2c = nn.Sequential(
            LightConv3x3(mid_channels, mid_channels),
            LightConv3x3(mid_channels, mid_channels),
            LightConv3x3(mid_channels, mid_channels),
        )
        # 基于Light3*3构建t=4的卷积流
        self.conv2d = nn.Sequential(
            LightConv3x3(mid_channels, mid_channels),
            LightConv3x3(mid_channels, mid_channels),
            LightConv3x3(mid_channels, mid_channels),
            LightConv3x3(mid_channels, mid_channels),
        )
        self.gate = ChannelGate(mid_channels)
        self.conv3 = Conv1x1Linear(mid_channels, out_channels)  # 线性组合的方式
        self.downsample = None
        if in_channels != out_channels:
            self.downsample = Conv1x1Linear(in_channels, out_channels)
        self.IN = None
        if IN:
            self.IN = nn.InstanceNorm2d(out_channels, affine=True)

    def forward(self, x):
        identity = x
        x1 = self.conv1(x)
        x2a = self.conv2a(x1)
        x2b = self.conv2b(x1)
        x2c = self.conv2c(x1)
        x2d = self.conv2d(x1)
        x2 = self.gate(x2a) + self.gate(x2b) + self.gate(x2c) + self.gate(x2d)  # 每个流对应的是不同的向量，向量是针对通道的
        x3 = self.conv3(x2)
        if self.downsample is not None:
            identity = self.downsample(identity)
        out = x3 + identity  # y = x + x^hat
        if self.IN is not None:
            out = self.IN(out)  # 可能会进行instance normazation，对应的结构为OS + INout block
        return F.relu(out)


##########
# Network architecture: OSnet的网络结构实现
##########
class OSNet(nn.Module):
    &quot;&quot;&quot;Omni-Scale Network.

    Reference:
        - Zhou et al. Omni-Scale Feature Learning for Person Re-Identification. ICCV, 2019.
        - Zhou et al. Learning Generalisable Omni-Scale Representations
          for Person Re-Identification. arXiv preprint, 2019.
    &quot;&quot;&quot;

    def __init__(self, num_classes, blocks, layers, channels, feature_dim=512, loss=&#39;softmax&#39;, IN=False, **kwargs):
        super(OSNet, self).__init__()
        num_blocks = len(blocks)
        assert num_blocks == len(layers)
        assert num_blocks == len(channels) - 1
        self.loss = loss
        self.feature_dim = feature_dim

        # convolutional backbone
        self.conv1 = ConvLayer(3, channels[0], 7, stride=2, padding=3, IN=IN) # kernel size: 7*7
        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        self.conv2 = self._make_layer(blocks[0], layers[0], channels[0], channels[1], reduce_spatial_size=True, IN=IN)
        self.conv3 = self._make_layer(blocks[1], layers[1], channels[1], channels[2], reduce_spatial_size=True)
        self.conv4 = self._make_layer(blocks[2], layers[2], channels[2], channels[3], reduce_spatial_size=False)
        self.conv5 = Conv1x1(channels[3], channels[3])
        self.global_avgpool = nn.AdaptiveAvgPool2d(1)
        # fully connected layer
        self.fc = self._construct_fc_layer(
            self.feature_dim, channels[3], dropout_p=None
        )
        # identity classification layer
        self.classifier = nn.Linear(self.feature_dim, num_classes)

        self._init_params()

    def _make_layer(self, block, layer, in_channels, out_channels, reduce_spatial_size, IN=False):
        layers = []

        layers.append(block(in_channels, out_channels, IN=IN))
        for i in range(1, layer):
            layers.append(block(out_channels, out_channels, IN=IN))

        # 对应网络结构中的transition stage部分：1*1 conv + 2*2 avg pool
        if reduce_spatial_size:
            layers.append(
                nn.Sequential(
                    Conv1x1(out_channels, out_channels),
                    nn.AvgPool2d(2, stride=2)
                )
            )

        return nn.Sequential(*layers)

    def _construct_fc_layer(self, fc_dims, input_dim, dropout_p=None):
        if fc_dims is None or fc_dims &lt; 0:
            self.feature_dim = input_dim
            return None

        if isinstance(fc_dims, int):
            fc_dims = [fc_dims]

        layers = []
        for dim in fc_dims:
            layers.append(nn.Linear(input_dim, dim))
            layers.append(nn.BatchNorm1d(dim))
            layers.append(nn.ReLU(inplace=True))
            if dropout_p is not None:
                layers.append(nn.Dropout(p=dropout_p))
            input_dim = dim

        self.feature_dim = fc_dims[-1]

        return nn.Sequential(*layers)

    def _init_params(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(
                    m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;
                )
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def featuremaps(self, x):
        x = self.conv1(x)
        x = self.maxpool(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        return x

    def forward(self, x, return_featuremaps=False):
        x = self.featuremaps(x)
        if return_featuremaps:
            return x
        v = self.global_avgpool(x)
        v = v.view(v.size(0), -1)
        if self.fc is not None:
            v = self.fc(v)
        if not self.training:
            return v  # 在检测时,输出全连接层的特征
        y = self.classifier(v)
        if self.loss == &#39;softmax&#39;:
            return y
        elif self.loss == &#39;triplet&#39;:
            return y, v
        else:
            raise KeyError(&quot;Unsupported loss: {}&quot;.format(self.loss))
</code></pre>
<h3 id="关于InstanceNorm在不同的位置的实现"><a href="#关于InstanceNorm在不同的位置的实现" class="headerlink" title="关于InstanceNorm在不同的位置的实现"></a>关于InstanceNorm在不同的位置的实现</h3><p>另外，还是实现了osnet_ain的网络结构，整体结构和上面基础版本的osnet基本一致，在一些细节上出现差异，比如新增了<code>LightConvStream</code>，是对<code>LightConv3x3</code>进行的叠加实现，并加入参数<code>t</code>来控制叠加的层数，并对<code>OSBlockINin</code>修改了上面osnet中<code>OSBlock</code>的InstanceNorm的位置。基于InstanceNorm在不同的位置，如下图所示：</p>
<p><img src="/uploads/instance_norm.png" srcset="/img/loading.gif" alt="instance normalzation"></p>
<p>当instance norm(在每个通道上对W*H算均值方差计算，进行归一化操作，具体可以参考batch norm，instance norm，layer norm及group norm区别等)在不同的位置时，<code>os residual block</code>结构存在小的差异。作者在实现时也考虑到了位置不同结构的实现。</p>
<h2 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h2><p>作者在不同的数据集(大型数据集，小型数据集)上，以及不同类型的任务上都进行了测试，模型效果非常理想。在项目上运用该模型，最终效果也是较好的。在不同的person上，特征之间的差异较大。在reid上的效果好于resnet，且在特征提取的效率上也提升了很多。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>OSnet论文阅读笔记：<a href="https://blog.csdn.net/weixin_40671425/article/details/93846404" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40671425/article/details/93846404</a></li>
<li>论文地址：<a href="https://arxiv.org/pdf/1905.00953.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.00953.pdf</a></li>
<li>osnet实现：<a href="https://github.com/KaiyangZhou/deep-person-reid/tree/master/torchreid/models" target="_blank" rel="noopener">https://github.com/KaiyangZhou/deep-person-reid/tree/master/torchreid/models</a></li>
</ol>

            </div>
            <hr>
            <div>
              <p>
                
                  <span>
                <i class="iconfont icon-inbox"></i>
                    
                      <a class="hover-with-bg" href="/categories/deep-learning/">deep learning</a>
                      &nbsp;
                    
                  </span>&nbsp;&nbsp;
                
                
                  <span>
                <i class="iconfont icon-tag"></i>
                    
                      <a class="hover-with-bg" href="/tags/deep-learning/">deep-learning</a>
                    
                      <a class="hover-with-bg" href="/tags/CNN/">CNN</a>
                    
                      <a class="hover-with-bg" href="/tags/convolution/">convolution</a>
                    
                      <a class="hover-with-bg" href="/tags/reid/">reid</a>
                    
                  </span>
                
              </p>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/03/19/Win%E5%B9%B3%E5%8F%B0python%E7%9A%84web(uvicorn)%E6%9C%8D%E5%8A%A1%E5%8C%96/">
                        <i class="fa fa-chevron-left"></i>
                        <span class="hidden-mobile">Win平台python的web(uvicorn)服务化</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/10/24/%E6%99%BA%E8%83%BD%E9%99%B7%E8%BF%9B%E8%A7%82%E5%90%8E%E6%84%9F/">
                        <span class="hidden-mobile">智能陷进观后感</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="fa fa-chevron-right"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

              
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc-start"></div>
<div id="toc">
  <p class="h5"><i class="far fa-list-alt"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/popper.js/1.16.1/umd/popper.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="https://cdn.staticfile.org/mdbootstrap/4.13.0/js/mdb.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var navHeight = $('#navbar').height();
      var toc = $('#toc');
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;
      var tocLimMax = 2 * boardTop + boardCtn.height();

      $(window).scroll(function () {
        var tocLimMin = $('#toc-start').offset().top - navHeight;
        var scroH = document.body.scrollTop + document.documentElement.scrollTop;

        if (tocLimMin <= scroH && scroH <= tocLimMax) {
          toc.css({
            'display': 'block',
            'position': 'fixed',
            'top': navHeight,
          });
        } else if (scroH <= tocLimMin) {
          toc.css({
            'position': '',
            'top': '',
          });
        } else if (scroH > tocLimMax) {
          toc.css('display', 'none');
        }
      });
      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc > p').css('visibility', 'visible');
      }
      var offset = boardCtn.css('margin-right')
      $('#toc-ctn').css({
        'right': offset
      })
    });
  </script>





  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>








<!-- Plugins -->



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "ReID的OSnet网络详解&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script defer src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });
      MathJax.Hub.Register.StartupHook("End Jax",function () {
        var BROWSER = MathJax.Hub.Browser;
        var jax = "HTML-CSS";
        if (BROWSER.isMSIE && BROWSER.hasMathPlayer) jax = "NativeMML";
        return MathJax.Hub.setRenderer(jax);
      });
      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script  src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" ></script>

  



  
  
    <script type="text/javascript">
      //定义获取词语下标
      var a_idx = 0;
      jQuery(document).ready(function ($) {
        //点击body时触发事件
        $("body").click(function (e) {
          //需要显示的词语
          var a = new Array("富强", "民主", "文明", "和谐", "自由", "平等", "公正", "法治", "爱国", "敬业", "诚信", "友善");
          //设置词语给span标签
          var $i = $("<span/>").text(a[a_idx]);
          //下标等于原来下标+1  余 词语总数
          a_idx = (a_idx + 1) % a.length;
          //获取鼠标指针的位置，分别相对于文档的左和右边缘。
          //获取x和y的指针坐标
          var x = e.pageX, y = e.pageY;
          //在鼠标的指针的位置给$i定义的span标签添加css样式
          $i.css({
            "z-index": 999,
            "top": y - 20,
            "left": x,
            "position": "absolute",
            "font-weight": "bold",
            "color": rand_color()
          });
          // 随机颜色
          function rand_color() {
            return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")"
          }
          //在body添加这个标签
          $("body").append($i);
          //animate() 方法执行 CSS 属性集的自定义动画。
          //该方法通过CSS样式将元素从一个状态改变为另一个状态。CSS属性值是逐渐改变的，这样就可以创建动画效果。
          //详情请看http://www.w3school.com.cn/jquery/effect_animate.asp
          $i.animate({
            //将原来的位置向上移动180
            "top": y - 180,
            "opacity": 0
            //1500动画的速度
          }, 1500, function () {
            //时间到了自动删除
            $i.remove();
          });
        });
      })
      ;
    </script>
  








</body>
</html>
